{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to compute $H^{-1}\\vec{v}  =\\vec{x}$, where $H$ is a hessian of some function $f$.\n",
    "\n",
    "*   One can perform calculation of Hessian first and multiply after.\n",
    "\n",
    "*   Both Hessian and inversion are expensive tasks. \n",
    "\n",
    "*   Efficient methods like Conjugate Gradient (CG) allows to approximate inverse in relatively few iterations.\n",
    "\n",
    "    This is the same as solving problem\n",
    "\n",
    "$$H \\vec{x} = \\vec{v}$$\n",
    "\n",
    "CG algorithm (for a problem $A \\vec{x} = \\vec{b}$) contains the following elements\n",
    "*   $$\\vec{d}_0 = \\vec{r}_0 = \\vec{b}_0 - A \\vec{x}_0 \\ \\text{ (Initialization)}$$\n",
    "*   $$\\alpha_i = \\frac{\\vec{r}_i^T\\vec{r}_i}{\\vec{d}_i^T A \\vec{d}_i} \\ \\text{ (Line search step size)}$$\n",
    "*   $$\\vec{x}_{i+1}  = \\vec{x}_i  + \\alpha_i \\vec{d}_i \\ \\text{ (Line search)}$$\n",
    "*   $$\\vec{r}_{i+1} = \\vec{r}_i - \\alpha_i A\\vec{d}_i \\ \\text{ (Beginning of this notebook)}$$\n",
    "*   $$\\beta_{i+1} = \\frac{\\vec{r}_{i+1}^T\\vec{r}_{i+1}}{\\vec{r}_{i}^T \\vec{r}_{i}}\\ \\text{ (Orthogonalization coef)} $$\n",
    "*   $$\\vec{d}_{i+1} = \\vec{r}_{i+1} + \\beta_{i+1} \\vec{d}_i \\ \\text{ (Orthogonalization)} $$\n",
    "\n",
    "It contains multiple steps where Hessian $(A)$ is multiplied by a 'search direction' $\\vec{d}$\n",
    "\n",
    "Recall that Hessian can be viewed as a Jacobian of a Gradient (1 dim Jacobian)\n",
    "$$H(\\cdot) = J(\\nabla(\\cdot))$$\n",
    "Thus we see that\n",
    "$$H \\vec{v} = J\\big(\\nabla f\\big) \\vec{v} = J\\big(\\nabla f \\cdot \\vec{v}\\big) $$\n",
    "So we can avoid forming NxN Hessian matrix but use Nx1 gradient vector and get a vector $H \\vec{v}$ directly as second gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Example of a model $f(x,\\theta)$ parametrized by \n",
    "$$\\theta = \\{w_1,w_2,\\dots\\}$$\n",
    "$$f(x,\\theta) = x_1 w_1^2 + x_1 w_2^2 + \\dots = \\sum_i^N x_i w_i^2$$\n",
    "Hessian entry is\n",
    "$$H_{i,j} = \\partial_{w_j}\\partial_{w_i} \\sum_k^N x_k w_k^2 = \\partial_{w_i} \\sum_k^N x_k \\frac{\\partial }{\\partial {w_j}}\\bigg(w_k^2\\bigg) =  \\partial{w_i} \\sum_k^N x_k  2 w_k \\frac{\\partial w_k}{\\partial {w_j}} = 2 \\ \\partial{w_i} \\sum_k^N w_k x_k \\delta_{k,j} =  2 \\ x_j  \\ \\frac{\\partial w_j}{\\partial {w_i}}= 2 x_j \\delta_{i,j}$$\n",
    "So Hessian is diagonal matrix with entires 2\n",
    "$$H_{i,i} = 2 x_i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of coures this is the same as computing jacobian of a gradient:\n",
    "$$[\\nabla f]_i = 2 x_i w_i$$\n",
    "$$J([\\nabla f]_i)_j = \\partial_{w_j} (2 x_i w_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 0., 0., 0., 0.],\n",
       "        [0., 2., 0., 0., 0.],\n",
       "        [0., 0., 2., 0., 0.],\n",
       "        [0., 0., 0., 2., 0.],\n",
       "        [0., 0., 0., 0., 2.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "_ = torch.manual_seed(0)\n",
    "\n",
    "params =  torch.arange(5, requires_grad=True, dtype=float)\n",
    "\n",
    "def model(inp, weights):\n",
    "    return torch.dot(inp, weights**2)\n",
    "\n",
    "x = torch.ones_like(params)\n",
    "f = model(x,params)\n",
    "grad = torch.autograd.grad(f, params, create_graph=True)[0]\n",
    "H = torch.stack([torch.autograd.grad(s, params, create_graph=True)[0] for s in grad])\n",
    "H.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if trick works\n",
    "Using this approach we calculate gradient once and for second gradient we save computational graph, which allows to reuse it after, for different $\\vec{v}$\n",
    "$$H \\vec{v} = J\\big(\\nabla f\\big) \\vec{v} = J\\big(\\nabla f \\cdot \\vec{v}\\big) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hv = J(Jf.v): True\n"
     ]
    }
   ],
   "source": [
    "grad = torch.autograd.grad(f, params, create_graph=True)[0]\n",
    "v = torch.randn(5, dtype=float)\n",
    "print('Hv = J(Jf.v):',torch.equal(H @ v, torch.autograd.grad(grad @ v, params, create_graph=True)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000], dtype=torch.float64); ff(x).item() = -1.25; tensor([0., 0., 0., 0., 0.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def conjugate_gradient_hess(b, x0=None, max_iters = None, tol = 1e-10):\n",
    "\n",
    "    grad = torch.autograd.grad(f, params, create_graph=True)[0]\n",
    "\n",
    "    def Av(x):\n",
    "        return torch.autograd.grad(grad @ x, params, create_graph=True)[0]\n",
    "    \n",
    "    x = torch.zeros_like(b) if x0 is None else x0  #init guess\n",
    "\n",
    "    r           = b - Av(x)    # residual\n",
    "    d           = r.clone()  # direction\n",
    "    rr          = torch.dot(r,r)\n",
    "    num_iters   = len(b) if max_iters is None else max_iters\n",
    "    for _ in range(num_iters):\n",
    "        Ad          = Av(d)\n",
    "        step_size   = rr/ (d @ Ad)\n",
    "        x           += step_size * d\n",
    "        r           -= step_size * Ad\n",
    "        rr_new      = torch.dot(r,r)\n",
    "        if rr_new < tol: break\n",
    "        d           = r + (rr_new/rr)*d\n",
    "        rr          = rr_new.clone()\n",
    "    return x.detach()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(0)\n",
    "    A = H.detach()\n",
    "    b = torch.ones_like(params)\n",
    "    \n",
    "    def ff(x):\n",
    "        return (0.5*x @ A @ x - torch.dot(b,x))\n",
    "    \n",
    "    x0 = torch.rand(size = b.shape, dtype=float)\n",
    "    x = conjugate_gradient_hess(b,x0)\n",
    "    with torch.no_grad():\n",
    "        print(f'{x = }; {ff(x).item() = }; {A @ x - b}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
