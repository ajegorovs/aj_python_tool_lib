{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to solve a linear equation\n",
    "$$A \\vec{x} = \\vec{b}$$\n",
    "we, can do it by finding minima of following quadratic form:\n",
    "$$f(\\vec{x}) = \\frac{1}{2} \\vec{x}^T A \\vec{x} - \\vec{b}^T\\vec{x}$$\n",
    "why? To show it, we have to find a gradient $\\nabla_{\\vec{x}} \\ f(\\vec{x})$.\n",
    "\n",
    "Gradient will tell us where local/global minima is $\\nabla_{\\vec{x}} \\ f(\\vec{x}) = \\vec{0}$ and direction to that minima.\n",
    "\n",
    "It is enough to find 'first derivative' since problem is parabolic, and there exists __either__ minimum ar maximum (questioning saddle points?! implies A should be positive).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find $m$-th component of a gradient $\\rightarrow \\frac{\\delta f}{\\delta{x_m}}$\n",
    "$$f(\\vec{x}) = \\frac{1}{2}\\sum_j x_j \\bigg[\\sum_i A_{ij} x_i\\bigg] - \\sum_k b_k x_k$$\n",
    "$$\\frac{\\delta f}{\\delta{x_m}} = \\frac{1}{2} \\bigg( \\sum_j  \\frac{\\delta x_j}{\\delta{x_m}} \\bigg[\\sum_i A_{ij} x_i\\bigg] + \\sum_j x_j \\bigg[\\sum_i A_{ij} \\frac{\\delta x_i}{\\delta{x_m}}\\bigg]\\bigg) - \\sum_k b_k \\frac{\\delta x_k}{\\delta{x_m}}$$\n",
    "$\\frac{\\delta x_a}{\\delta{x_b}}$ is a delta function or identity matrix so  \n",
    "$$A_{mj} = (AI)_{mj} = \\sum_i A_{ij} \\delta^i_m = A_{mj}$$\n",
    "which simply replaces index $i \\leftrightarrow m$. \n",
    "\n",
    "Same with vectors \n",
    "$$\\sum_k b_k \\delta^k_m = b_m$$\n",
    "\n",
    "We rewrite our gradient component expression\n",
    "$$\\frac{\\delta f}{\\delta{x_m}} = \\frac{1}{2} \\bigg( \\sum_j  \\delta^j_m \\underbrace{\\bigg[\\sum_i A_{ij} x_i\\bigg]}_{\\text{vector's j-th component}} + \\sum_j x_j \\bigg[\\sum_i A_{ij} \\delta^i_m\\bigg]\\bigg) - \\sum_k b_k \\delta^k_m$$\n",
    "$$\\frac{\\delta f}{\\delta{x_m}} = \\frac{1}{2} \\bigg( \\sum_i A_{im} x_i + \\sum_j x_j A_{mj}\\bigg) -  b_m$$\n",
    "\n",
    "Rename indices and combine terms:\n",
    "$$\\frac{\\delta f}{\\delta{x_m}} = \\frac{1}{2} \\sum_i (A_{im} + A_{mi}) \\ x_i  -  b_m$$\n",
    "which implies that\n",
    "$$\\nabla_{\\vec{x}} \\ f(\\vec{x})  = \\frac{1}{2} (A + A^T) \\ \\vec{x} - \\vec{b}$$\n",
    "\n",
    "if matrix $A$ is symmetric $A^T = A$, then\n",
    "$$\\nabla_{\\vec{x}} \\ f(\\vec{x})  = A \\vec{x} - \\vec{b}$$\n",
    "so a point $\\vec{x}^*$ where gradient of $f$ is zero,\n",
    "$$\\nabla_{\\vec{x}} \\ f(\\vec{x})\\bigg|_{\\vec{x}=\\vec{x}^*}  = A\\vec{x}^* - \\vec{b} = \\vec{0}$$\n",
    "satisfies our initial problem\n",
    "$$ A\\vec{x}^*= \\vec{b}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "(Optional) Using Einstein summation:\n",
    "\n",
    "Using this approach we raise and lower indices with metric tensor, swap indices using Kronecker delta function.\n",
    "\n",
    "By default we assume that we use contravariant vector components (with indices on top).\n",
    "\n",
    "Summation is made over diagonal indices. Double indices imply summation.\n",
    "$$ \\vec{x} = \\sum_i x^i \\vec{e}_i = x^i \\vec{e}_i$$\n",
    "$$A \\vec{x} = \\vec{b} \\rightarrow A^{\\cdot j}_{i \\cdot} x^i = b^j$$\n",
    "\n",
    "$$ f = \\frac{1}{2} x_j A^{\\cdot j}_{i \\cdot} x^i - b_i x^i$$\n",
    "_Arguably one can write $f$ as_\n",
    "$$ f = \\frac{1}{2} x^i A_{ij} x^j - b_i x^i$$\n",
    "_which makes taking derivative of $x^i A_{ij} x^j$ easier, but we still cant escape covariant components in $b_i x^i$ and $A_{ij}$._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write gradient using Einstein summation  as\n",
    "$$\\frac{\\delta f}{\\delta{x^m}} =  \\frac{1}{2}\\frac{\\delta x_j}{\\delta{x^m}}A^{\\cdot j}_{i \\cdot} x^i +  \\frac{1}{2}x_j A^{\\cdot j}_{i \\cdot} \\frac{\\delta x^i}{\\delta{x^m}} - b_k \\frac{\\delta x^k}{\\delta{x^m}}$$\n",
    "partial derivatives are \"orthogonal\" $(\\frac{\\delta x}{\\delta{y}} = \\frac{\\delta y}{\\delta{x}} = 0; \\frac{\\delta x}{\\delta{x}} = \\frac{\\delta y}{\\delta{y}} = 1)$\n",
    "$$\\frac{\\delta x^i}{\\delta{x^m}} = \\delta^i_m$$\n",
    "We can raise and lower indices using metric tensor\n",
    "$$x_j = g_{jk}x^k $$\n",
    "$$g_{jk}g^{km} = \\delta^m_j$$\n",
    "Using metric tensor we can derive derivative of covariant component w.r.t contravariant variable\n",
    "$$\\frac{\\delta x_j}{\\delta{x^m}} = \\frac{\\delta g_{jk}x^k}{\\delta{x^m}} = g_{jk} \\frac{\\delta x^k}{\\delta{x^m}} = g_{jk} \\delta^k_m = g_{jm}$$\n",
    "_here we rename $g_{jk}$ and not lower $\\delta^k_m$. But these should be closely related via $g_{jk}g^{km} = \\delta^m_j$._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with\n",
    "$$\\frac{\\delta f(\\vec{x})}{\\delta{x^m}} =  \\frac{1}{2}g_{jm} A^{\\cdot j}_{i \\cdot} x^i +  \\frac{1}{2}x_j A^{\\cdot j}_{i \\cdot} \\delta^i_m - b_k \\delta^k_m$$\n",
    "$$ =  \\frac{1}{2}A_{i m} x^i +  \\frac{1}{2}x_j A^{\\cdot j}_{m \\cdot} - b_m$$\n",
    "$$ =  \\frac{1}{2}A_{i m} x^i + \\frac{1}{2}g_{js} x^s A^{\\cdot j}_{m \\cdot} - b_m$$\n",
    "$$ =  \\frac{1}{2}A_{i m} x^i + \\frac{1}{2}A_{m s} x^s - b_m$$\n",
    "\n",
    "By changing indices and gathering terms we see that\n",
    "$$\\frac{\\delta f}{\\delta{x^m}} =  \\frac{1}{2}(A_{i m} +  A_{m i}) \\ x^i - b_m$$\n",
    "\n",
    "With same results as previously. \n",
    "\n",
    "_I dont know why it leaves us with covariant components though xd. In Cartesian C.S there is not difference between two._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
