{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See prev chapter [What_are_state_and_action_values](Sutton_P1_Ch03_What_are_state_and_action_values.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy iteration method\n",
    "\n",
    "This method continuously improves policy via Bellman's equation turned into 'assignment' task.\n",
    "\n",
    "\n",
    "Lets see how.\n",
    "\n",
    "We know that with any given initial policy $\\pi_0(a|s)$ we can associate state values $v_{\\pi_0}(s)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct policy evaluation\n",
    "*   One way of determining $v_{\\pi_0}(s)$ is to solve system of $|S|$ linear equations.\n",
    "\n",
    "    In simple terms, you can say that value $v(s_i)$ is formed from instantaneous reward $r(s_i)$ and expected rewards from all possible transition  for all states $s_i$\n",
    "    $$\n",
    "    \\begin{cases}\n",
    "    v(s_0) = r(s_0) + \\gamma \\underset{s^\\prime \\in S}{\\sum} p(s^\\prime | s_0, a = \\pi(s_0)) \\cdot v(s^\\prime)\\\\\n",
    "    v(s_1) = r(s_1) + \\gamma \\underset{s^\\prime \\in S}{\\sum} p(s^\\prime | s_1, a = \\pi(s_1)) \\cdot v(s^\\prime)\\\\\n",
    "    \\dots\\\\\n",
    "    v(s_n) = r(s_n) + \\gamma \\underset{s^\\prime \\in S}{\\sum} p(s^\\prime | s_n, a = \\pi(s_n)) \\cdot v(s^\\prime)\n",
    "    \\end{cases}$$\n",
    "\n",
    "    Which you can rewrite as matrix equation\n",
    "\n",
    "    $$V = (I-\\gamma P)^{-1}R$$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Policy Evaluation\n",
    "*   Alternatively, we can do it iteratively, by 'guessing' values states starting from $v_{\\pi_0}(s)$ \n",
    "\n",
    "    This method is called <b>Iterative Policy Evaluation</b>\n",
    "\n",
    "    Expressed in terms of expected rewards (Bellman's equation) we formulate update loop as\n",
    "    $$v_{\\pi_0}^{k+1}(s) = \\sum_a \\pi(a|s) \\sum_{r \\in R} \\sum_{s^\\prime \\in S} p(s^\\prime, r|s,a) \\cdot \\biggl[ r + \\gamma \\cdot v_{\\pi_0}^{k}(s^\\prime)\\biggr]$$\n",
    "    In fact, known immediate rewards guide this iterative process to the true value $$\\underset{k\\to \\infty}{\\lim} v_{\\pi_0}^{k}(s) = v_{\\pi_0, True}(s)$$\n",
    "\n",
    "<i>We will define these in an easier to understand form later in text</i>\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy improvement\n",
    "Even if policy $v_{\\pi_0}(s)$ is sub-optimal, its value states show approximate regions in state-space where you can get good rewards.\n",
    "\n",
    "<b>Policy improvement</b> extracts better performing actions (thus policy $\\pi_1(s)$) from given state values $v_{\\pi_0}(s)$.\n",
    "\n",
    "$$\\pi^\\prime(s) = \\underset{a}{\\mathrm{argmax}} \\sum_{r \\in R} \\sum_{s^\\prime \\in S} p(s^\\prime, r|s,a) \\cdot \\biggl[ r + \\gamma \\cdot v_{\\pi}(s^\\prime)\\biggr]$$\n",
    "When policy is changed to $\\pi_1(s)$, it invalidates value states $v_{\\pi_0}(s)$, so these have to be recalculated.\n",
    "\n",
    "## Policy Iteration\n",
    "\n",
    "<b>Policy Iteration</b> repeatedly perform policy evaluation $E$, followed by policy improvement $I$. \n",
    "\n",
    "$$\\pi_0 \\overset{E}{\\rightarrow} v_{\\pi_0}  \\overset{I}{\\rightarrow} \\pi_1 \\overset{E}{\\rightarrow} v_{\\pi_1}  \\overset{I}{\\rightarrow}  \\dots \\overset{I}{\\rightarrow}  \\pi_\\ast \\overset{E}{\\rightarrow} v_{\\ast}$$\n",
    "This leads to optimal policy and associated value states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm:\n",
    "\n",
    "<img    src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*A1WWiVls4dS5tpb7bDMCGg.png\" \n",
    "        alt=\"image info\" \n",
    "        style=\"background-color:white;padding:5px;\" \n",
    "        width=\"300\" \n",
    "         />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input example : \n",
      ">>> arr_T = np.array([[r'\\vec{v}_1', r'\\vec{v}_2']]).T\n",
      ">>> print_tex(arr_T,'=', np.arange(1,5).reshape(2,-1)/4, r'; symbols: \\otimes, \\cdot,\\times')\n",
      "output: \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\begin{bmatrix}\\vec{v}_1 \\\\ \\vec{v}_2\\end{bmatrix}=\\begin{bmatrix} 1/4 & 1/2 \\\\ 3/4 & 1 \\end{bmatrix}; symbols: \\otimes, \\cdot,\\times$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from misc_tools.print_latex import print_tex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Frozen Lake environment\n",
    "\n",
    "*   Any actions result in transitions to any tangential state with equal state\n",
    "\n",
    "    Later we can observe how this leads to weird but optimal choices for actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env=gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True,render_mode=\"ansi\")#,render_mode=\"human\"\n",
    "env.reset()\n",
    "#env.close()\n",
    "def p_sp_sa(state, action):\n",
    "    1\n",
    "print(env.render())\n",
    "rewards = np.array(env.reward_range)# [0,1]\n",
    "num_states= env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "where_holes = np.argwhere(env.unwrapped.desc.flatten() == b'H').flatten()\n",
    "where_goals = np.argwhere(env.unwrapped.desc.flatten() == b'G').flatten()\n",
    "where_start = np.argwhere(env.unwrapped.desc.flatten() == b'S').flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(s', a) normalized: True\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "policy_init = np.ones((num_states, num_actions))/num_actions\n",
    "v_pi_init = np.zeros((num_states,))\n",
    "p_spr_sa = np.zeros((num_states,  num_actions, num_states, rewards.size))\n",
    "for state, actions_d in env.P.items():\n",
    "    for action, transitions in actions_d.items():\n",
    "        for (p, state_tar, reward, is_terminal) in transitions:\n",
    "            p_spr_sa[state, action, state_tar, int(reward)] += p   \n",
    "            # apparently walls actions transition to same state = different actions to same state. so += p\n",
    "print(f'(s\\', a) normalized: {np.all(p_spr_sa.sum(axis=(-2,-1)) == 1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from [RL_Reinforced_Learning/Meaning_of_Psasr_probability_matrix.ipynb](data_processing/neural_networks/RL_Reinforced_Learning/Meaning_of_Psasr_probability_matrix.ipynb)\n",
    "$$\\boxed{r(s,a,s^\\prime) = \\frac{\\sum_r  r\\cdot p(s^\\prime,r|s,a)}{p(s^\\prime|s,a)}; \\ \n",
    "p(s^\\prime|s,a) = \\sum_r p(s^\\prime,r|s,a)}$$\n",
    "We can redefine $q_{\\pi}(s,a)$ in terms of expected rewards and transition probabilities to states $s^\\prime$:\n",
    "$$q_{\\pi}(s,a) = \\sum_{r \\in R} \\sum_{s^\\prime \\in S} p(s^\\prime, r|s,a) \\cdot \\biggl[ r + \\gamma \\cdot v_{\\pi}(s^\\prime)\\biggr]=  \\sum_{s^\\prime \\in S} \\left[\\sum_{r \\in R} p(s^\\prime, r|s,a) \\cdot r + \\gamma \\sum_{r \\in R} p(s^\\prime, r|s,a) \\cdot v_{\\pi}(s^\\prime)\\right] = $$\n",
    "$$ = \\sum_{s^\\prime \\in S} \\left[p(s^\\prime|s,a)\\cdot r(s,a,s^\\prime) + \\gamma \\cdot v_{\\pi}(s^\\prime) \\cdot \\sum_{r \\in R} p(s^\\prime, r|s,a) \\right]= \\sum_{s^\\prime \\in S} \\biggl[  p(s^\\prime|s,a)\\cdot r(s,a,s^\\prime) + \\gamma \\cdot v_{\\pi}(s^\\prime) \\cdot p(s^\\prime|s,a) \\biggr] =$$ \n",
    "$$= \\sum_{s^\\prime \\in S} p(s^\\prime|s,a)\\cdot\\biggl[r(s,a,s^\\prime) + \\gamma \\cdot v_{\\pi}(s^\\prime)\\biggr]$$\n",
    "\n",
    "$$\\boxed{q_{\\pi}(s,a)  = \\sum_{s^\\prime \\in S} p(s^\\prime|s,a)\\cdot\\biggl[r(s,a,s^\\prime) + \\gamma \\cdot v_{\\pi}(s^\\prime)\\biggr]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sp_sa = p_spr_sa.sum(-1)\n",
    "r_sas_top = p_spr_sa @ rewards\n",
    "r_sas = np.divide(r_sas_top, p_sp_sa, out = np.zeros_like(r_sas_top), where= (p_sp_sa != 0))\n",
    "\n",
    "def get_q_sa(state_value):\n",
    "   return (p_sp_sa * (r_sas + gamma*state_value)).sum(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classic policy evaluation (Expected reward)\n",
    "$$v_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s) \\sum_{r \\in R} \\sum_{s^\\prime \\in S} p(s^\\prime, r|s,a) [ r + \\gamma v_{\\pi}(s^\\prime)] = \\sum_{a \\in A} \\pi(a|s) \\cdot q_{\\pi}(s,a) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(state_action_value, policy):\n",
    "   v_pi = (policy * state_action_value).sum(-1)\n",
    "   return v_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy policy improvement\n",
    "$$\\pi(s) = \\underset{a}{\\mathrm{argmax}} \\sum_{r \\in R} \\sum_{s^\\prime \\in S} p(s^\\prime, r|s,a) [ r + \\gamma v_{\\pi}(s^\\prime)] = \\underset{a}{\\mathrm{argmax}} \\ q_{\\pi}(s,a) $$\n",
    "Can do $\\pi(a|s)$ for action-states with same value ([as here](https://aleksandarhaber.com/policy-iteration-algorithm-in-python-and-tests-with-frozen-lake-openai-gym-environment-reinforcement-learning-tutorial/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy_k(state_value, policy, num_iters_max = 50, eps = 1e-6):\n",
    "   v_pi = state_value.copy()\n",
    "   for k in range(num_iters_max):\n",
    "      v_pi_old = v_pi.copy()\n",
    "      q_pi     = get_q_sa(v_pi)\n",
    "      v_pi     = evaluate_policy(q_pi, policy)\n",
    "      if np.max(np.abs(v_pi-v_pi_old)) < eps:\n",
    "         print(f'Policy eval: early break at iter: {k}')\n",
    "         break\n",
    "         \n",
    "   q_pi  = get_q_sa(v_pi)\n",
    "   return v_pi, q_pi\n",
    "\n",
    "def improve_policy(state_action_value):\n",
    "   # ChatGPT +  modified. Multiple maxes\n",
    "   a = state_action_value\n",
    "   max_values = np.max(a, axis=-1)\n",
    "   max_indices = [np.where(a[i] == max_values[i])[0] for i in range(a.shape[0])]\n",
    "\n",
    "   one_hot_matrix = np.zeros_like(a)\n",
    "\n",
    "   for i, indices in enumerate(max_indices):\n",
    "      one_hot_matrix[i, indices] = 1/len(indices)\n",
    "   max_indices,one_hot_matrix\n",
    "\n",
    "   return one_hot_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy iteration loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy eval: early break at iter: 32\n",
      "Policy iter: early break at iter: 1\n"
     ]
    }
   ],
   "source": [
    "v_pi  = v_pi_init.copy()\n",
    "pi    = policy_init.copy()\n",
    "\n",
    "for k in range(500):\n",
    "   pi_old = pi.copy()\n",
    "   v_pi, q_pi = evaluate_policy_k(v_pi, pi)\n",
    "   pi = improve_policy(q_pi)\n",
    "   if np.max(np.abs(pi-pi_old)) < 1e-5:\n",
    "         print(f'Policy iter: early break at iter: {k}')\n",
    "         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle q_\\pi(s,a)=\\begin{bmatrix}\\_ & \\leftarrow & \\downarrow & \\rightarrow & \\uparrow \\\\ s_{0} & 0.07 & 0.07 & 0.07 & 0.06 \\\\ s_{1} & 0.04 & 0.04 & 0.04 & 0.06 \\\\ s_{2} & 0.07 & 0.07 & 0.07 & 0.06 \\\\ s_{3} & 0.04 & 0.04 & 0.03 & 0.06 \\\\ s_{4} & 0.09 & 0.07 & 0.06 & 0.05 \\\\ s_{5} & 0.0 & 0.0 & 0.0 & 0.0 \\\\ s_{6} & 0.11 & 0.09 & 0.11 & 0.02 \\\\ s_{7} & 0.0 & 0.0 & 0.0 & 0.0 \\\\ s_{8} & 0.07 & 0.12 & 0.1 & 0.15 \\\\ s_{9} & 0.16 & 0.25 & 0.2 & 0.13 \\\\ s_{10} & 0.3 & 0.27 & 0.23 & 0.11 \\\\ s_{11} & 0.0 & 0.0 & 0.0 & 0.0 \\\\ s_{12} & 0.0 & 0.0 & 0.0 & 0.0 \\\\ s_{13} & 0.19 & 0.31 & 0.38 & 0.27 \\\\ s_{14} & 0.4 & 0.64 & 0.61 & 0.54 \\\\ s_{15} & 0.0 & 0.0 & 0.0 & 0.0\\end{bmatrix};v_\\pi(s) = \\begin{bmatrix}a & s_{0,\\cdot} & s_{1,\\cdot} & s_{2,\\cdot} & s_{3,\\cdot} \\\\ s_{\\cdot,0} & 0.07 & 0.06 & 0.07 & 0.06 \\\\ s_{\\cdot,1} & 0.09 & 0.0 & 0.11 & 0.0 \\\\ s_{\\cdot,2} & 0.15 & 0.25 & 0.3 & 0.0 \\\\ s_{\\cdot,3} & 0.0 & 0.38 & 0.64 & 0.0\\end{bmatrix};\\pi(a|s) = \\begin{bmatrix}\\_ & \\leftarrow & \\downarrow & \\rightarrow & \\uparrow \\\\ s_{0} & 1.0 & 0.0 & 0.0 & 0.0 \\\\ s_{1} & 0.0 & 0.0 & 0.0 & 1.0 \\\\ s_{2} & 1.0 & 0.0 & 0.0 & 0.0 \\\\ s_{3} & 0.0 & 0.0 & 0.0 & 1.0 \\\\ s_{4} & 1.0 & 0.0 & 0.0 & 0.0 \\\\ s_{5} & 0.25 & 0.25 & 0.25 & 0.25 \\\\ s_{6} & 0.5 & 0.0 & 0.5 & 0.0 \\\\ s_{7} & 0.25 & 0.25 & 0.25 & 0.25 \\\\ s_{8} & 0.0 & 0.0 & 0.0 & 1.0 \\\\ s_{9} & 0.0 & 1.0 & 0.0 & 0.0 \\\\ s_{10} & 1.0 & 0.0 & 0.0 & 0.0 \\\\ s_{11} & 0.25 & 0.25 & 0.25 & 0.25 \\\\ s_{12} & 0.25 & 0.25 & 0.25 & 0.25 \\\\ s_{13} & 0.0 & 0.0 & 1.0 & 0.0 \\\\ s_{14} & 0.0 & 1.0 & 0.0 & 0.0 \\\\ s_{15} & 0.25 & 0.25 & 0.25 & 0.25\\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = np.array([[r'a']+['s_{'+ r'\\cdot,'+str(i)+'}' for i in range(4)]]).T\n",
    "b = np.array([['s_{'+str(i)+ r',\\cdot'+'}' for i in range(4)]])\n",
    "v_s = np.hstack([a, np.vstack([b, v_pi.reshape(4,4).round(2) ])])\n",
    "\n",
    "a       = np.array([[r'\\_']+[r'\\leftarrow', r'\\downarrow',r'\\rightarrow', r'\\uparrow']])\n",
    "b       = np.array([['s_{'+str(i)+'}'  for i in range(num_states)]]).T\n",
    "pi2      = np.vstack([a, np.hstack([b, pi.round(2) ])])\n",
    "\n",
    "q_sa    = np.vstack([a, np.hstack([b, q_pi.round(2)])])\n",
    "\n",
    "print_tex('q_\\pi(s,a)=',q_sa, ';v_\\pi(s) = ',v_s, ';\\pi(a|s) = ', pi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot policy\n",
    "Notice how stochasticity impacts policy. \n",
    "\n",
    "Its less 'safe' to move between holes as this action will, with probability $p = \\frac{2}{3}$, put you into any of these holes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_arrows_on_grid(A, N):\n",
    "    # ChatGPT + modified\n",
    "    fig, axes = plt.subplots(N, N, figsize=(4,4), sharex=True,sharey=True)\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    axes = axes.flatten()\n",
    "    for i,(ax,arrows) in enumerate(zip(axes,A)):\n",
    "        if i in where_holes:\n",
    "            ax.set_facecolor('red')\n",
    "        elif i in where_goals:\n",
    "            ax.set_facecolor('green')\n",
    "        elif i in where_start:\n",
    "            ax.set_facecolor('orange')\n",
    "        if all(arrows): continue\n",
    "        if arrows[0]:  # Left arrow\n",
    "            ax.arrow(0.5, 0.5, -0.3, 0, head_width=0.1, head_length=0.1, fc='k', ec='k')\n",
    "        if arrows[1]:  # Bottom arrow\n",
    "            ax.arrow(0.5, 0.5, 0, -0.3, head_width=0.1, head_length=0.1, fc='k', ec='k')\n",
    "        if arrows[2]:  # Right arrow\n",
    "            ax.arrow(0.5, 0.5, 0.3, 0, head_width=0.1, head_length=0.1, fc='k', ec='k')\n",
    "        if arrows[3]:  # Top arrow\n",
    "            ax.arrow(0.5, 0.5, 0, 0.3, head_width=0.1, head_length=0.1, fc='k', ec='k')\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.show()\n",
    "plot_arrows_on_grid(pi>0,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate play\n",
    "Code (modified) from\n",
    "https://zoo.cs.yale.edu/classes/cs470/materials/hws/hw7/FrozenLake.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hot Mexican\\VS_Code_Proj\\.venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "You took an average of 43 steps to get the frisbee\n",
      "And you fell in the hole 22.05 % of the times\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_score(env, policy, episodes=10_000):\n",
    "  misses = 0\n",
    "  steps_list = []\n",
    "  for episode in range(episodes):\n",
    "    observation = env.reset()[0]\n",
    "    steps=0\n",
    "    while True:\n",
    "      \n",
    "      pi_as = policy[observation]\n",
    "      action = np.random.choice(np.arange(num_actions),p=pi_as)\n",
    "      observation, reward, done = env.step(action)[:3]\n",
    "      steps+=1\n",
    "      if done and reward == 1:\n",
    "        # print('You have got the fucking Frisbee after {} steps'.format(steps))\n",
    "        steps_list.append(steps)\n",
    "        break\n",
    "      elif done and reward == 0:\n",
    "        # print(\"You fell in a hole!\")\n",
    "        misses += 1\n",
    "        break\n",
    "  print('----------------------------------------------')\n",
    "  print('You took an average of {:.0f} steps to get the frisbee'.format(np.mean(steps_list)))\n",
    "  print('And you fell in the hole {:.2f} % of the times'.format((misses/episodes) * 100))\n",
    "  print('----------------------------------------------')\n",
    "\n",
    "get_score(env, pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Code](https://aleksandarhaber.com/policy-iteration-algorithm-in-python-and-tests-with-frozen-lake-openai-gym-environment-reinforcement-learning-tutorial/) from A.Haber\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatePolicy(env,valueFunctionVector,policy,discountRate,maxNumberOfIterations,convergenceTolerance):\n",
    "    import numpy as np\n",
    "    convergenceTrack=[]\n",
    "    for iterations in range(maxNumberOfIterations):\n",
    "        convergenceTrack.append(np.linalg.norm(valueFunctionVector,2))\n",
    "        valueFunctionVectorNextIteration=np.zeros(env.observation_space.n)\n",
    "        for state in env.P:\n",
    "            outerSum=0\n",
    "            for action in env.P[state]:\n",
    "                innerSum=0\n",
    "                for probability, nextState, reward, isTerminalState in env.P[state][action]:\n",
    "                    #print(probability, nextState, reward, isTerminalState)\n",
    "                    innerSum=innerSum+ probability*(reward+discountRate*valueFunctionVector[nextState])\n",
    "                outerSum=outerSum+policy[state,action]*innerSum\n",
    "            valueFunctionVectorNextIteration[state]=outerSum\n",
    "        if(np.max(np.abs(valueFunctionVectorNextIteration-valueFunctionVector))<convergenceTolerance):\n",
    "            valueFunctionVector=valueFunctionVectorNextIteration\n",
    "            print('Iterative policy evaluation algorithm converged!')\n",
    "            break\n",
    "        valueFunctionVector=valueFunctionVectorNextIteration       \n",
    "    return valueFunctionVector\n",
    "\n",
    "def improvePolicy(env,valueFunctionVector,numberActions,numberStates,discountRate):\n",
    "    import numpy as np\n",
    "    qvaluesMatrix=np.zeros((numberStates,numberActions))\n",
    "    improvedPolicy=np.zeros((numberStates,numberActions))\n",
    "     \n",
    "    for stateIndex in range(numberStates):\n",
    "        for actionIndex in range(numberActions):\n",
    "            for probability, nextState, reward, isTerminalState in env.P[stateIndex][actionIndex]:\n",
    "                qvaluesMatrix[stateIndex,actionIndex]=qvaluesMatrix[stateIndex,actionIndex]+probability*(reward+discountRate*valueFunctionVector[nextState])\n",
    "             \n",
    "        bestActionIndex=np.where(qvaluesMatrix[stateIndex,:]==np.max(qvaluesMatrix[stateIndex,:]))\n",
    " \n",
    "        improvedPolicy[stateIndex,bestActionIndex]=1/np.size(bestActionIndex)\n",
    "    return improvedPolicy,qvaluesMatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterative policy evaluation algorithm converged!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "You took an average of 43 steps to get the frisbee\n",
      "And you fell in the hole 21.55 % of the times\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "discountRate=0.9\n",
    "stateNumber=16\n",
    "actionNumber=4\n",
    "\n",
    "maxNumberOfIterationsOfPolicyIteration=3000\n",
    "initialPolicy=(1/actionNumber)*np.ones((stateNumber,actionNumber))\n",
    "valueFunctionVectorInitial=np.zeros(env.observation_space.n)\n",
    "maxNumberOfIterationsOfIterativePolicyEvaluation=1000\n",
    "convergenceToleranceIterativePolicyEvaluation=10**(-6)\n",
    "currentPolicy=initialPolicy\n",
    "valueFunctionVectorComputed =evaluatePolicy(env,valueFunctionVectorInitial,currentPolicy,discountRate,maxNumberOfIterationsOfIterativePolicyEvaluation,convergenceToleranceIterativePolicyEvaluation)\n",
    "improvedPolicy,qvaluesMatrix=improvePolicy(env,valueFunctionVectorComputed,actionNumber,stateNumber,discountRate)\n",
    "#print_tex(improvedPolicy)\n",
    "plot_arrows_on_grid(improvedPolicy>0,4)\n",
    "get_score(env, improvedPolicy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration Method\n",
    "\n",
    "Value iteration method can be viewed as truncated \"Policy Iteration Method\"\n",
    "\n",
    "We see that its not necessary for state value to converge and greedy policy can be extracted sooner.\n",
    "\n",
    "In fact we can stop searching for improved policy as solve instead, only for optimal state values.\n",
    "\n",
    "## Bellman's optimality equation\n",
    "\n",
    "Or in terms of Bellman's optimality equation\n",
    "\n",
    "$$v_{\\ast}^{k+1}(s) = \\underset{a \\in A}{\\mathrm{max}} \\ q_{\\ast}(s,a) = \\underset{a \\in A}{\\mathrm{max}} \\ \\sum_{r \\in R} \\sum_{s^\\prime \\in S} p(s^\\prime, r|s,a) \\cdot \\biggl[ r + \\gamma \\cdot v_{\\ast}^{k}(s^\\prime)\\biggr] $$\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "<img    src=\"https://bpb-us-w2.wpmucdn.com/sites.gatech.edu/dist/d/958/files/2020/12/Value_Iteration-1.png\" \n",
    "        alt=\"image info\" \n",
    "        style=\"background-color:white;padding:0px;\" \n",
    "        width=\"400\" \n",
    "         />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy_max(state_action_value):\n",
    "   v_pi =  state_action_value.max(-1)\n",
    "   return v_pi\n",
    "\n",
    "def get_q_sa(state_value):\n",
    "   return (p_sp_sa * (r_sas + gamma*state_value)).sum(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy iter: early break at iter: 60\n"
     ]
    }
   ],
   "source": [
    "v_pi  = v_pi_init.copy()\n",
    "\n",
    "for k in range(500):\n",
    "   v_pi_old   = v_pi.copy()\n",
    "   v_pi     = evaluate_policy_max(get_q_sa(v_pi_old))\n",
    "\n",
    "   if np.max(np.abs(v_pi-v_pi_old)) < 1e-5:\n",
    "         print(f'Policy iter: early break at iter: {k}')\n",
    "         pi = improve_policy(get_q_sa(v_pi))    # extract maxes + deals with ties\n",
    "         break\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "You took an average of 43 steps to get the frisbee\n",
      "And you fell in the hole 21.92 % of the times\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "plot_arrows_on_grid(pi>0,4)\n",
    "get_score(env, pi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
