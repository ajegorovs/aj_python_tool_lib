{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Entropy Method (CEM) RL Cartpole case\n",
    "General idea is to record samples using different policies, take best performing policies and mutate them to get better policy.\n",
    "\n",
    "In this implementation, author keeps one (state of) policy, but picks episodes where policy took more rewarding actions.\n",
    "\n",
    "Then uses optimization to improve policy. Not sure it is correct.\n",
    "\n",
    "Main idea of this implementation is the following:\n",
    "* Actions will be determined via NN composed of two Linear layers\n",
    "* Batch of episodes will be generated via current state of NN\n",
    "* Best ones selected and from them NN will learn via backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Hot Mexican\\VS_Code_Proj\\.venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch, gym\n",
    "import torch.nn as nn\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#tensorboard --logdir 'runs\\RL' --host localhost --port 8888\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hot Mexican\\AppData\\Local\\Temp\\ipykernel_19600\\4026813443.py:39: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  obs_v       = torch.FloatTensor([obs])\n",
      "c:\\Users\\Hot Mexican\\VS_Code_Proj\\.venv\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.702, reward_mean=17.2, rw_bound=18.0\n",
      "1: loss=0.688, reward_mean=22.4, rw_bound=30.0\n",
      "2: loss=0.680, reward_mean=26.7, rw_bound=27.5\n",
      "3: loss=0.656, reward_mean=28.3, rw_bound=36.0\n",
      "4: loss=0.650, reward_mean=34.4, rw_bound=45.0\n",
      "5: loss=0.646, reward_mean=38.6, rw_bound=38.5\n",
      "6: loss=0.635, reward_mean=40.9, rw_bound=47.5\n",
      "7: loss=0.641, reward_mean=37.1, rw_bound=44.5\n",
      "8: loss=0.627, reward_mean=42.9, rw_bound=53.0\n",
      "9: loss=0.599, reward_mean=41.1, rw_bound=54.5\n",
      "10: loss=0.599, reward_mean=40.9, rw_bound=48.5\n",
      "11: loss=0.602, reward_mean=56.6, rw_bound=65.5\n",
      "12: loss=0.596, reward_mean=56.1, rw_bound=70.5\n",
      "13: loss=0.593, reward_mean=60.7, rw_bound=69.0\n",
      "14: loss=0.582, reward_mean=65.5, rw_bound=73.5\n",
      "15: loss=0.571, reward_mean=63.1, rw_bound=70.5\n",
      "16: loss=0.576, reward_mean=68.2, rw_bound=83.0\n",
      "17: loss=0.584, reward_mean=60.6, rw_bound=64.5\n",
      "18: loss=0.562, reward_mean=66.8, rw_bound=77.5\n",
      "19: loss=0.583, reward_mean=61.8, rw_bound=73.0\n",
      "20: loss=0.556, reward_mean=63.5, rw_bound=67.0\n",
      "21: loss=0.560, reward_mean=86.0, rw_bound=99.5\n",
      "22: loss=0.547, reward_mean=82.7, rw_bound=80.5\n",
      "23: loss=0.571, reward_mean=70.1, rw_bound=84.5\n",
      "24: loss=0.543, reward_mean=72.8, rw_bound=78.5\n",
      "25: loss=0.553, reward_mean=71.9, rw_bound=80.0\n",
      "26: loss=0.539, reward_mean=70.4, rw_bound=85.0\n",
      "27: loss=0.564, reward_mean=70.8, rw_bound=77.5\n",
      "28: loss=0.538, reward_mean=78.1, rw_bound=90.5\n",
      "29: loss=0.527, reward_mean=78.4, rw_bound=90.0\n",
      "30: loss=0.549, reward_mean=91.2, rw_bound=99.5\n",
      "31: loss=0.547, reward_mean=93.6, rw_bound=106.0\n",
      "32: loss=0.550, reward_mean=94.0, rw_bound=100.0\n",
      "33: loss=0.526, reward_mean=95.4, rw_bound=108.5\n",
      "34: loss=0.546, reward_mean=90.3, rw_bound=101.5\n",
      "35: loss=0.524, reward_mean=93.4, rw_bound=107.5\n",
      "36: loss=0.529, reward_mean=102.0, rw_bound=120.0\n",
      "37: loss=0.534, reward_mean=107.9, rw_bound=111.0\n",
      "38: loss=0.546, reward_mean=92.1, rw_bound=105.0\n",
      "39: loss=0.535, reward_mean=110.1, rw_bound=113.0\n",
      "40: loss=0.526, reward_mean=129.7, rw_bound=134.0\n",
      "41: loss=0.545, reward_mean=115.2, rw_bound=127.0\n",
      "42: loss=0.526, reward_mean=164.9, rw_bound=183.5\n",
      "43: loss=0.537, reward_mean=164.8, rw_bound=192.5\n",
      "44: loss=0.534, reward_mean=191.2, rw_bound=203.0\n",
      "45: loss=0.531, reward_mean=168.0, rw_bound=194.5\n",
      "46: loss=0.533, reward_mean=199.9, rw_bound=228.5\n",
      "SOLVED!\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "        Takes in observation returns un-normalized action prob distribution.\n",
    "        Output is not passed though SoftMax (SM), since training loop will use nn.CrossEntropyLoss()\n",
    "        Which applies SM automatically. Testing requires explicit use of SM.\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_size, hidden_size, n_actions) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "# episode length has no set length. must store it as dynamic memory\n",
    "Episode     = namedtuple('Episode'      , field_names= ['reward'        , 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep'  , field_names= ['observation'   , 'action'])\n",
    "\n",
    "# Generator function that will indefinitely (if/when asked) return batched episodes \n",
    "def iterate_batches(env, net, batch_size):\n",
    "    # setup first 'iteration'\n",
    "    batch           = []\n",
    "    episode_reward  = 0.0\n",
    "    episode_steps   = []\n",
    "    obs             = env.reset()[0]\n",
    "    sm              = nn.Softmax(dim=1)\n",
    "    # Record an episode. Collect step info into 'episode_steps'.\n",
    "    # Loop is infinite. when function called via next() (or __next__), \n",
    "    # it executes all code before it encounters next 'yield'\n",
    "    while True:\n",
    "        obs_v       = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs   = act_probs_v.data.numpy()[0]\n",
    "        action      = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done = env.step(action)[:3]\n",
    "        episode_reward += reward\n",
    "        step        = EpisodeStep(observation=obs, action=action)\n",
    "        episode_steps.append(step)\n",
    "        # end of episode. collect episode data and append to batch. reset env and metrics.\n",
    "        if is_done:\n",
    "            e = Episode(reward=episode_reward, steps=episode_steps)\n",
    "            batch.append(e)\n",
    "            episode_reward  = 0.0\n",
    "            episode_steps   = []\n",
    "            next_obs        = env.reset()[0]\n",
    "            if len(batch) == batch_size: # when batch is full\n",
    "                yield batch     # Execute and stop here, return batched data.\n",
    "                batch = []      # On next call clear batch, proceed with a while-loop.\n",
    "        obs = next_obs\n",
    "\n",
    "\n",
    "def filter_batch(batch, percentile):\n",
    "    # Extract best episodes which are in top percentile by total reward.\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for reward, steps in batch:\n",
    "        if reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation  , steps))\n",
    "        train_act.extend(map(lambda step: step.action       , steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env         = gym.make(\"CartPole-v1\")\n",
    "    obs_size    = env.observation_space.shape[0]\n",
    "    n_actions   = env.action_space.n\n",
    "\n",
    "    net         = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective   = nn.CrossEntropyLoss()\n",
    "    optimizer   = torch.optim.Adam(params=net.parameters(), lr = 0.01)\n",
    "\n",
    "    now         = datetime.datetime.now()\n",
    "    s2          = now.strftime(\"%H_%M_%S\")\n",
    "    writer      = SummaryWriter(fr'runs/RL/{s2}',comment = '-cartpolse')\n",
    "\n",
    "    # generate a batches using current state of a netural network.\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "        # take only top percentile episodes\n",
    "        obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)                # generate logits [n_steps, n_actions]\n",
    "        loss_v = objective(action_scores_v, acts_v) # SoftMax and CrossEntropy\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        print(\"%d: loss=%.3f, reward_mean=%.1f, rw_bound=%.1f\" % (\n",
    "        iter_no, loss_v.item(), reward_m, reward_b))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "\n",
    "        if reward_m > 199:\n",
    "            print(\"SOLVED!\")\n",
    "            break\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy function that explains how batches are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_samples(func, inp):\n",
    "    # init data\n",
    "    sample = []\n",
    "    while True:\n",
    "        # do some stuff\n",
    "        sample.append(func(inp))\n",
    "        # enough samples?\n",
    "        if len(sample) == 10:\n",
    "            yield sample    # return and stop\n",
    "            # Start from here. Reset data. \n",
    "            sample = []     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
