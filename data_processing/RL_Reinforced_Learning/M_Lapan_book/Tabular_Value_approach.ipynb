{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilties\n",
    "see [intro_to_stats_for_RL.ipynb](../../../../educational/statistics_sets/intro_to_stats_for_RL.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL Resources:\n",
    "*   https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf\n",
    "*   https://www.cs.mcgill.ca/~jpineau/files/mfard-jair11.pdf\n",
    "*   https://gibberblot.github.io/rl-notes/single-agent/MDPs.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Goal of Reinforcement Learning (RL) decision model is to maximize accumulated reward when 'released' in random state of environment.\n",
    "*   Classic way to approach this RL problem is to use probabilistic/stochastic (Finite) Markov decision process (Finite MDP):<br><br>\n",
    "    <i>Markov Processes (MP)</i> describes a problem only in terms of states $S = \\{s_0, s_1,\\dots\\}$ and probabilities to transition from some state $s$ to state $s^\\prime$<br>\n",
    "    $$P(s^\\prime|s) = Pr\\{S_{next} = s^\\prime | S_{prev} = s\\}$$\n",
    "    MDP is an extension of MP that introduces actions $a$ and rewards $r$ (or $R$).<br>\n",
    "    Note: Finite MDP approach is model based - all transitions and rewards are known beforehand.\n",
    "    In simple terms we can say that \"action\" is used to make a transition $s \\rightarrow s^\\prime$.<br>\n",
    "    There can be multiple actions available for same transitions which yield different rewards $R = R(a,s,s^\\prime)$<br>\n",
    "    *   Note: Reward $r$ is awarded after 'stepping' into state $s$. \"After\" is important, since \"thinking\" of visiting state $s$ does not reward anything.\n",
    "    *   Each state might have limited set of available actions $A_s = \\{a_{s1},a_{s2},\\dots\\}$.<br>\n",
    "        According to Sutton, we might collect all actions in big set of actions $A = \\{a_0,a_1,\\dots\\}$ and think $A_s$ as being a subset of $A$: $A_s = \\{a_5,a_9,\\dots\\} \\subseteq A$\n",
    "    \n",
    "*   Policy $\\pi$ is probabilistic model that decides which actions $a$ to take from state $s$. This is being learned by an agent.\n",
    "    $$\\pi(a|s) = Pr\\{A_t = a| S = s\\} \\ (\\forall \\ (s,a) \\subset S \\times A)^\\star$$\n",
    "    Policies of picking action at random or to use only one specific action are perfectly valid policies. Although following such policies, in general, will not yeld maximal reward.<br>\n",
    "    In theory, if action $a_i$ is not available from state $s_j$, then agent should learn $\\pi(a_i|s_j) = 0$. Actual response of a system (environment)- whether to transition or not, to penalize attempt, depends on an environment.\n",
    "*   discounted ($\\gamma$) future reward $G$ (starting from state $s$ and guided by policy $\\pi$):<br>\n",
    "    is a weighted sum of rewards ($r_t$) accumulated during all episode time steps $t = \\{1,2,\\dots,T\\}$, where $T$ is a time at which terminal state is reached<br>\n",
    "    Decision 'trajectory' is probabilistic and decided by a policy $\\pi$.<br>\n",
    "    There are many possible 'trajectories' to a terminal state/-es. Event the reward $r_0$ might be different for different episodes.<br>\n",
    "    Sutton proposes that we use top limit if $\\infty$ not only for 'endless' episodes, but also for episodes with terminal state, given that all rewards post-terminal state are zero.\n",
    "    $$G_s = G(s,\\gamma,\\{r_1, r_2,\\dots\\}) = r_0 + \\gamma \\ r_1 + \\dots + \\gamma^T \\ r_T + 0 + 0 + \\dots   = \\sum_{t=0}^\\infty \\gamma^t \\ r_t $$\n",
    "    $$\\gamma = [0,1)$$\n",
    "    Discounting $\\gamma$ forces 'shorter' foresight into future, which is needed for 'endless' problems.\n",
    "*   Value of a state $V_\\pi(s)$:<br>\n",
    "    is an expected ($\\mathbb{E}$) discounted reward, if we start from state $s$ and make all decisions based on current policy $\\pi$\n",
    "    \n",
    "    $$V_\\pi(s) = \\mathbb{E}_\\pi \\left[ G_s \\right] = \\mathbb{E} \\left[\\sum_{t=0}^\\infty \\gamma^t \\ r_t   | S_0 = s, \\pi \\right]$$\n",
    "    <u>Bad policies result in low expected rewards!</u><br>\n",
    "* Optimal policy $\\pi^*$ (or multiple policies):<br>\n",
    "    Is a policy that produces highest expected reward $V_{{\\pi^*}}(s)$ for <u>all</u> possible states $s$.\n",
    "    $$V_{{\\pi^*}}(s) = \\underset{\\pi}{\\mathrm{max}} \\ V_\\pi(s) $$\n",
    "    Translation/repeat: by following an optimal policy you will achieve maximal expected reward from any state $s$.\n",
    "\n",
    "\n",
    "$^\\star S \\times A$ is a cartesian product that yields all pairs of $(s_j,a_i)$.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete-time Markov chain (DTMC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DTMC is a sequence of random variables $X_1, X_2, \\dots$ with the <i>Markov property</i>.\n",
    "\n",
    "Lets unpack this below:\n",
    "* Random variable $X$ during 'experiment/sampling' can return one value from a collection of sample space $S$.<br>\n",
    " Return value is picked at random with a given probability distribution.<br><br>\n",
    "For example $X$ can be a result of a coin toss.\n",
    "    * Sample space is a set of two outcomes heads and tails $S = \\{H,T\\}$\n",
    "    * Probability of each outcome can be arbitrary, but all probabilities should sum to 1. We can consider unbiased coin toss so\n",
    "    $$P(X = H) = p_H = P(X = T) = p_T = \\frac{1}{2}$$\n",
    "    $$p_H + p_T = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sequence of random variables $X_1, X_2, \\dots, X_n$ is describes a series of identical 'experiments' conducted $n$ times.\n",
    "    * Index $1,2,\\dots, n$ helps to differentiate each individual 'experiment'.\n",
    "    * For our coin toss example, sequence $X_1, X_2$ can unfold in various outcomes: $\\{(H,H),(H,T),(T,H),(T,T)\\}$\n",
    "    * If coin toss is unbiased, probability of each outcome is equals.<br>\n",
    "        In the extreme if $p_H = 1$ (so $p_T = 1 - p_H = 0$), only possible outcome is $(H,H)$\n",
    "    * In general to achieve result $(H,H)$, due to independence of coin tosses we calculate probability of this outcome as<br>\n",
    "        $p_H \\times p_H$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Markov property tells that result of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In general, its a model of random walk on graph with some states $X_i \\in S$. \n",
    "\n",
    "Example sequence\n",
    "Randomness/stochastic nature comes in from non-deterministic transitions from one state to another.\n",
    "\n",
    "Markov property imposes restrictions on a model which simplifies analysis. Namely:\n",
    "1. Limited history- probability of transition to next state only depends on current state and not previous states.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/ad/Markov_Decision_Process.svg\" alt=\"image info\" style=\"background-color:white;padding:0px;\" width=\"200\" height=\"200\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value `V` of state `s` $V(s)$\n",
    "\n",
    "Value of state is formally defined as expected total reward that can be obtained from some state `s`.\n",
    "\n",
    "<i> To deal with infinite/looping trajectories or to enforce short time memory we can apply decreasing discount factor $\\gamma \\in [0,1)$ to each next obtained reward.</i>\n",
    "\n",
    "$$V(s) = \\mathbb{E} \\left[\\sum_{t=0}^\\infty \\gamma^t \\ r_t   | S = s\\right]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets consider a case with initial state $s_0$ with two nearby states $S = \\{s_1,s_2\\}$ upon reaching which episode terminates and agent receives a reward from $R = \\{r_1,r_2\\}$.<br>\n",
    "Probability to transition from state $s_0$ to state $s_1 = p_{0 \\rightarrow 1}$ and similarly with $p_{0 \\rightarrow 2}$.<br>\n",
    "We dont use self-loops $p_{0 \\rightarrow 0}$ since it would introduce infinite episode.\n",
    "\n",
    "Expected reward is a probability-weighted sum of rewards:\n",
    "\n",
    "$$V(s_0) = \\mathbb{E} \\left[r_{t = 0} \\right] = p_{0 \\rightarrow 1} \\ r_1 + p_{0 \\rightarrow 2} \\c r_2  = \\sum_{s^\\prime \\in S} p_{0 \\rightarrow s^\\prime} \\ r_{s^\\prime}$$\n",
    "of course given that \n",
    "$$\\sum_{s^\\prime \\in S} p_{0 \\rightarrow s^\\prime} = 1$$\n",
    "Depending on transition probabilities expected reward may vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we add additional states $S_2 = \\{ s_3, s_4\\}$, one-way connected to $s_2$, agent can 'explore further' and accumulate more reward past $r_2$\n",
    "$$V(s_2) = p_{2 \\rightarrow 3} \\ r_3 + p_{2 \\rightarrow 4} \\ r_4 = \\sum_{s^\\prime \\in S_2} p_{2 \\rightarrow s^\\prime} \\ r_{s^\\prime} $$\n",
    "\n",
    "$$V(s_0) = \\mathbb{E} \\left[r_{t=0} + \\gamma \\ r_{t=1} \\right] = p_{0 \\rightarrow 1} \\ r_1 + p_{0 \\rightarrow 2} \\ [r_2  + \\gamma \\ p_{2 \\rightarrow 3} \\ r_3 + \\gamma \\ p_{2 \\rightarrow 4} \\ r_4]\n",
    "= p_{0 \\rightarrow 1} \\ r_1 + p_{0 \\rightarrow 2} \\ [r_2  + \\gamma \\ V(s_2)]\n",
    "$$\n",
    "You can observe a recursion of unfolding time steps. It is heavily used in <i>Bellman equations of optimality</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman equations of optimality\n",
    "Bellman modifies definition of $V(s)$ with introduction of `action` $a_s = \\pi(s)$ and a `policy` $\\pi$, which defines transition probabilities.<br> \n",
    "Actions are issued by following a policy and they brings agent from one state to another.<br> \n",
    "If agent is guided solely by policy, its rewards and thus value of each state will depend on how successful this policy is.<br>\n",
    "Bellman equation of arbitrary policy shows decoupled `now` reward and `future` reward.<br>\n",
    "$$ V(s) = \n",
    "\\mathbb{E} \\left[r_{t=0} + \\sum_{t=1}^\\infty \\gamma^t \\ r_t  \\right] \n",
    "\\rightarrow \n",
    "V^\\pi(s) = \\mathbb{E}_\\pi \\left[R(s,a = \\pi[s]) + \\gamma \\ V^\\pi (s^\\prime) \\right]$$\n",
    "* `Now` reward $R(s,a = \\pi[s])$ is only concerned about reward gained by performing action $a_s$ based on current policy, that brings you from state $s$ to state $s^\\prime$<br>\n",
    "* Discounted future reward $\\gamma \\ V^\\pi (s^\\prime)$ is defined via Bellman's equation recursively\n",
    "\n",
    "Bellman's condition of optimality states (rather obviously), that maximal reward can be reached using `optimal` policy $\\pi^*$\n",
    "\n",
    "Value of a state $V^\\pi(s)$ depends on a policy $\\pi$, and so does reward gathered from state $s: R(s,a_s) + $ .<br>\n",
    "Action is required in order to define an `optimal` policy $V^{\\pi *}$. Agent following $V^{\\pi *}$ selects optimal actions and achieves most reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
