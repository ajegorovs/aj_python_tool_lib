{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State and action values\n",
    "It is a brief overview of this topic. \n",
    "\n",
    "Newest edition of book by Sutton & Bartol gives a good explanation, but beginning is weak.\n",
    "\n",
    "https://www.deeplearningwizard.com/deep_learning/deep_reinforcement_learning_pytorch/bellman_mdp/#bellman-expectation-equations\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important probabilities\n",
    "\n",
    "<i>Parts regarding probabilities are more flashed out in [Meaning_of_Psasr_probability_matrix.ipynb](data_processing/neural_networks/RL_Reinforced_Learning/Meaning_of_Psasr_probability_matrix.ipynb).\n",
    "\n",
    "Here i change some stuff because i now have a better understanding of topic and notation.</i>\n",
    "\n",
    "Join probability of getting reward can be written as\n",
    "$$p(s,a,s^\\prime,r) = p(s) \\cdot p(a|s) \\cdot p(s^\\prime|s,a) \\cdot p(r|s,a,s^\\prime)$$\n",
    "Following part is controlled by a system (see link how to 'condition away' $p(s) \\cdot p(a|s)$ )\n",
    "$$p(s^\\prime, r|s,a) = p(s^\\prime|s,a) \\cdot p(r|s,a,s^\\prime)$$\n",
    "Following is referred as policy, which describes how 'agent' behaves\n",
    "$$\\pi(a|s) = p(a|s)$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent's policy (Behavior rules ) $\\pi(a|s)$\n",
    "\n",
    "Policy may be uniform random (p is proportional to number available actions at $s$):\n",
    "$$\\pi(a|s) = \\frac{1}{|A(s)|}$$\n",
    "Or it can be deterministic. Say only action $K$ is relevant:\n",
    "$$ \\pi(a_i|s)= \\begin{cases}\n",
    "    1; \\ i = K\\\\\n",
    "    0; \\ i \\neq K\n",
    "    \\end{cases}$$\n",
    "\n",
    "Or anything in-between\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected immediate reward after reaching state $s,a,s^\\prime \\rightarrow r(s,a,s^\\prime)$\n",
    "\n",
    "Stochastisity of a system might give rewards randomly drawn from some distribution (for specific tuple $(s,a,s^\\prime)$).\n",
    "\n",
    "In order to compute expected reward $\\mathbb{E}[R(s,a,s^\\prime)]$ we should compute weighted sum:\n",
    "$$r(s,a,s^\\prime) = \\mathbb{E}[R(s,a,s^\\prime)] = \\sum_r  r\\cdot p(r|s,a,s^\\prime)$$\n",
    "_Here $R(s,a,s^\\prime)$ is just a random variable._\n",
    "\n",
    "We can express $p(r|s,a,s^\\prime)$ by using system dynamics chain rule equation\n",
    "$$p(r|s,a,s^\\prime) = \\frac{p(s^\\prime,r|s,a)}{p(s^\\prime|s,a)}$$\n",
    "\n",
    "So expected immediate reward at $s^\\prime$ is:\n",
    "$$\\boxed{r(s,a,s^\\prime) = \\frac{\\sum_r  r\\cdot p(s^\\prime,r|s,a)}{p(s^\\prime|s,a)}}$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p(s^\\prime|s,a)$ is a marginalization of $p(s^\\prime,r|s,a)$ over $r$\n",
    "$$p(s^\\prime|s,a) = \\sum_r p(s^\\prime,r|s,a)$$\n",
    "<i>It is so because, by conditioning, $p(s^\\prime,r|s,a)$ is normalized:</i>\n",
    "$$\\sum_r \\sum_{s^\\prime} p(s^\\prime,r|s,a) = 1$$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected immediate reward for using action $a$ in state $s,a \\rightarrow \\hat q(s,a)$\n",
    "System might be stochastic in terms of target states which can be reached by using action $a$ in state $s$.<br>\n",
    "We have to calculate expected reward $\\mathbb{E}[r(s,a,s^\\prime)]$ based on probability that agent will transition to some state $s^\\prime$ by using action $a$:\n",
    "\n",
    "$$\\hat q(s,a) = \\mathbb{E}[r(s,a,s^\\prime)] = \\sum_{s^\\prime \\in S} p(s^\\prime | s,a) \\cdot r(s,a,s^\\prime)$$\n",
    "We have calculated $r(s,a,s^\\prime)$ previously\n",
    "$$\\hat q(s,a) = \\sum_{s^\\prime \\in S} p(s^\\prime | s,a) \\cdot \\frac{\\sum_{r \\in R} r \\cdot p(s^\\prime, r|s,a)}{p(s^\\prime | s,a) } = \\sum_{r \\in R} \\sum_{s^\\prime \\in S} r \\cdot p(s^\\prime, r|s,a) $$\n",
    "$$\\boxed{\\hat q(s,a) =  \\sum_{r \\in R} \\sum_{s^\\prime \\in S} r \\cdot p(s^\\prime, r|s,a) }$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected immediate reward for being in state $s\\rightarrow \\hat v_{\\pi}(s)$\n",
    "Expected immediate reward $\\mathbb{E}_\\pi[r(s,a)]$ __depends on policy__ $\\pi(a|s)$ agent follows. \n",
    "\n",
    "For example agent can have a policy of always picking 0 reward, thus expected immediate reward will be 0.\n",
    "\n",
    "_This is the reason why we specify subscript $\\pi$, to differentiate rewards achieved in a same system but using different policies._\n",
    "\n",
    "$$\\boxed{\\hat v_\\pi(s) = \\mathbb{E}_\\pi[r(s,a)] = \\sum_a \\pi(a|s) \\cdot \\hat q(s,a)}$$\n",
    "\n",
    "We plug in definition of $\\hat q(s,a)$:\n",
    "\n",
    "$$\\boxed{\\hat v_\\pi(s) = \\sum_{a \\in A} \\pi(a|s) \\sum_{s^\\prime \\in S}\\sum_{r \\in R}r \\cdot p(s^\\prime, r|s,a)}$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman equations\n",
    "\n",
    "### Introduction of episode and time\n",
    "Bellman equations allows to give an estimate of all future rewards for a particular state $s$ (or state-action pair).\n",
    "\n",
    "This implies that we have a trajectory of transitions along which rewards are gathered.\n",
    "\n",
    "Each transition is assumed to happen on specific time step $t$. We introduce notation for event trajectory:\n",
    "\n",
    "$$S_{t = 0} \\rightarrow A_{t = 0} \\rightarrow R_{t = 1} \\rightarrow S_{t = 1} \\rightarrow \\dots \\rightarrow S_{t = T}= \\{S_0,A_0,R_1,S_1,\\dots, S_T\\}$$\n",
    "\n",
    "Where $T$ is trajectory termination time, if it exists.\n",
    "***\n",
    "### Discounted future (cumulative) reward\n",
    "Lets examine a such singlet trajectory/experiment/trial.\n",
    "\n",
    "For some time step $t$ and state $S_t$ we can calculate cumulative rewards $G_t$.\n",
    "\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{T-t-1} R_T = \\sum_{i = t+1}^{T} \\gamma^{i-t-1} R_i$$\n",
    "Where $\\gamma $ is discount, which is defined to deal with infinite episodes.\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Deriving power offsets:\n",
    "$$\\gamma^0 R_{t+1} \\rightarrow \\gamma^{0 + \\alpha} R_{t+1 + \\alpha} = \\gamma^{\\alpha} R_{t+1 + \\alpha}$$ \n",
    "$$ t+1 + \\alpha = T \\rightarrow \\alpha = T - (t+1)$$\n",
    "$$\\gamma^{\\alpha} R_T \\rightarrow \\gamma^{T- t - 1} R_T$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Move sum to 0-start:\n",
    "$$G_t = \\sum_{i = t+1}^{T} \\gamma^{i-t-1} R_i$$\n",
    "$$j  = i - (t+1)\\rightarrow j_{min} = 0 \\rightarrow i_{min} = t+ 1$$\n",
    "$$ i_{max} = T\\rightarrow j_{max} = T - t - 1$$\n",
    "$$i = j + t + 1$$\n",
    "$$G_t= \\sum_{j = 0}^{T - t - 1} \\gamma^j R_{t + j + 1}$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected discounted cumulative reward at state $s$ (and $(s,a)$ pair)\n",
    "We can define state value $v_{\\pi}(s)$- an expected discounted cumulative reward at state $s$ as:\n",
    "\n",
    "$$v_{\\pi}(s) = \\mathbb{E}_\\pi [G_t|S_t = s] = \\mathbb{E}_\\pi \\bigg[ \\sum_{j = 0}^{\\infty} \\gamma^j R_{t + j + 1}|S_t = s\\bigg]$$\n",
    "Here we changed upper bound for convenience, given that all rewards past terminal state are 0.\n",
    "\n",
    "Also we can define action value $q_{\\pi}(s,a)$ which includes conditioning that we have taken action $a$:\n",
    "\n",
    "$$q_{\\pi}(s,a) = \\mathbb{E}_\\pi [G_t|S_t = s, A_t = a] = \\mathbb{E}_\\pi \\bigg[ \\sum_{j = 0}^{\\infty} \\gamma^j R_{t + j + 1}|S_t = s, A_t = a\\bigg]$$\n",
    "Impact of conditioning in these definitions is not obvious. But clearly pre-selecting action will result in deterministic first reward.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoupling immediate rewards and future rewards\n",
    "We can consider two cases:\n",
    "$$G_t = R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{T-t-1} R_T$$\n",
    "and $G_{t+1}$:\n",
    "$$G_{t+1} = R_{t+2} + \\gamma R_{t+3} + \\dots + \\gamma^{T-(t+1)-1} R_T$$\n",
    "multiply by $\\gamma$\n",
    "$$\\gamma G_{t+1}  = \\gamma R_{t+2} + \\dots + \\gamma\\gamma^{T-t-2} R_T = \\gamma R_{t+2} + \\dots + \\gamma^{T-t-1} R_T$$\n",
    "We see that we can rewrite $G_t$ as:\n",
    "$$G_t = R_{t+1} + \\underbrace{\\gamma R_{t+2} + \\dots + \\gamma^{T-t-1} R_T}_{\\gamma G_{t+1}} = R_{t+1} + \\gamma G_{t+1}$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So value state can be defined as:\n",
    "$$v_{\\pi}(s) = \\mathbb{E}_\\pi [G_t|S_t = s] = \\mathbb{E}_\\pi [R_{t+1} + \\gamma G_{t+1}|S_t = s]$$\n",
    "Expression can be split via linearity, but conditioning/causality should carry over. \n",
    "$$v_{\\pi}(s) = \\mathbb{E}_\\pi [R_{t+1}|S_t = s] + \\mathbb{E}_\\pi [\\gamma G_{t+1}|S_t = s]$$\n",
    "_NOTE: Interesting aspect is that for a single episode, due to Markov property, conditioning on history longer than 1 step makes no difference._\n",
    "\n",
    "Left part is our defined immediate reward\n",
    "$$\\mathbb{E}_\\pi [R_{t+1}] = \\sum_{a \\in A} \\pi(a|s) \\sum_{s^\\prime \\in S}\\sum_{r \\in R}r \\cdot p(s^\\prime, r|s,a)$$\n",
    "One can rewrite it via\n",
    "$$r(s,a,s^\\prime) = \\frac{\\sum_r  r\\cdot p(s^\\prime,r|s,a)}{p(s^\\prime|s,a)}$$\n",
    "as\n",
    "$$\\mathbb{E}_\\pi [R_{t+1}] = \\sum_{a \\in A} \\pi(a|s) \\sum_{s^\\prime \\in S} p(s^\\prime|s,a) \\cdot r(s,a,s^\\prime) $$\n",
    "Right part should be weighted as for all $(a,s^\\prime)$\n",
    "$$\\mathbb{E}_\\pi [ \\gamma G_{t+1} |S_{t} = s] = \\sum_{a \\in A} \\pi(a|s) \\sum_{s^\\prime \\in S} p(s^\\prime|s,a) \\cdot  \\gamma v_{\\pi}(s^\\prime)$$\n",
    "\n",
    "<i>Note: Notation is a bit wonky. But its clear that future cum-rewards are captured in weighted discounted sum of next state values.</i>\n",
    "\n",
    "So $$\\boxed{v_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s) \\sum_{s^\\prime \\in S} p(s^\\prime|s,a) \\cdot \\bigg[ r(s,a,s^\\prime) + \\gamma v_{\\pi}(s^\\prime) \\bigg]}$$\n",
    "\n",
    "By using definition for $r(s,a,s^\\prime)$ and\n",
    "\n",
    "$$p(s^\\prime|s,a) = \\sum_r p(s^\\prime,r|s,a)$$ \n",
    "\n",
    "we can express $v_{\\pi}(s)$ in terms of $p(s^\\prime,r|s,a)$\n",
    "\n",
    "\n",
    "$$\\boxed{v_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s) \\sum_{s^\\prime, r} p(s^\\prime,r|s,a) \\cdot \\bigg[ r + \\gamma v_{\\pi}(s^\\prime) \\bigg]}$$\n",
    "\n",
    "Definition for $q_{\\pi}(s))$ is analogues to $\\hat q_{\\pi}(s)$ in that summation over $a$ is absent\n",
    "\n",
    "$$\\begin{matrix}\n",
    "\\boxed{q_{\\pi}(s) = \\sum_{s^\\prime \\in S} p(s^\\prime|s,a) \\cdot \\bigg[ r(s,a,s^\\prime) + \\gamma v_{\\pi}(s^\\prime) \\bigg]}\n",
    "&\n",
    "\\boxed{q_{\\pi}(s) = \\sum_{s^\\prime, r} p(s^\\prime,r|s,a) \\cdot \\bigg[ r + \\gamma v_{\\pi}(s^\\prime) \\bigg]}\n",
    "\\end{matrix}$$\n",
    "\n",
    "So relation between $v_{\\pi}(s)$ and $q_{\\pi}(s)$ is elementary:\n",
    "$$\\boxed{v_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s) \\cdot q_{\\pi}(s) }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman optimality equations: optimal state and action value\n",
    "\n",
    "Optimal implies that we drop stochastisity and select transitions with p=1 that maximize reward.\n",
    "$$\\pi(a|s) \\rightarrow \\pi_\\ast(a|s) \\text{ or } a_\\ast = \\pi_\\ast(s)$$\n",
    "_(depending on the context)_\n",
    "\n",
    "Optimal state value $v_\\ast$\n",
    "$$v_\\ast = \\underset{a}{\\mathrm{max}} \\ q_{\\pi}(s,a) = q_{\\pi}(s,a = \\pi_\\ast(s))$$\n",
    "\n",
    "Optimal state value $q_\\ast$\n",
    "\n",
    "$$q_{\\ast}(s,a) = \\mathbb{E}\\bigg[ R_{t+1} + \\gamma \\cdot \\underset{a^\\prime}{\\mathrm{max}} \\ q_{\\ast}(s^\\prime,a^\\prime)\\bigg|S_t = a, A_t = a \\bigg]$$\n",
    "Which reduces to hops from one state-action to another"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
