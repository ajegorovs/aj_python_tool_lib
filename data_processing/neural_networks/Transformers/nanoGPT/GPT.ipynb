{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_str = open(os.path.join('data_processing','media','tinyshakespeare.txt'), 'r').read() # should be simple plain text file\n",
    "character_all = set(data_str)\n",
    "SIZE_VOCAB  = len(character_all)\n",
    "# data = re.sub(r'[^a-zA-Z\\s]', '', data) # remove non-alphabet\n",
    "# seq_length = 8\n",
    "# words_all = data.split()\n",
    "# words = list(set([w for w in words_all if len(w) == seq_length]))\n",
    "# chars_all = [c for w in words for c in w] #[*data]\n",
    "# chars = list(set(chars_all))  #list(set(data))\n",
    "# data_size, vocab_size = len(words), len(chars)\n",
    "# print('data has %d words of size %d, and %d unique characters.' % (len(words), seq_length, vocab_size))\n",
    "# 'Chars: ' + ' '.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_encoded = [32, 39, 12, 12, 40, 38, 33, 40, 21, 12, 51, 42]\n",
      "test_str = 'Hello world!', decode(encode(test_str)) == test_str = True\n"
     ]
    }
   ],
   "source": [
    "d_char2ID = {c:i for i,c in enumerate(character_all)}\n",
    "d_ID2char = {i:c for i,c in enumerate(character_all)}\n",
    "encode = lambda string: [d_char2ID[s] for s in string]\n",
    "decode = lambda code: ''.join([d_ID2char[i] for i in code])\n",
    "test_str = 'Hello world!'\n",
    "_encoded = encode(test_str)\n",
    "\n",
    "print(f'{_encoded = }\\n{test_str = }, {decode(encode(test_str)) == test_str = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(data_str), dtype=torch.long)\n",
    "train_part = int(0.9* len(data))\n",
    "data_train = data[:train_part]\n",
    "data_valid = data[train_part:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[40, 14,  7, 53, 24,  8, 38,  3],\n",
       "         [40, 21, 51, 46,  7, 53,  9, 38],\n",
       "         [12,  2, 24, 22, 16, 23, 52, 14]]),\n",
       " tensor([[14,  7, 53, 24,  8, 38,  3, 62],\n",
       "         [21, 51, 46,  7, 53,  9, 38,  3],\n",
       "         [ 2, 24, 22, 16, 23, 52, 14, 39]]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(69)\n",
    "SIZE_CONTEXT = 8\n",
    "SIZE_BATCH   = 3\n",
    "\n",
    "def get_batch(which):\n",
    "    \n",
    "    data_which = data_train if which == 'train' else data_valid\n",
    "    data_big = torch.zeros(size=(SIZE_BATCH, SIZE_CONTEXT + 1), dtype = torch.long, device=device)\n",
    "    # if len(data_which) = size_context + 1 -> index_start = randint(0,1) -> only 0\n",
    "    index_start = torch.randint(0, len(data_which) - SIZE_CONTEXT, size = (SIZE_BATCH,))  \n",
    "    for batch, i in enumerate(index_start):\n",
    "        data_big[batch] = data_which[i:i+SIZE_CONTEXT+1]\n",
    "    x, y = data_big[:,:-1], data_big[:,1:]\n",
    "    return x,y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"W'? Sqjv3&QRXzTeb$N'suq;QcYiCea3U:SUqqiJVWdt TdvBMtPH$QY A;, ZSXq\\nz;\\nHtP ctMt h&s;l.FvuAwI\\nzGIbak!B'f\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        # table retrieves vocab encodings. SEQ-> [SEQ_LEN, VOCAB]. and its batched.\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self,  IDs, targets = None):\n",
    "        logits = self.token_embedding_table(IDs) # [BATCH, SEQ_LEN, VOCAB]\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            BATCH, SEQ_LEN, VOCAB = logits.size()\n",
    "            logits = logits.view(BATCH*SEQ_LEN, VOCAB)  \n",
    "            targets = targets.reshape(BATCH*SEQ_LEN)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, IDs, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(IDs)\n",
    "            # take last in SEQ\n",
    "            logits = logits[:,-1,:]             \n",
    "            # prob-normalize\n",
    "            probs = F.softmax(logits, dim = -1) \n",
    "            # get ID from prob\n",
    "            sample_probs = torch.multinomial(probs, num_samples=1)  \n",
    "            # add to SEQ dimension\n",
    "            IDs = torch.cat((IDs, sample_probs), dim = 1)\n",
    "        return IDs\n",
    "\n",
    "\n",
    "\n",
    "mod = BigramLanguageModel(SIZE_VOCAB)\n",
    "decode(mod.generate(IDs = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(mod.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.479257345199585\n",
      "Wind mathe m w cht d's meswit t bu I de,\n",
      "Tovil;\n",
      "D whiseng.\n",
      "Anove,\n",
      "K:\n",
      "Th.\n",
      "Theg ath w, mee s whinuth he\n"
     ]
    }
   ],
   "source": [
    "SIZE_BATCH = 32\n",
    "\n",
    "for step in range(1):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = mod(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())\n",
    "print(decode(mod.generate(IDs = torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
