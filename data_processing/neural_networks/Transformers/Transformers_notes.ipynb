{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from misc_tools.print_latex import print_tex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Pre-trained Transformer (GPT) networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is GPT\n",
    "GPT is a transformer neural network architecture that is used in sequence analysis. \n",
    "Previous architectures such as RNN, and in extension LSTM, work in cyclic-unrolling fashion, by analyzing single element sequence at a time and some encoded representation of previous context. RNN approach shows difficulty in keeping track of long-range (history) dependencies due in learning \"vanishing\" or \"exploding\" gradients, which simply means that first sequence entry affects current entry though multiple unfoldings of RNN. Cyclic RNN architecture also prevents effective parallelization, since it is basically a for-loop. \n",
    "GPT architecture introduces a method that addresses issue with long-range dependencies by introducing \"attention mechanism\", which is also parallelizable due to matrix multiplication implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic idea of sequence prediction in GPT\n",
    "General steps of data prediction via GPT is the following:\n",
    "1. Transforming input data entries to vectors (called an `embedding`) which, by construction,<br>\n",
    "capture relations between them (and others of same input type);\n",
    "\n",
    "2. Assign an importance or weight for each embedding, via `attention-mechanism`, <br>\n",
    "based on context of position-neighbor embeddings;\n",
    "\n",
    "3. Performing weighted sum of embedding (called `aggregation`) using weights from step 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT requirements for \"robustness\"\n",
    "1. Context length can be as low as `1` and as high as some `MAX_CONTEXT_LENGTH ` = $N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Dummy problem) Aggregation of all sequence elements via weighted sum\n",
    "\n",
    "Lets consider that for our sequence of length $N$ we can find and store embeddings matrices $H$ as rows. \n",
    "$$H = \n",
    "\\begin{bmatrix}\n",
    "\\vec{h}_0^T \\\\\\vec{h}_1^T  \\\\ \\vdots \\\\ \\vec{h}_{N - 1}^T\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Suppose we have an array of weights $\\vec{w}^T$\n",
    "$$\n",
    "\\vec{w}^T =\n",
    "\\begin{bmatrix}\n",
    "w_0 & w_1 & \\dots w_{N - 1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Given equation $\\vec{y}^T = \\vec{x}^T M$, we know that after right vector-matrix multiply, $\\vec{y}^T$ will contain a linear combination of rows of $M$.\n",
    "\n",
    "So, in our case:\n",
    "$$\n",
    "\\vec{h}^{T\\prime} = \n",
    "\\vec{w}^T H = \n",
    "\\begin{bmatrix}\n",
    "w_0 & w_1 & \\dots w_{N - 1}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\vec{h}_0^T \\\\\\vec{h}_1^T  \\\\ \\vdots \\\\ \\vec{h}_{N - 1}^T\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\sum_{i=0}^{N-1} w_i \\vec{h}_i^T\n",
    "$$\n",
    "$$\n",
    "\\mathrm{dim(\\vec{h}^{T\\prime})} = \\mathrm{dim(\\vec{h}_i^T)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\vec{w}^{T} = \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}; \\ H = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & -1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\vec{h}^{T\\prime} = \\vec{w}^TH = \\begin{bmatrix} 2 & -1 & 1 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\vec{h}^T = w_0 \\vec{h}_0^T + w_1 \\vec{h}_1^T + w_2 \\vec{h}_2^T  = \\begin{bmatrix} 2 & -1 & 1 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h1, h2, h3 = H = np.array([[2,0,0],[0,-1,0],[0,0,1]])\n",
    "w1, w2, w3 = wT = np.array([1,1,1])\n",
    "print_tex(r'\\vec{w}^{T} = ', wT, r'; \\ H = ', H)\n",
    "print_tex(r'\\vec{h}^{T\\prime} = \\vec{w}^TH = ', wT @ H)\n",
    "print_tex(r'\\vec{h}^T = w_0 \\vec{h}_0^T + w_1 \\vec{h}_1^T + w_2 \\vec{h}_2^T  = ', w1*h1 +  w2*h2 + w3*h3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>This dummy problem is not practical, but shows that given correct weights and embeddings, information can be collected and passed between elements of a sequence. </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention mechanism\n",
    "Attention mechanism is aimed at determining weights for embedding aggregation\n",
    "### Query (Q) and Key (K) matrices\n",
    "\n",
    "Weight for each individual embedding is generated from analyzing interaction of this embedding and its neighbors.\n",
    "\n",
    "Interaction (or attention) for a pair is not symmetric.\n",
    "\n",
    "For example in scope of natural language processing (NLP) a sentence, which is a sequence of words, has a specific structure: <i>subject $\\rightarrow$ verb $\\rightarrow$ object</i>. Thus, pairs (subject, verb) and (verb, object) are more frequent in text, so are highly 'correlated'.\n",
    "\n",
    "This asymmetric nature requires for embeddings $\\vec{h}_i$ to have two descriptors: \n",
    "1. key $\\vec{k}_i$ - 'what am I'\n",
    "2. query $\\vec{q}_i$ - 'who I am looking for'\n",
    "$$\n",
    "\\vec{q}_i =f_q(\\vec{h}_i) = \\ \\stackrel{query}{\\leftarrow} \\  \\vec{h}_i \\ \\stackrel{key}{\\rightarrow} \\ = f_k(\\vec{h}_i) = \\vec{k}_i\n",
    "$$\n",
    "\n",
    "Strength of connection of $\\vec{h}_i$ with its neighbors is determined by a dot product of its `query` $\\vec{q}_i$ and all neighbor's `key` $\\vec{k}_i$.\n",
    "\n",
    "$$Q=\n",
    "\\begin{bmatrix}\n",
    "\\vec{q}_0^T \\\\\\vec{q}_1^T  \\\\ \\vdots \\\\ \\vec{q}_{N - 1}^T\n",
    "\\end{bmatrix}\n",
    "; \\ \n",
    "K = \n",
    "\\begin{bmatrix}\n",
    "\\vec{k}_0^T \\\\\\vec{k}_1^T  \\\\ \\vdots \\\\ \\vec{k}_{N - 1}^T\n",
    "\\end{bmatrix}\n",
    "; \\ K^T=\n",
    "\\begin{bmatrix}\n",
    "\\vec{k}_0 & \\vec{k}_1 & \\dots & \\vec{k}_{N - 1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This can be packed into a matrix $A$\n",
    "$$\n",
    "A = Q K^T \n",
    "; \\ \\mathrm{dim(A)} =\n",
    "[ N \\times N]\n",
    "$$\n",
    "\n",
    "$$A_{i,j} = \\sum_{f=0}^{F-1} Q_{i,f} (K^T)_{f,j }= \\sum_{f=0}^{F-1} Q_{i,f} K_{j,f} = \\vec{q}_i^T \\cdot \\vec{k}_j^T\n",
    "\\rightarrow W = Q K^T =\n",
    "\\begin{bmatrix}\n",
    "\\vec{q}_0^T \\vec{k}_0 & \\vec{q}_0^T \\vec{k}_1 & \\dots \\\\\n",
    "\\vec{q}_1^T \\vec{k}_0 & \\vec{q}_1^T \\vec{k}_1 & \\dots \\\\\n",
    "\\vdots & \\vdots & \\ddots  \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Where i-th row shows interaction strength between embedding $\\vec{h}_i$ and all of other embeddings:\n",
    "$$A_{i} = \n",
    "\\begin{bmatrix}\n",
    "\\alpha_{i,0} & \\alpha_{i,1} & \\dots & \\alpha_{i,N - 1}\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "\\vec{q}_i^T \\vec{k}_0 & \\vec{q}_i^T \\vec{k}_1 & \\dots & \\vec{q}_i^T \\vec{k}_{N - 1}  \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value (V) matrix and aggregation\n",
    "Additionally we need a value $\\vec{v}_i$ representation for aggregation using $A_{i,j}$ weights:\n",
    "\n",
    "$$ \\vec{h}_i \\ \\stackrel{value}{\\rightarrow} \\ = f_v(\\vec{h}_i) = \\vec{v}_i $$\n",
    "\n",
    "We get a new `value` neighbor-aware embedding $\\vec{v}_i^\\prime$ by aggregating all neighbor embeddings $\\vec{v}_i$ using weights $\\alpha_{i,j}$:\n",
    "\n",
    "$$V = \n",
    "\\begin{bmatrix}\n",
    "\\vec{v}_0^T \\\\ \\vec{v}_1^T  \\\\ \\vdots \\\\ \\vec{v}_{N - 1}^T\n",
    "\\end{bmatrix}\n",
    "; \\ \\mathrm{dim(V)} =\n",
    "[ N \\times E]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec{v}_i^{T\\prime} = \n",
    "A_{i} V = \n",
    "\\begin{bmatrix}\n",
    "\\alpha_{i,0} & \\alpha_{i,1} & \\dots & \\alpha_{i,N - 1}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\vec{v}_0^T \\\\ \\vec{v}_1^T  \\\\ \\vdots \\\\ \\vec{v}_{N - 1}^T\n",
    "\\end{bmatrix}=\n",
    "\\sum_{k = 0}^{N-1} \\alpha_{i,k}\\vec{v}_k^T \n",
    "$$\n",
    "Or pack all $\\vec{v}_i^\\prime$ into a matrix $V^\\prime$\n",
    "\n",
    "$$V^\\prime = \n",
    "\\begin{bmatrix}\n",
    "\\vec{v}_0^{T\\prime} \\\\ \\vec{v}_1^{T\\prime}  \\\\ \\vdots \\\\ \\vec{v}_{N - 1}^{T\\prime}\n",
    "\\end{bmatrix}=\n",
    "AV\n",
    "; \\ \\mathrm{dim(V^\\prime)} =\n",
    "[ N \\times E]\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking communication\\aggregation\n",
    "Embeddings in $H$ are ordered by their appearance in a sequence.<br>\n",
    "Since we are performing a forecasting task, we know that last entry in a sequence is influenced by previous entries by some rule (that we ultimately want to uncover by training a model). Actually, this applies to each sequence entry. \n",
    "\n",
    "This means that for embedding indexed `i` we should consider only neighbors up to `i` (self-including), which can be done by setting all according weights to zero:\n",
    "\n",
    "$$\\alpha_{i,k}: k \\in [0,1,\\dots, i]$$ \n",
    "\n",
    "$$\\vec{v}_i^{T\\prime} = \\sum_{k = 0}^{i} \\alpha_{i,k}\\vec{v}_k^T  $$\n",
    "\n",
    "$$\n",
    "\\vec{v}_1^{T\\prime} = \n",
    "A_{1}^{masked} V = \n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 0  & \\dots &  0 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\vec{v}_0^T \\\\ \\vec{v}_1^T \\\\ \\vec{v}_2^T  \\\\ \\vdots \\\\ \\vec{v}_{N - 1}^T\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\vec{v}_0^T & + & \\vec{v}_1^T & + & \\vec{0}^T & + & \\dots & + &  \\vec{0}^T \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\vec{v}_0^T + \\vec{v}_1^T\n",
    "$$\n",
    "\n",
    "Due to ordered nature of a sequence\n",
    "* first entry $\\vec{v}_0^{\\prime}$ will have all summation weights zeroed except itself,\n",
    "* last entry $\\vec{v}_{N-1}^{\\prime}$ will have all weights available.\n",
    "\n",
    "Resulting mask $T$ will have a lower triangle shape. \n",
    "\n",
    "For example lets do aggregation using mask alone:\n",
    "$$\n",
    "V^\\prime = \n",
    "\\begin{bmatrix}\n",
    "\\vec{v}_0^{T\\prime} \\\\ \\vec{v}_1^{T\\prime} \\\\ \\vdots \\\\ \\vec{v}_{N-1}^{T}\n",
    "\\end{bmatrix}=\n",
    "T V = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & \\dots & 0 \\\\\n",
    "1 & 1 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "1 & 1 & \\dots & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\vec{v}_0^{T} \\\\ \\vec{v}_1^{T} \\\\  \\vdots \\\\ \\vec{v}_{N-1}^{T}\n",
    "\\end{bmatrix}=\n",
    "\\begin{bmatrix}\n",
    "\\vec{v}_0^{T} \\\\ \\vec{v}_0^{T} + \\vec{v}_1^{T}  \\\\ \\vdots \\\\ \\vec{v}_0^{T} + \\dots + \\vec{v}_{N - 1}^{T}\n",
    "\\end{bmatrix}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\vec{v}_0^T =\\begin{bmatrix} 2 & 0 & 0 \\end{bmatrix}; \\ \\vec{v}_1^T =\\begin{bmatrix} 0 & -1 & 0 \\end{bmatrix}; \\ \\vec{v}_2^T =\\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix}; \\ V = \\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & -1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle A = \\begin{bmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ 1 & 1 & 1 \\end{bmatrix}; \\ H\\prime = A V = \\begin{bmatrix} 2 & 0 & 0 \\\\ 2 & -1 & 0 \\\\ 2 & -1 & 1 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle A_0 V = 1 \\vec{v}_0^T + 0 \\vec{v}_1^T + 0 \\vec{v}_2^T  = \\begin{bmatrix} 2 & 0 & 0 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle A_1 V = 1 \\vec{v}_0^T + 1 \\vec{v}_1^T + 0 \\vec{v}_2^T  = \\begin{bmatrix} 2 & -1 & 0 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle A_2 V = 1 \\vec{v}_0^T + 1 \\vec{v}_1^T + 1 \\vec{v}_2^T  = \\begin{bmatrix} 2 & -1 & 1 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "v1,v2,v3 = V = np.array([[2,0,0],[0,-1,0],[0,0,1]])\n",
    "w1, w2, w3 = A = np.tril(np.ones((3,3)))\n",
    "\n",
    "print_tex(r'\\vec{v}_0^T =', v1, r'; \\ \\vec{v}_1^T =', v2, r'; \\ \\vec{v}_2^T =', v3, r'; \\ V = ', V)\n",
    "print_tex(r'A = ', A, r'; \\ H\\prime = A V = ', A @ V)\n",
    "print_tex(r'A_0 V = 1 \\vec{v}_0^T + 0 \\vec{v}_1^T + 0 \\vec{v}_2^T  = ', w1*v1 +  0*v2 + 0*v3)\n",
    "print_tex(r'A_1 V = 1 \\vec{v}_0^T + 1 \\vec{v}_1^T + 0 \\vec{v}_2^T  = ', w1*v1 +  w2*v2 + 0*v3)\n",
    "print_tex(r'A_2 V = 1 \\vec{v}_0^T + 1 \\vec{v}_1^T + 1 \\vec{v}_2^T  = ', w1*v1 +  w2*v2 + w3*v3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>In practice weights $A_i$ are normalized via Softmax()</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional encoding\n",
    "Because summation is invariant of ordering and elements of a sequence should adhere to order we introduce positional.\n",
    "\n",
    "Easiest practical approach is to inject positional information $\\vec{p}_i$ directly in original embedding $\\vec{h}_i$\n",
    "\n",
    "$$H^\\prime = H + P = \n",
    "\\begin{bmatrix}\n",
    "\\vec{h}_0^T \\\\\\vec{h}_1^T  \\\\ \\vdots \\\\ \\vec{h}_{N - 1}^T\n",
    "\\end{bmatrix}+\n",
    "\\begin{bmatrix}\n",
    "\\vec{p}_0^T \\\\\\vec{p}_1^T  \\\\ \\vdots \\\\ \\vec{p}_{N - 1}^T\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Positional encoding can be fixed or learned via NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting\n",
    "After applying attention mechanism we have aggregated old $N$ embeddings into $N$ new neighbor-aware embeddings. \n",
    "In case forecasting, each new embedding contains some information from its time-previous neighbors. Even if we needed originally only final prediction, we can use all new token embeddings to predict intermediate results, which will force a model to learn analysis of sequences of any size shorter than context window.\n",
    "\n",
    "Typically, in NLP, new embeddings are projected to size of vocabulary, softmax is applied and Cross-Entropy calculated for loss vs known predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
