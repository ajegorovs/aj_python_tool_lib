{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Base RNN architecture: https://github.com/karpathy/randomfun/blob/master/min-char-rnn-nb.ipynb\n",
    "* Nice info on architecture: https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\n",
    "* Some code structure taken from: https://medium.com/@CallMeTwitch/building-a-neural-network-zoo-from-scratch-the-recurrent-neural-network-9357b43e113c\n",
    "* Backprop: https://mkffl.github.io/2019/07/08/minimalist-RNN.html\n",
    "\n",
    "Code reimplemented in pytorch (for practice). It is mix of info from multiple resources, plus bunch of custom stuff.\n",
    "\n",
    "General goal is to learn list of words and when given beginning of the word, to predict the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch, os, re\n",
    "from collections import Counter\n",
    "from IPython.display import Image\n",
    "from typing import List\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN (Recurrent Neural Network)\n",
    "RNN is an architecture that utilizes a basic building block - a cell, to iteratively and cyclically analyze data of unlimited length and give a prediction on next entry. \n",
    "\n",
    "Data point passes through the cell and long-term information is imprinted on a temporal layer - *hidden state*.\n",
    "\n",
    "images from :\n",
    "https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img src=\"https://stanford.edu/~shervine/teaching/cs-230/illustrations/architecture-rnn-ltr.png\" align=\"center\" width=660 height=200 style=\"background-color:white\"/>\n",
    "\n",
    "(Unfolded view of RNN achitecture. Hidden state (blue/green) of iteration t is shown as a^t)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<center><img src=\"https://stanford.edu/~shervine/teaching/cs-230/illustrations/description-block-rnn-ltr.png\" align=\"center\" width=460 height=260 style=\"background-color:white\"/>\n",
    "\n",
    "(Contents of RNN hidden state cell)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents of hidden state is shaped each iteration and depends on values of  previous hidden state $a^{<t-1>}$ and current input $x^{<t>}$:\n",
    "\n",
    "$$\n",
    "a^{<t>} = g_1 ( W_{aa} a^{<t-1>} + W_{ax} x^{<t>} + b_a)\n",
    "$$\n",
    "\n",
    "And an output of a cell (relevant for final evaluation) is:\n",
    "$$\n",
    "y^{<t>} = g_2 ( W_{ya} a^{<t>} + b_y)\n",
    "$$\n",
    "\n",
    "where $g_1$ and $g_2$ are activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "It is more common to denote hidden layer as $h$. So we will rename entries accordingly:\n",
    "$$\n",
    "\\begin{cases}\n",
    "h_{t} = g_1 ( W_{hh} h_{t-1} + W_{xh} x_{t} + b_h) \\\\\n",
    "y_{t} = g_2 ( W_{hy} h_{t} + b_y)\n",
    "\\end{cases}\n",
    "$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "Take a segment of shakespeare, split it into characters.\n",
    "\n",
    "from \n",
    "https://github.com/karpathy/char-rnn/tree/master/data/tinyshakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1975 words of size 8, and 50 unique characters.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Chars: m P I E N l K Z y U q Y n g S h r L z v T A M O e w W i d V u R k H a p f o G C b c x s B t j J F D'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = open(os.path.join('data_processing','media','tinyshakespeare.txt'), 'r').read() # should be simple plain text file\n",
    "data = re.sub(r'[^a-zA-Z\\s]', '', data) # remove non-alphabet\n",
    "seq_length = 8\n",
    "words_all = data.split()\n",
    "words = list(set([w for w in words_all if len(w) == seq_length]))\n",
    "chars_all = [c for w in words for c in w] #[*data]\n",
    "chars = list(set(chars_all))  #list(set(data))\n",
    "data_size, vocab_size = len(words), len(chars)\n",
    "print('data has %d words of size %d, and %d unique characters.' % (len(words), seq_length, vocab_size))\n",
    "'Chars: ' + ' '.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all characters and encode them into one-hot-vector\n",
    "\n",
    "Bonus: sort words by use frequency, so most common characters are in the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sorted chars: e s r i n t a o d l c u g h p m f b y v w k C S A R M I E q B T D P O L j x N U H F z V W G K J Y Z'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency = dict(Counter(chars_all))\n",
    "vocab_unique = list(sorted(frequency.keys(), key=lambda x: frequency[x], reverse=True))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(vocab_unique) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(vocab_unique) }\n",
    "'Sorted chars: ' + ' '.join(vocab_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each letter is encoded into one-hot-vector (column), a word is a collection of letters-> <br>\n",
    "matrix of shape (len(alphabet),len(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s', 'c', 'a', 'l', 'd', 'i', 'n', 'g']\n",
      "[1, 10, 6, 9, 8, 3, 4, 12]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def oneHotEncode(chars: List[str]) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "        lets hold encoded vectors as columns\n",
    "    \"\"\"\n",
    "    encode = torch.zeros(len(chars),vocab_size)\n",
    "    x,y = torch.tensor([(i,char_to_ix[a]) for i,a in enumerate(chars)]).T\n",
    "    encode[x,y] = 1\n",
    "    return encode.T\n",
    "split_word = [*words[0]];print(split_word)\n",
    "word_index = [char_to_ix[a] for a in split_word];print(word_index)\n",
    "oneHotEncode(split_word)[:10,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a training set of N words.<br>\n",
    "word: ['c', 'o', 'm', 'f', 'o', 'r', 't', 's']<br>\n",
    "During training it will be sliced into [:-1] and [1:] parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words_max = 20\n",
    "X_train = torch.zeros(size=(num_words_max, vocab_size, seq_length), device=device) # words, chars, seq of chars\n",
    "for i in range(num_words_max):\n",
    "    X_train[i] = oneHotEncode(words[i])\n",
    "\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize network weights by \n",
    "https://365datascience.com/tutorials/machine-learning-tutorials/what-is-xavier-initialization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3956,  0.6284],\n",
       "        [-0.5509,  0.4450]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xavier Normalized Initialization\n",
    "def initWeights(input_size: int, output_size: int) -> torch.Tensor:\n",
    "    rnd = torch.rand(size=(input_size, output_size),device=device)\n",
    "    rnd = 2*rnd - 1\n",
    "    return rnd * torch.sqrt(torch.tensor(6) / (input_size + output_size))\n",
    "\n",
    "initWeights(2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Weights and biases:\n",
    "Given that\n",
    "$$\n",
    "\\begin{cases}\n",
    "h_{t} = g_1 ( W_{hh} h_{t-1} + W_{xh} x_{t} + b_h) \\\\\n",
    "y_{t} = g_2 ( W_{hy} h_{t} + b_y)\n",
    "\\end{cases}\n",
    "$$\n",
    "and $x_{t}$ thus $y_{t}$ will be stored as columns of shape $(N_x,1)$<br>\n",
    "and $h_{t}$ will be stored as columns of shape $(N_h,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then dimensions of weights and biases are as follows: \n",
    "$$\\begin{cases}\n",
    "\n",
    "\\mathrm{dim}(W_{hh})  = (N_h,N_h) \\text{, such that } (N_h,N_h) , (N_h, 1) \\rightarrow (N_h, 1) \\\\\n",
    "\\mathrm{dim}(W_{xh})  = (N_h,N_x) \\text{, such that } (N_h,N_x) , (N_x, 1) \\rightarrow (N_h, 1) \\\\\n",
    "\\mathrm{dim}(W_{yh})  = (N_x,N_h) \\text{, such that } (N_x,N_h) , (N_h, 1) \\rightarrow (N_x, 1) \\\\\n",
    "\\mathrm{dim}(b_h)     = \\mathrm{dim}(h_t)= (N_h, 1)                                             \\\\\n",
    "\\mathrm{dim}(b_y) = \\mathrm{dim}(y_t)= (N_x, 1)\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "## Loss function:\n",
    "Cross-entropy can be derived from K-L Divergence.<br> \n",
    "Similarly it compares two distributions $p$ and $q$.\n",
    "$$H(p,q) = -\\sum_{x\\in\\mathcal{vocabulary}} p(x)\\, \\log q(x)$$\n",
    "In our case, we are comparing one-hot-encoded representation of a character $p(x)$,<br>\n",
    "with only 1 non zero entry, to output of NN, which is its guess distribution.\n",
    "\n",
    "$p(x)$ has only one entry of '1' at index $k$, then the sum is not required- can replace $p(x)$ with $\\delta_{x,k}$\n",
    "$$H(p,q) = -\\sum_{x\\in\\mathcal{vocabulary}} \\delta_{x,k}\\ \\log q(x) = -\\log q(k) $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Recurrent Neural Network Class #####\n",
    "class RNN:\n",
    "    \"\"\"\n",
    "        input_size- one-hot length (vocabulary); hidden_size - internal buffer; output_size - negative/positive mood = 2\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        # Network\n",
    "        self.Whh = initWeights(hidden_size, hidden_size)\n",
    "        self.Wxh = initWeights(hidden_size, input_size)\n",
    "        self.Wyh = initWeights(output_size, hidden_size)\n",
    "\n",
    "        self.bh = torch.zeros((hidden_size, 1),device=device)\n",
    "        self.by = torch.zeros((output_size, 1),device=device)\n",
    "\n",
    "        self.g1 = torch.tanh\n",
    "        self.hs0 = torch.zeros(size=(hidden_size, 1),device=device)\n",
    "\n",
    "        self.vocab_len = input_size\n",
    "\n",
    "        \n",
    "    def softmax(self,x: torch.Tensor) -> torch.Tensor:\n",
    "        y = torch.exp(x)\n",
    "        return y / torch.sum(y)\n",
    "\n",
    "    # Forward Propagation\n",
    "    def forward(self, xs: torch.Tensor, hprev: torch.Tensor):\n",
    "        vocab_len, num_data = xs.size()\n",
    "\n",
    "        ps = torch.zeros_like(xs, device=device)   # same number of predictions as input. except staggered in time.\n",
    "        hs = torch.zeros(size=(self.bh.size(0), num_data + 1), device=device)   # + 1 'zeroth' hidden state\n",
    "        hs[:,[0]] = hprev.clone()\n",
    "        \n",
    "        for t in range(num_data):\n",
    "            # process this like slices, not explicit matrix mults.\n",
    "            a = self.Wxh @ xs[:,t]\n",
    "            b = self.Whh @ hs[:,t]              # t here is prev step\n",
    "            c = self.bh.squeeze(1)\n",
    "            hs[:,t+1]   = self.g1(a + b + c)    # t+1 is current step\n",
    "            #hs[:,[t+1]] = self.g1(self.Wxh @ xs[:,[t]] + self.Whh @ hs[:,[t]] + self.bh)\n",
    "            y   = self.Wyh @ hs[:,t + 1] + self.by.squeeze(1)\n",
    "            ps[:,t]   = self.softmax(y)\n",
    "\n",
    "        self.hs = hs\n",
    "        self.ps = ps\n",
    "\n",
    "        return hs[:,[-1]], ps[:,[-1]]\n",
    "    \n",
    "    def test(self, input, first_n_chars = 1):\n",
    "        \"\"\"\n",
    "            Input is a whole word.\n",
    "            * Part of data will be used to determine latest hidden state ->\n",
    "            -> data columns up to index 'first_n_chars'                             i.e for first_n_chars = 1, cols [0]\n",
    "            * store known columns in a  solution.                                   i.e cols [0]\n",
    "            * predict column index 'first_n_chars', generate latest hidden state    i.e cols [1]\n",
    "            * store prediction\n",
    "            * start iterating from i = 'first_n_chars + 1' and store predictions    i.e i = [2,3,..]\n",
    "\n",
    "        \"\"\"\n",
    "        store = torch.zeros(size=(self.vocab_len, input.size(1)), device=device)\n",
    "        store[:,:first_n_chars] = input[:,:first_n_chars]\n",
    "        # develop hidden state from known input data and get latest prediction:\n",
    "        hidden_state, prediction = self.forward(input[:,:first_n_chars], self.hs0)\n",
    "\n",
    "        store[:,[first_n_chars]] = prediction\n",
    "\n",
    "        for i in range(first_n_chars + 1, store.size(1)):\n",
    "            hidden_state, prediction = self.forward(prediction, hidden_state)\n",
    "            store[:,[i]] = prediction\n",
    "        # get index of one highest entry. \n",
    "        \n",
    "        known = torch.topk(store[:,:first_n_chars], dim = 0, k = 1)[1].view(-1)\n",
    "        rest = torch.topk(store[:,first_n_chars:], dim = 0, k = 1)[1].view(-1)\n",
    "\n",
    "        true = torch.topk(input, dim = 0, k = 1)[1].view(-1)\n",
    "        outp = 'true: ' + ''.join([ix_to_char[int(i)] for i in true])   + '; guess: ('\n",
    "        outp += ''.join([ix_to_char[int(i)] for i in known])            + ')' \n",
    "        outp += ''.join([ix_to_char[int(i)] for i in rest])\n",
    "\n",
    "        print(outp)\n",
    "        return\n",
    "\n",
    "\n",
    "    def backward(self,  xs: torch.Tensor, tar: torch.Tensor):\n",
    "        dWxh    = torch.zeros_like(self.Wxh, device=device)\n",
    "        dWhh    = torch.zeros_like(self.Whh, device=device)\n",
    "        dWyh    = torch.zeros_like(self.Wyh, device=device)\n",
    "\n",
    "        dbh     = torch.zeros_like(self.bh, device=device)\n",
    "        dby     = torch.zeros_like(self.by, device=device)\n",
    "        dhnext  = torch.zeros_like(self.bh, device=device)\n",
    "\n",
    "        for t in reversed(range(xs.size(1))):\n",
    "            hs_col  = self.hs[:,[t+1]]   # hidden state column\n",
    "            ps_col  = self.ps[:,[t]]     # prediction column\n",
    "            xs_col  = xs[ :,[t]]         # og value column\n",
    "            tr_col  = tar[:,[t]]         # target column\n",
    "            dy      = ps_col.clone()\n",
    "            dy[tr_col.to(bool)] -= 1 \n",
    "            dWyh    += dy @ hs_col.T\n",
    "            dby     += dy\n",
    "            dh = self.Wyh.T @ dy + dhnext # backprop into h\n",
    "            dhraw   = (1 - hs_col * hs_col) * dh    # d-tanh\n",
    "            dbh     += dhraw\n",
    "            dWxh    += dhraw @ xs_col.T\n",
    "            dWhh    += dhraw @ hs_col.T\n",
    "            dhnext  =  self.Whh.T @ dhraw\n",
    "            \n",
    "        for dparam in [dWxh, dWhh, dWyh, dbh, dby]:\n",
    "            torch.clip(dparam, -5, 5, out=dparam)\n",
    "\n",
    "        self.Wxh += -self.learning_rate * dWxh\n",
    "        self.Whh += -self.learning_rate * dWhh\n",
    "        self.Wyh += -self.learning_rate * dWyh    \n",
    "        self.bh  += -self.learning_rate * dbh\n",
    "        self.by  += -self.learning_rate * dby\n",
    "\n",
    "\n",
    "    def loss(self, ps_true):\n",
    "        \"\"\"\n",
    "            1) extract q(k) by elem-wise mult with mask p(x)\n",
    "            2) q(k) for all iterations -> take log()\n",
    "            3) -sum( logs ) -> total error\n",
    "            -----------------------------------------\n",
    "            c       = torch.tensor([[5,3,6],[1,8,3]]).T\n",
    "            b       = torch.tensor([[1,0,0],[0,0,1]]).T\n",
    "            c * b   = torch.tensor([[5,0,0],[0,0,3]]).T\n",
    "            sum(d0) = torch.tensor([5,3])   # sum is 'reduce'\n",
    "            torch.sum(b * c, dim=0)\n",
    "        \"\"\"\n",
    "        mask_sum = torch.sum(self.ps * ps_true, dim=0)\n",
    "        return -torch.sum(torch.log(mask_sum))\n",
    "    \n",
    "    def train(self, inputs, num_epochs, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        for i in range(num_epochs):\n",
    "\n",
    "            shuffle_words = inputs[torch.randperm(len(inputs))]\n",
    "            loss = 0\n",
    "            for word in shuffle_words:\n",
    "                input  = word[:,:-1]\n",
    "                output = word[:,1:]\n",
    "                hprev = self.hs0  # for epoch start reset hidden state.\n",
    "\n",
    "                self.forward(input, hprev)  #\n",
    "                #hprev = self.hs[:,[-1]]\n",
    "                loss += self.loss(output)\n",
    "                \n",
    "                self.backward(input, output)\n",
    "                \n",
    "            if i % (num_epochs//10) == 0:\n",
    "                print(f'epoch: {i:<3}, error:{loss:0.3f}')\n",
    "                idx = torch.randint(len(inputs),(1,))[0]\n",
    "                self.test(inputs[idx], 3)\n",
    "            \n",
    "net = RNN(input_size=vocab_size,hidden_size=100, output_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  , error:548.805\n",
      "true: anothers; guess: (ano)Hando\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100, error:138.712\n",
      "true: scatters; guess: (sca)lding\n",
      "epoch: 200, error:53.599\n",
      "true: MONTAGUE; guess: (MON)TAGUE\n",
      "epoch: 300, error:32.885\n",
      "true: MONTAGUE; guess: (MON)TAGUE\n",
      "epoch: 400, error:24.454\n",
      "true: acquaint; guess: (acq)uaint\n",
      "epoch: 500, error:20.526\n",
      "true: grinning; guess: (gri)nning\n",
      "epoch: 600, error:18.471\n",
      "true: drunkard; guess: (dru)nkard\n",
      "epoch: 700, error:16.831\n",
      "true: homicide; guess: (hom)icide\n",
      "epoch: 800, error:16.877\n",
      "true: magician; guess: (mag)ician\n",
      "epoch: 900, error:15.175\n",
      "true: drinking; guess: (dri)nking\n"
     ]
    }
   ],
   "source": [
    "net.train(X_train, num_epochs = 1000, learning_rate=0.002)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
