{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(observed episode) trajectory of states and actions (time = T is terminal):\n",
    "$$\\tau = \\{s_0, a_0, s_1,\\dots\\,s_{T-1}, a_{T-1}, s_T\\}$$\n",
    "_notice there is no action $a_T$ from terminal state $s_T$._\n",
    "\n",
    "(observed episode) rewards from trajectory\"\n",
    "$$ r_\\tau =  \\{r_0, r_1,\\dots,r_T\\}$$\n",
    "finite-horizon undiscounted return:\n",
    "$$ R(\\tau) = \\sum_{t = 0}^T r_t$$\n",
    "finite-horizon discounted return:\n",
    "$$ R(\\tau) = \\sum_{t = 0}^T \\gamma^t r_t$$\n",
    "Policy $\\pi$ decides which actions to take from state $s$:\n",
    "$$\\pi = \\pi(a_t|s_t) \\ (\\forall t \\in [0,1,\\dots, T - 1])$$\n",
    "Stochasticity of environment shows itself in probabilistic response to actions:\n",
    "$$P(s_{t+1}|s_t,a_t)$$\n",
    "Probability of a trajectory under policy $\\pi$:\n",
    "$$P(\\tau|\\pi) = \\rho_0(s_0) \\cdot \\pi(a_0|s_0) \\cdot P(s_1|s_0,a_0)\\cdot\\pi(a_1|s_1)\\cdot P(s_2|s_1,a_1)\\cdot \\dots$$\n",
    "$$ = \\rho_0(s_0)\\cdot \\prod_{t=0}^{T-1} \\pi(a_t|s_t) \\cdot P(s_{t+1}|s_t,a_t)$$\n",
    "_where $\\rho_0(s_0)$ is a probability of starting in state $s_0$. Its controlled by environment._\n",
    "\n",
    "_we employ Markov property here to get rid of the history:_\n",
    "$$P(s_{t+2}|s_t,s_{t+1},a_t,a_{t+1}) = P(s_{t+2}|s_{t+1}, a_{t+1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can measure performance $J(\\pi)$ of a given policy $\\pi$ by examining many trajectories $\\tau \\in D$, their cumulative rewards $R(\\tau)$. This can be done by computing expected rewards:\n",
    "$$J(\\pi) = \\sum_{\\tau \\in D} P(\\tau|\\pi) \\cdot R(\\tau) = \\underset{\\tau \\sim \\pi}{\\mathbb{E}}[R(\\tau)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases policy should be approximated by parametric representation \n",
    "$$\\pi_{\\vec{\\theta}}=\\pi(a|s,\\vec{\\theta})$$\n",
    ", where $\\vec{\\theta}$ is a vector of parameters. _I will omit vector sign in the future._\n",
    "\n",
    "Approximation can be learned by performing gradient de-/ascent:\n",
    "$$\\theta_{t+1} = \\theta_t + \\alpha \\nabla_{\\theta} J(\\pi_{\\theta_t})$$\n",
    "where $t$ are 'time steps' at which wight update is conducted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in order to determine \n",
    "$$\\nabla_{\\theta} J(\\pi_{\\theta}) =  \\sum_{\\tau \\in D} \\nabla_{\\theta} \\bigg[P(\\tau|\\pi_{\\theta}) \\cdot R(\\tau) \\bigg] = \\sum_{\\tau \\in D} \\nabla_{\\theta} \\bigg[\\rho_0(s_0)\\cdot \\prod_{t=0}^{T-1} \\pi_\\theta(a_t|s_t) \\cdot P(s_{t+1}|s_t,a_t) \\cdot R(\\tau) \\bigg]$$ \n",
    "we can calculate:\n",
    "$$\\nabla_{\\theta}P(\\tau|\\theta) = P(\\tau|\\theta)\\cdot\\nabla_{\\theta} \\ log P(\\tau|\\theta)$$\n",
    "and\n",
    "$$\\nabla_{\\theta} \\ log P(\\tau|\\theta) = \\sum_{t=0}^{T} \\nabla_{\\theta} \\ log  \\ \\pi_\\theta(a_t|s_t)$$\n",
    "After few mathematical manipulations:\n",
    "$$\\boxed{\\nabla_{\\theta} J(\\pi_{\\theta}) =  \\underset{\\tau \\sim \\pi}{\\mathbb{E}}\\bigg[ \\sum_{t=0}^{T} \\nabla_{\\theta} \\ log  \\ \\pi_\\theta(a_t|s_t) R(\\tau) \\bigg]}$$\n",
    "where expectation is taken over probability of a trajectory, _but given many sample episodes it can be estimated by a mean_:\n",
    "$$\\underset{\\tau \\sim \\pi}{\\mathbb{E}}[\\ \\cdot \\ ] \\longrightarrow \\frac{1}{|D|} \\sum_{\\tau \\in D}[\\ \\cdot \\ ]$$\n",
    "\n",
    "$$\\boxed{\\nabla_{\\theta} J(\\pi_{\\theta}) =  \\frac{1}{|D|} \\sum_{\\tau \\in D}\\sum_{t=0}^{T} \\nabla_{\\theta} \\ log  \\ \\pi_\\theta(a_t|s_t) R(\\tau)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
