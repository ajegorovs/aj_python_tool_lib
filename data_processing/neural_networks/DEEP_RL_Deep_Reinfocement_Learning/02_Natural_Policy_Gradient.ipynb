{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Policy Gradient (NPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "*   Natural Policy Gradients In Reinforcement Learning Explained https://arxiv.org/pdf/2209.01820\n",
    "*   Deep RL Bootcamp Lecture 5: Natural Policy Gradients, TRPO, PPO https://youtu.be/xvRrgxcpaHY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issues:\n",
    "*   Optimization of policy is made using 'noisy' data (high variance);\n",
    "*   High chance that during update policy will be 'pushed'/ changed to drastically.\n",
    "*   If policy becomes bad (overshoots optimum parameters), new data gathered via that policy will be bad quality\n",
    "*   Whole learning loop may break down\n",
    "\n",
    "Solution:\n",
    "*   Limit how much policy can change from one iteration to another\n",
    "*   Use robust metric- distance in 'policy space' not 'policy parameter space'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extension of 'Vanilla' Policy Gradient (VPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Everything from notes [01_Simplest_Policy_Gradient_Implementations.ipynb](01_Simplest_Policy_Gradient_Implementations.ipynb) applies\n",
    "\n",
    "-We want to perform gradient ascent in model's parameter space to maximize policy's performance $J(\\pi_{\\theta_t})$\n",
    "\n",
    "_(remember that $J$ is expected rewards over all possible trajectories)_\n",
    "\n",
    "So the update rule is\n",
    "$$\\theta_{t+1} = \\theta_t + \\alpha \\nabla_{\\theta} J(\\pi_{\\theta_t}) = \\theta_t + \\alpha \\vec{g}$$\n",
    "where step direction \n",
    "$$\\boxed{\\vec{g} = \\nabla_{\\theta} J(\\pi_{\\theta}) =  \\frac{1}{|D|} \\sum_{\\tau \\in D}R(\\tau)\\cdot\\sum_{t=0}^{T} \\nabla_{\\theta} \\ log  \\ \\pi_\\theta(a_t|s_t) }$$\n",
    "Where $R(\\tau)$ is a reward for a particular trajectory $\\tau$. Check different variants in a linked notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust distance metric: KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL divergence [KL_Divergence.ipynb](../../Statistics/KL_Divergence.ipynb) has few qualitative properties as euclidean distance\n",
    "*   distance between same policy is 0\n",
    "    $$D_{KL}(p||p) = \\int p(x) \\log \\frac{p(x)}{p(x)} dx = \\int p(x) \\cdot (\\log p(x)- \\log p(x)) \\ dx = \\int 0 dx$$\n",
    "*   distance between policies is greater or equal to 0 (no easy proof)\n",
    "\n",
    "Downsides:\n",
    "* unlike geometric distance, not symmetric\n",
    "\n",
    "Why use in NPG?\n",
    "*   Small policy's parameter change (step size) makes policy learning more stable. \n",
    "\n",
    "*   This 'smooth' trajectory though parameter space can be enforced by keeping old policy 'similar enough' to new policy.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL Divergence and Fisher information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL divergence is related to Fisher Information (Matrix) [Fisher_Information.ipynb (ending)](../../Statistics/Fisher_Information.ipynb)\n",
    "$$\\boxed{D_{KL}\\bigg(\\pi(x; \\theta) ||\\pi(x; \\theta + \\delta) \\bigg) \\overset{\\text{2nd order}}{\\approx} \\frac{1}{2}\\delta^T \\mathbb{E}\\bigg[\\nabla_\\theta \\log \\pi(x; \\theta) \\ \\nabla_\\theta \\log \\pi(x; \\theta)^T\\bigg] \\delta}$$\n",
    "Where expectation is Fisher information $F$ (or $I(\\theta)$)\n",
    "$$F = \\mathbb{E}\\bigg[\\nabla_\\theta \\log \\pi(x; \\theta) \\ \\nabla_\\theta \\log \\pi(x; \\theta)^T\\bigg]$$\n",
    "and perturbation $\\delta$ is difference between new and old policy (parameter) variants\n",
    "$$\\delta = \\theta_{new} - \\theta_{old}$$\n",
    "> Relation of KL divergence and Fisher Information Matrix (FIM) shows that<br>\n",
    "> FIM describes how 'sensitive' distribution is to small deviations in parametrization.\n",
    "\n",
    "__NOTE:__<br>\n",
    "_Similar impementations stop at the intermediate step and say_\n",
    "$$D_{KL}\\bigg(\\pi(x; \\theta) ||\\pi(x; \\theta + \\delta) \\bigg) \\overset{\\text{2nd order}}{\\approx} \\frac{1}{2}\\delta^T \\nabla^2 D_{KL} \\delta$$\n",
    "_Which requires to calculate KL Divergence. We will use it in TRPO method._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Why is curvature important? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, descent method is augmented with information about second order derivatives (curvature).\n",
    "\n",
    "Lets remind us that in 2nd order optimization we use Taylor expansion and search for its minima $\\vec{x}^*$.    \n",
    "$$f(\\vec{x}) \\approx f(\\vec{x}_0) + \\nabla f(\\vec{x}_0) \\cdot (\\vec{x} - \\vec{x}_0) + \\frac{1}{2} (\\vec{x} - \\vec{x}_0)\\cdot H(\\vec{x}_0) (\\vec{x} - \\vec{x}_0)$$\n",
    "\n",
    "$$\\nabla f(\\vec{x}^*) = \\nabla f(\\vec{x}_0) + H(\\vec{x}_0) (\\vec{x}^*- \\vec{x}_0) = \\vec{0}$$\n",
    "\n",
    "$$\\vec{x}^* = \\vec{x}_0 - H(\\vec{x}_0)^{-1}  \\nabla f(\\vec{x}_0)$$\n",
    "\n",
    "Here a _Hessian matrix_ $H$ provides information about the curvature of objective function.\n",
    "\n",
    "_Intuition: Curvature - rate of change of slope, or slope on steroids xd. Hessian is inverted, since if curvature is low -  we are on plateau, we want to scale steps larger (1/small = big). For high curvature we want an opposite effect._\n",
    "\n",
    "It is an elegant method to adapt step size, but it is computationally expensive.\n",
    "([Notes_Second_Order_Methods.ipynb](../../optimization/Notes_Second_Order_Methods.ipynb))\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization goal for each iteration could be to find such step:\n",
    "$$\\boxed{\\delta^* = \\underset{s.t. \\ D_{KL}\\big(\\pi(\\theta)||\\pi(\\theta+\\delta)\\big) \\lt \\epsilon}{\\argmax_\\delta} J(\\theta + \\delta)}$$\n",
    "_(https://www.andrew.cmu.edu/course/10-703/slides/Lecture_NaturalPolicyGradientsTRPOPPO.pdf)_\n",
    "\n",
    "We can soften constrain by forming an _unconstrained_ objective function\n",
    "\n",
    "Constraint is violated if $$D_{KL}\\big(\\pi(\\theta)||\\pi(\\theta+\\delta)\\big) > \\epsilon$$ \n",
    "So penalty function is\n",
    "$$P_{KL}(\\delta) = D_{KL}\\big(\\pi(\\theta)||\\pi(\\theta+\\delta)\\big)- \\epsilon > 0$$\n",
    "\n",
    "Uncostrained objective function\n",
    "$$f_U(\\delta) = J(\\theta + \\delta) - \\lambda P_{KL}(\\delta)$$\n",
    "Term $- \\lambda P_{KL}(\\delta)$ brings objective function down.\n",
    "Unconstrained optimization problem:\n",
    "$$\\delta^* = \\argmax_\\delta J(\\theta + \\delta) - \\lambda P_{KL}(\\delta)$$\n",
    "$$\\boxed{\\delta^* = \\argmax_\\delta J(\\theta + \\delta) - \\lambda \\bigg(D_{KL}\\big(\\pi(\\theta)||\\pi(\\theta+\\delta)\\big)- \\epsilon\\bigg)}$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding optimization direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We approximate $J(\\theta + \\delta)$ up to 2nd order via Taylor's expansion and get\n",
    "\n",
    "$$\\delta^* = \\argmax_\\delta J(\\theta) + \\nabla_\\theta J(\\theta)\\big|_{\\theta = \\theta_{old}}\\delta - \\lambda D_{KL}\\big(\\pi(\\theta)||\\pi(\\theta+\\delta)\\big)+ \\lambda \\epsilon$$\n",
    "$$ = \\argmax_\\delta  \\nabla_\\theta J(\\theta)_{old}\\delta - \\frac{\\lambda}{2}\\delta^T F|_{\\theta = \\theta_{old}} \\delta \\underbrace{+ \\lambda \\epsilon + J(\\theta)}_{\\text{not important for optimization}}$$\n",
    "Our maximization objective function is \n",
    "$$f = \\nabla_\\theta J(\\theta)_{old}\\delta - \\frac{\\lambda}{2}\\delta^T F|_{\\theta = \\theta_{old}} \\delta$$\n",
    "_We can change maximization task into minimization by multiplying objective by -1._\n",
    "\n",
    "Searching for extremum we compute a derivative and set it to zero:\n",
    "$$\\frac{\\partial}{\\partial \\delta} \\bigg(\\nabla_\\theta J(\\theta)_{old}\\delta - \\frac{\\lambda}{2}\\delta^T F|_{\\theta = \\theta_{old}} \\delta\\bigg) = 0$$\n",
    "and we get\n",
    "$$0 = \\nabla_\\theta J(\\theta)_{old} - \\frac{\\lambda}{2}F|_{\\theta = \\theta_{old}} \\delta^*$$\n",
    "$$\\frac{\\lambda}{2}F|_{\\theta = \\theta_{old}} \\delta^* = \\nabla_\\theta J(\\theta)_{old} $$\n",
    "$$ \\delta^* = \\frac{2}{\\lambda} F^{-1}\\nabla_\\theta J(\\theta) $$\n",
    "Which is a modified original step direction/size. \n",
    "\n",
    "Modify gradient ascent - paramter update\n",
    "$$\\theta_{t+1} = \\theta_t + \\beta \\delta^*$$\n",
    "By absorbing constants into $\\alpha$ our iteration update is\n",
    "$$\\theta_{t+1} = \\theta_t + \\alpha  F^{-1}\\nabla_\\theta J(\\theta) = \\theta_t + \\alpha  F^{-1}\\vec{g}$$\n",
    "We retrieve intermediate version of NPG parameter update rule\n",
    "$$\\boxed{\\theta_{t+1} = \\theta_t+ \\alpha  F^{-1}\\vec{g}}$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding step size via KL threshold value $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet we still have unknown step size $\\alpha$ and KL divergence threshold has be discarded during optimization.\n",
    "\n",
    "We can retrieve step size via \"normalization under the Fisher metric\":\n",
    "$$D_{KL}\\bigg(\\pi(x; \\theta) ||\\pi(x; \\theta + \\alpha\\delta^*) \\bigg) = \\frac{\\alpha^2}{2}\\delta^{*T} F \\delta^* \\leq \\epsilon$$\n",
    "$$\\frac{\\alpha^2}{2}\\delta^{*T} F \\delta^* = \\frac{\\alpha^2}{2}\\bigg(F^{-1}\\vec{g}\\bigg)^T F \\bigg(F^{-1}\\vec{g}\\bigg) \\leq \\epsilon$$\n",
    "$$ = \\frac{\\alpha^2}{2} \\vec{g}^T {F^{-1}}^T F F^{-1}\\vec{g}\\leq \\epsilon$$\n",
    "Fisher information, like Hessian, due to second derivatives is a symmetric matrix: \n",
    "$${F^{-1}}^T F = {F^{-1}}^T F^T = (F F^{-1})^T = I^T$$\n",
    "Almost done\n",
    "$$\\frac{\\alpha^2}{2} \\vec{g}^T {F^{-1}}\\vec{g}\\leq \\epsilon$$\n",
    "$$\\alpha^2 \\leq \\frac{2 \\epsilon}{\\vec{g}^T {F^{-1}}\\vec{g}}$$\n",
    "At maximum ('$\\leq$' $\\longrightarrow$ '=' ), $\\alpha$ should be \n",
    "$$\\boxed{\\alpha = \\sqrt{\\frac{2\\epsilon}{\\nabla \\vec{g}^T {F^{-1}}\\vec{g}}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta_{t+1} = \\theta_t + \\alpha  F^{-1}\\vec{g}$$\n",
    "update rule is the following\n",
    "$$\\boxed{\\theta_{t+1} = \\theta_t + \\sqrt{\\frac{2\\epsilon}{\\vec{g}^T {F^{-1}} \\vec{g}}} F^{-1}\\vec{g}}$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
