{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __NEW ROADMAP__\n",
    "\n",
    "https://github.com/openai/spinningup/blob/master/spinup/algos/tf1/trpo/core.py\n",
    "\n",
    "*   TRPO implemented using diagonal multivariate normal distributions (looking at two sources and some info in OG paper)\n",
    "*   gradient is calculated as usual, it will be remapped via inverse Hessian\n",
    "*   objective function contains KL div constraint, which is a Hessian of KL divergence.\n",
    "*   It can be computed fast and ez with conjugate gradient (CG). But we need to calculate KL div. Unlike using Fisher Information Matrix (FIM)\n",
    "\n",
    "STEPS\n",
    "\n",
    "0.  Run the experiment (this is somewhere inside point 1.)\n",
    "1.  Define MLP Gaussian policy\n",
    "    *   it takes in list of states and list of actions, which were conducted based on states using old version of policy\n",
    "    *   MLP produces mean value parameter for each action thats possible in a given state.\n",
    "\n",
    "        so if state list is of shape (steps, num_observables), MLP will produce parameters of shape (steps, num_actions). only mean. stdev is unique for each action\n",
    "\n",
    "    *   we sample actions using reparametrization trick: \n",
    "        $$a_\\pi \\sim \\mu(states) + \\N(\\theta)*\\sigma$$\n",
    "        are these logits? im not sure. should be same shape as \\mu-> (steps, num_actions)\n",
    "\n",
    "    *   compute log likelihood of $a_\\pi$, sum over actions dim (might be multiplication in log representation)\n",
    "\n",
    "    *   compute log likelihood for observed actions $a$. This implies that dims of $a$ and $a_\\pi$ are the same so no distribution over actions? Not possible, since mu has last dim of num_actions.\n",
    "\n",
    "    *   retrieve $\\mu$ and $\\sigma$ of old policy. What to do if num states were different??\n",
    "\n",
    "    *   compute analytic KL divergence using old and new distribution parameters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Compute Surrogate reward\n",
    "    *   ratio of log likelihoods new/old\n",
    "    *   times advantage -> objective f-n\n",
    "3.  Update weights\n",
    "    *   get MLP params\n",
    "    *   calculate grad w.r.t. objective f-n\n",
    "    *   define hessian-vector product function for arbitrary vector\n",
    "    *   perform CG iterations to get direction and initial step size\n",
    "    *   backtracking line search\n",
    "\n",
    "4. update value function (baseline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surrogate reward $(U(\\theta))$\n",
    "(https://www.youtube.com/watch?v=KjWF8VIMGiY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a loss which, when optimized, will produce an incremental improvement to a policy with respect to old policy version.\n",
    "\n",
    "Given we have gathered trajectory using old policy, we can evaluate expected rewards using new policy via _Importance Sampling_.\n",
    "\n",
    "My explanation can be seen midway throughout [Sutton_P1_Ch05_Monte_Carlo.ipynb notebook (off-policy)](../../RL_Reinforced_Learning/Sutton_P1_Ch05_Monte_Carlo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$U(\\theta) = \\mathbb{E}_{\\tau \\sim \\theta_{old}} \\bigg[ \\frac{P(\\tau|\\theta)}{P(\\tau|\\theta_{old})} R(\\tau)\\bigg]$$\n",
    "Gradient of this expected return is:\n",
    "$$\\nabla_{\\theta}U(\\theta) = \\mathbb{E}_{\\tau \\sim \\theta_{old}} \\bigg[ \\frac{\\nabla_{\\theta} P(\\tau|\\theta)}{P(\\tau|\\theta_{old})} R(\\tau)\\bigg]$$\n",
    "if old policy and new policy are the same ($\\theta = \\theta_{old}$), via\n",
    "$$\\nabla_{\\theta}P(\\tau|\\theta) = P(\\tau|\\theta)\\cdot\\nabla_{\\theta} \\ log P(\\tau|\\theta)$$\n",
    "we recover 'vanilla' policy gradient update ([Simplest_Policy_Gradient](01_Simplest_Policy_Gradient_Implementations.ipynb)):\n",
    "$$\\underset{\\tau \\sim \\pi}{\\mathbb{E}}\\bigg[ \\sum_{t=0}^{T} \\nabla_{\\theta} \\ log  \\ \\pi_\\theta(a_t|s_t) \\cdot R(\\tau) \\bigg]$$\n",
    "\n",
    "One may say that case $\\theta = \\theta_{old}$ is a first (linear) term in approximation of $\\nabla_{\\theta}U(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One caveat is that rewards $R(\\tau)$ are gathered under policy and if new policy changes drastically, it invalidates reward.\n",
    "\n",
    "Goal is to reduce difference of old and new policies, so rewards can be reused.\n",
    "\n",
    "This can be done by enforcing $\\epsilon$ - small KL divergence $D_{KL}(\\pi||\\pi_{old}) \\leq \\epsilon$\n",
    "\n",
    "$$D_{KL}(P||Q) \\ \\dot = \\ \\sum_{x \\in X} P(x) \\cdot \\ log \\ \\bigg( \\frac{P(x)}{Q(x)}\\bigg)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to case of MC importance sampling, many product terms in ratio $\\frac{P(\\tau|\\theta)}{P(\\tau|\\theta_{old})}$ will cancel out, due to being the same for current environment.\n",
    "\n",
    "In fact expression reduces to\n",
    "$$D_{KL}\\bigg( P(\\tau|\\theta) \\ || \\ P(\\tau|\\theta_{old})\\bigg) = \\sum_\\tau P(\\tau|\\theta) \\cdot \\frac{\\prod_{t = 0}^{T} \\pi_\\theta(a_t|s_t)}{\\prod_{t = 0}^{T} \\pi_{\\theta_{old}}(a_t|s_t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "img {\n",
    "  display: block;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<img  src=\"https://spinningup.openai.com/en/latest/_images/math/5808864ea60ebc3702704717d9f4c3773c90540d.svg\" \n",
    "      width=460 \n",
    "      style=\"background-color:white\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 8. Start unpacking from the end:\n",
    "1. KL divergence of sample\n",
    "\n",
    "conjugate gradient method explored in\n",
    "\n",
    "[Notes_Method_of_Conjugate_Gradient_Descent.ipynb](../../optimization/Notes_Method_of_Conjugate_Gradient_Descent.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "step 9. uses backtracking line search. notes in\n",
    "\n",
    "[Notes_Line_search.ipynb](../../optimization/Notes_Line_search.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
