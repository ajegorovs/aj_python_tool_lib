{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __NEW ROADMAP__ (Irrelevant. see next sections for detailed explanation)\n",
    "\n",
    "https://github.com/openai/spinningup/blob/master/spinup/algos/tf1/trpo/core.py\n",
    "\n",
    "*   TRPO implemented using diagonal multivariate normal distributions (looking at two sources and some info in OG paper)\n",
    "*   gradient is calculated as usual, it will be remapped via inverse Hessian\n",
    "*   objective function contains KL div constraint, which is a Hessian of KL divergence.\n",
    "*   It can be computed fast and ez with conjugate gradient (CG). But we need to calculate KL div. Unlike using Fisher Information Matrix (FIM)\n",
    "\n",
    "STEPS\n",
    "\n",
    "0.  Run the experiment (this is somewhere inside point 1.)\n",
    "1.  Define MLP Gaussian policy\n",
    "    *   it takes in list of states and list of actions, which were conducted based on states using old version of policy\n",
    "    *   MLP produces mean value parameter for each action thats possible in a given state.\n",
    "\n",
    "        so if state list is of shape (steps, num_observables), MLP will produce parameters of shape (steps, num_actions). only mean. stdev is unique for each action\n",
    "\n",
    "    *   we sample actions using reparametrization trick: \n",
    "        $$a_\\pi \\sim \\mu(states) + \\N(\\theta)*\\sigma$$\n",
    "        are these logits? im not sure. should be same shape as \\mu-> (steps, num_actions)\n",
    "\n",
    "    *   compute log likelihood of $a_\\pi$, sum over actions dim (might be multiplication in log representation)\n",
    "\n",
    "    *   compute log likelihood for observed actions $a$. This implies that dims of $a$ and $a_\\pi$ are the same so no distribution over actions? Not possible, since mu has last dim of num_actions.\n",
    "\n",
    "    *   retrieve $\\mu$ and $\\sigma$ of old policy. What to do if num states were different??\n",
    "\n",
    "    *   compute analytic KL divergence using old and new distribution parameters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Compute Surrogate reward\n",
    "    *   ratio of log likelihoods new/old\n",
    "    *   times advantage -> objective f-n\n",
    "3.  Update weights\n",
    "    *   get MLP params\n",
    "    *   calculate grad w.r.t. objective f-n\n",
    "    *   define hessian-vector product function for arbitrary vector\n",
    "    *   perform CG iterations to get direction and initial step size\n",
    "    *   backtracking line search\n",
    "\n",
    "4. update value function (baseline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function - surrogate reward $U(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "(https://www.youtube.com/watch?v=KjWF8VIMGiY)\n",
    "\n",
    "*change section later* \n",
    "\n",
    "We can define a loss which, when optimized, will produce an incremental improvement to a policy with respect to old policy version.\n",
    "\n",
    "Given we have gathered trajectory using old policy, we can evaluate expected rewards using new policy via _Importance Sampling_.\n",
    "\n",
    "My explanation can be seen midway throughout [Sutton_P1_Ch05_Monte_Carlo.ipynb notebook (off-policy)](../../RL_Reinforced_Learning/Sutton_P1_Ch05_Monte_Carlo.ipynb)\n",
    "\n",
    "$$U(\\theta) = \\mathbb{E}_{\\tau \\sim \\theta_{old}} \\bigg[ \\frac{P(\\tau|\\theta)}{P(\\tau|\\theta_{old})} R(\\tau)\\bigg]$$\n",
    "> Using reward for whole trajectory $R(\\tau)$ is naive. Usually it is a function of time $R(t)$ -  a cumulative reward sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient $\\vec{g}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Gradient of this expected return is:\n",
    "$$\\vec{g} = \\nabla_{\\theta}U(\\theta) = \\mathbb{E}_{\\tau \\sim \\theta_{old}} \\bigg[ \\frac{\\nabla_{\\theta} P(\\tau|\\theta)}{P(\\tau|\\theta_{old})} R(\\tau)\\bigg]$$\n",
    "***\n",
    "_if old policy and new policy are the same $(\\theta = \\theta_{old} \\ ; \\ P(\\tau|\\theta) = P(\\tau|\\theta_{old}))$, via_\n",
    "$$\\nabla_{\\theta} \\ log P(\\tau|\\theta) = \\frac{\\nabla_{\\theta}P(\\tau|\\theta)}{P(\\tau|\\theta)}$$\n",
    "by expanding probability of trajectory and taking a derivative (system dynamics drops out)\n",
    "$$\\nabla_{\\theta} \\ log P(\\tau|\\theta) = \\sum_{t=0}^{T} \\nabla_{\\theta} \\ log \\ \\pi_\\theta(a_t|s_t)$$\n",
    "_we recover 'vanilla' policy gradient ([Simplest_Policy_Gradient](01_Simplest_Policy_Gradient_Implementations.ipynb)):_\n",
    "$$\\nabla_{\\theta} J(\\pi_{\\theta})  = \\underset{\\tau \\sim \\pi}{\\mathbb{E}}\\bigg[ \\sum_{t=0}^{T-1} \\nabla_{\\theta} \\ log  \\ \\pi_\\theta(a_t|s_t) \\cdot R(\\tau) \\bigg]$$\n",
    "\n",
    "_One may say that case $\\theta = \\theta_{old}$ is a first (linear) term in approximation of $\\nabla_{\\theta}U(\\theta)$. (not sure about this)_\n",
    "***\n",
    "If policies are different ($\\theta \\neq \\theta_{old}$), then gradient is\n",
    "$$\\vec{g} = \\nabla_{\\theta}U(\\theta) = \\mathbb{E}_{\\tau \\sim \\theta_{old}} \\bigg[ \\frac{P(\\tau|\\theta)}{P(\\tau|\\theta_{old})} \\sum_{t=0}^{T-1} \\nabla_{\\theta} \\ log \\ \\pi_\\theta(a_t|s_t) \\cdot R(\\tau)\\bigg]$$\n",
    "Which looks like importance-sampling version of \"Vanilla Gradient\"\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL divergence constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Similar to Natural Policy Gradient (NPG) [02_Natural_Policy_Gradient.ipynb](02_Natural_Policy_Gradient.ipynb), we want to constraint new policy to be similar to old policy. Again, we will do it via KL divergence \n",
    "$$D_{KL}(\\pi||\\pi_{old}) \\leq \\epsilon$$\n",
    "_Might better to write it for all possible actions $\\pi(\\cdot,s)$ and we want to limit maximal divergence encountered in any state_$s$\n",
    "$$ \\max_s D_{KL}(\\pi(\\cdot,s)|| \\pi_{old}(\\cdot,s)) \\leq \\epsilon$$\n",
    "_But in practice we might take __average__ over all $s$ instead_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NPG we used quadratic approximation to KL divergence (and we will do it again) in form of Fisher Information Matrix (FIM) ([Fisher_Information.ipynb](../../Statistics/Fisher_Information.ipynb)) which reused \n",
    "$$\\nabla_{\\theta} \\ log \\ \\pi_\\theta(a_t|s_t)$$\n",
    "needed for a gradient.\n",
    "\n",
    "Alternative approach (same but different) is to express KL divergence in terms of its Hessian $H$ ([KL_Divergence.ipynb](../../Statistics/KL_Divergence.ipynb))\n",
    "$$\\boxed{D_{KL}\\bigg(p(x; \\theta) ||p(x; \\theta + \\delta) \\bigg) \\overset{\\text{2nd order}}{\\approx} \\frac{1}{2}\\delta^T \\underbrace{H}_{\\nabla^2 D_{KL}} \\delta = \\frac{1}{2}(\\theta - \\theta_{old})^T H (\\theta - \\theta_{old}) }$$\n",
    "But this requires us to actually calculate $D_{KL}$. We will see how later.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because policy parameter update rule is the same (except we use Hessian instead of FIM)\n",
    "$$\\theta_{t+1} = \\theta_t + \\sqrt{\\frac{2\\epsilon}{\\vec{g}^T {H^{-1}} \\vec{g}}} H^{-1}\\vec{g}$$\n",
    "Term $H^{-1}\\vec{g}$ can be calculated (or approximated) via Conjugate Gradient (CG) method ([Notes_Method_of_Conjugate_Gradient_Descent.ipynb](../../optimization/Notes_Method_of_Conjugate_Gradient_Descent.ipynb)), which iteratively solves problem \n",
    "$$H \\vec{x} = \\vec{g} \\longrightarrow \\vec{x}^* = H^{-1} \\vec{g}$$\n",
    "Part of CG iterations is repeated calculation of Hessian-descent direction product $H \\vec{v}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because in problem $H \\vec{x} = \\vec{g}$ involves a Hessian matrix, we can rewrite it as a Jacobian of a gradient\n",
    "$$H(\\cdot)= \\underbrace{J}_{\\text{Jacobian}}\\nabla (\\cdot)$$\n",
    "and we can rewrite Hessian-vector product using only gradients\n",
    "$$H \\vec{v} = J\\big(\\nabla D_{KL}\\big) \\vec{v} = J\\big(\\nabla D_{KL} \\cdot \\vec{v}\\big) $$\n",
    "This to so called 'Hessian-vector product trick' ([CG_Hessian_vector_trick.ipynb](../../optimization/CG_Hessian_vector_trick.ipynb))\n",
    "\n",
    "In practice that means that we define a function $$H_v(\\vec{v}) = J\\big(\\nabla D_{KL} \\cdot \\vec{v}\\big)$$\n",
    "where $\\nabla D_{KL}$ is precomputed and outer Jacobian 'remembers' computational graph to make repeated calculations fast.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function (revisited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to use \"Hessian-vector trick\", we dont use FIM, so there is no need to calculate\n",
    "$$\\nabla_{\\theta} \\ log \\ \\pi_\\theta(a_t|s_t)$$\n",
    "But we see it in definition of $\\nabla_{\\theta}U(\\theta)$.\n",
    "\n",
    "Instead of defining $\\nabla_{\\theta}U(\\theta)$ explicitly, we can let auto-differentiation library do the 'heavy' lifting and calculate\n",
    "$$\\nabla_{\\theta} U(\\theta) = \n",
    "\\nabla_{\\theta}\n",
    "\\left( \n",
    "    \\mathbb{E}_{\\tau \\sim \\theta_{old}} \n",
    "        \\left[\\frac{P(\\tau|\\theta)}{P(\\tau|\\theta_{old})} R(\\tau)\\right]\n",
    "\\right)$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance sampling ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still its obvious that we have to calculate importance sampling ratio\n",
    "$\\rho = \\frac{P(\\tau|\\theta)}{P(\\tau|\\theta_{old})}$\n",
    "\n",
    "We know that\n",
    "$$P(\\tau|\\pi) = \\rho_0(s_0) \\cdot \\pi(a_0|s_0) \\cdot P(s_1|s_0,a_0)\\cdot\\pi(a_1|s_1)\\cdot P(s_2|s_1,a_1)\\cdot \\dots$$\n",
    "$$ = \\rho_0(s_0)\\cdot \\prod_{t=0}^{T-1} \\pi(a_t|s_t) \\cdot P(s_{t+1}|s_t,a_t)$$\n",
    "and by taking a ratio, system governed dynamics (and statics) $ P(s_{t+1}|s_t,a_t)$ will cancel out leaving\n",
    "$$\\rho = \\frac{P(\\tau|\\theta)}{P(\\tau|\\theta_{old})} = \\prod_{t=0}^{T-1} \\frac{\\pi(a_t|s_t)}{\\pi_{old}(a_t|s_t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_It is not clear whether importance sampling ratio multiplies whole reward (advantage function), or does it in a manner where prior history is not taken into account, similarly to rewards-to-go:_\n",
    "$$L(\\tau) = \\sum_{t=0}^{T-1} \\ log  \\ \\pi_\\theta(a_t|s_t) \\cdot \\bigg(\\sum_{t^\\prime = t}^{T-1} R(s_{t^\\prime}, a_{t^\\prime}, s_{t^\\prime +1}) -b(s_t)\\bigg)$$\n",
    "_in this case objective function can be defined as_\n",
    "$$U(\\theta) = \\mathbb{E}_{\\tau \\sim \\theta_{old}} \n",
    "        \\left[\\prod_{t=0}^{T-1} \\frac{\\pi(a_t|s_t)}{\\pi_{old}(a_t|s_t)} \\cdot \n",
    "                \\bigg( \n",
    "                    \\underbrace{R(s_{t^\\prime}, a_{t^\\prime}, s_{t^\\prime +1}) -b(s_t)}_{R(t)}\n",
    "                \\bigg)\\right]$$\n",
    "***\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy parametrization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Reminder:<br>Simple environments with small number of observation space can be dealt with by tabular approach. When observation space becomes large/continuous, we have to use approximations that generalize. Common case is to use neural networks (NNs), in simplest case Multi Layer Perceptron.\n",
    "\n",
    "Depending on whether action space is discrete or continuous, we will have a different approach on how we build NN and produce actions.\n",
    "\n",
    "Our goal is to calculate for a policy:\n",
    "1. probability (likelihood) of a trajectory $\\prod_{t=0}^{T-1} \\pi(a_t|s_t)$\n",
    "2. KL divergence $D_{KL}$\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete actions (easy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here we will consider that agent can take only one action from list on $N_{act}$ actions at a time. \n",
    "\n",
    "> In discrete action case actions are simply integers $[0,1,2,\\dots]$, an abstraction for what they mean in environment [North, East, North-East,...]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a policy implemented via NN we want to feed in a state (a list of states) and for NN to produce list of probabilities for each action in that state (for each state in a list).\n",
    "$$\\vec{\\pi }(s)= \\{\\pi(a_1|s),\\dots,\\pi(a_{N_{act}}|s)\\}^T$$\n",
    "Setting NN to produce probabilities directly is difficult, because $\\vec{\\pi(s)}$ has specific requirements\n",
    "$$|\\vec{\\pi}(s)|_1 = \\sum_a \\pi(a|s)= 1; \\  0\\leq \\pi(\\cdot|s) \\leq 1$$\n",
    "Common method is to produce 'logits' $e_i$ which are turned into probabilities via 'Softmax':\n",
    "$$\\pi(a_i|s) = \\frac{\\exp(e_{i})}{\\sum_{k=1}^{N_{act}} \\exp(e_{k})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In practice we feed a trajectory of states\n",
    "> $$\\vec{s} = \\{s_1,\\dots,s_N\\}^T$$\n",
    "> and NN (via Softmax) retrieves a matrix \n",
    "> $$P:[N \\times N_{actions}] = \\{\\vec{\\pi }(s_1),\\dots,\\vec{\\pi }(s_N)\\}^T$$ \n",
    "> each prob dist for a state can be sampled.\n",
    "\n",
    "> Additionally we will define logP for KL divergence\n",
    "> $$\\log P = \\{\\log \\vec{\\pi }(s_1),\\dots,\\log \\vec{\\pi }(s_N)\\}^T= \\log\\begin{bmatrix} \\pi(a_1,s_1) & \\pi(a_2,s_1) \\\\ \\pi(a_1,s_2) & \\pi(a_2,s_2) \\end{bmatrix}$$ \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling actions \n",
    "$$a_i\\sim \\pi(\\cdot|s_i)$$\n",
    "can be done via categorical distribution.\n",
    "\n",
    ">In pytorch we can use <br>`torch.distributions.categorical.Categorical(probs=None, logits=None, validate_args=None)`, </br> which accepts input in either form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We sample $P$ for what action policy _thinks is the best_ based on current NN parameters $\\theta$\n",
    "> $$\\vec{a}_\\pi = \\{a_{\\pi,1},\\dots,a_{\\pi,N}\\}^T$$\n",
    "> We also feed list of actions that previous version of NN took at steps $\\vec{s}$\n",
    "> $$\\vec{a}_{old} = \\{a_{old, 1},\\dots,a_{old, N}\\}^T$$\n",
    "> We extract new probabilities from $P$ that correspond to $\\vec{a}_\\pi$ and $\\vec{a}_{old}$\n",
    "> $$\\vec{\\pi}= \\{\\pi(a_{\\pi, 1}, s_1), \\dots, \\pi(a_{\\pi, N}, s_N)\\}$$\n",
    "> $$\\vec{\\pi}_{old} = \\{\\pi(a_{old, 1}, s_1), \\dots, \\pi(a_{old, N}, s_N)\\}$$\n",
    "> And for convenience in log-form\n",
    "> $$ \\log \\vec{\\pi}\\ ; \\ \\log \\vec{\\pi}_{old}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood of a trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Calculation of likelihood of trajectory $\\prod_{t=0}^{T-1} \\pi(a_t|s_t)$ is trivial. Except, maybe, for need to extract entry relevant to a given entry. -->\n",
    "As mentioned earlier, its most likely that importance sampling ratio is not multiplied, but before interleaved ith rewards-to-go\n",
    "$$U(\\theta) = \\mathbb{E}_{\\tau \\sim \\theta_{old}} \n",
    "        \\left[\n",
    "            \\prod_{t=0}^{T-1} \\frac{\\pi(a_t|s_t)}{\\pi_{old}(a_t|s_t)} \\cdot \n",
    "                R(t)\n",
    "        \\right]$$\n",
    "> Which is why we are isolating individual term\n",
    ">$$\\rho_t = \\frac{\\pi(a_t|s_t)}{\\pi_{old}(a_t|s_t)}$$\n",
    ">by\n",
    ">$$ \\vec{\\rho} = \\exp \\left(\\log \\vec{\\pi}-\\log \\vec{\\pi}_{old}\\right) = \\exp \\left\\{log \\frac{\\pi(a_{\\pi, 1})}{\\pi(a_{old, 1}, s_1)}, \\dots \\right\\} = \\left\\{\\frac{\\pi(a_{\\pi, 1})}{\\pi(a_{old, 1}, s_1)}, \\dots \\right\\}$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean KL divergence for discrete case.\n",
    "\n",
    "Instead of selected probabilities for actions, we have to compare all probabilities. \n",
    "> We have computed them previously\n",
    "$$\\log P = \\{\\log \\vec{\\pi }(s_1),\\dots,\\log \\vec{\\pi }(s_N)\\}^T= \\log\\begin{bmatrix} \\pi(a_1,s_1) & \\pi(a_2,s_1) \\\\ \\pi(a_1,s_2) & \\pi(a_2,s_2) \\end{bmatrix}$$\n",
    "We KL sum divergence over actions in each observed state (axis = -1)\n",
    "$$D_{KL}(\\pi_{old}(\\cdot|s_i)||\\pi(\\cdot|s_i)) =\\sum_a \\pi_{old}(a|s_i) \\bigg(\\log \\pi_{old}(a|s_i) - \\log \\pi(a|s_i)\\bigg) $$\n",
    "and take a mean over all states\n",
    "$$\\bar D_{KL}(\\pi_{old}||\\pi) =   \\frac{1}{T}\\sum_{i=1}^{T} D_{KL}(\\pi_{old}(\\cdot|s_i)||\\pi(\\cdot|s_i))$$\n",
    ">_For some reason in implementation (https://github.com/openai/spinningup/blob/master/spinup/algos/tf1/trpo/core.py) authors compare old to new policy, not the other way around. Although its not symmetric. Id personally follow derivation of KL div approximation. But ill order it their way._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous actions (harder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All implementations for TRPO where action space is continuous parametrize policy as normal distribution $\\mathcal{N}_\\pi(\\mu,\\sigma^2)$.\n",
    "Specifically multivariate diagonal normal distribution.\n",
    "\n",
    "> Use of classic distributions allows to pre-compute various parameters analytically, which reduces overall computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our environment receives $N_{act}$ continuous actions, then policy for each state $s$ should produce $N_{act}$ probability distributions, from which individual actions will sampled.\n",
    "$$\\text{Action dist. for state s} = \\bigg\\{ \\mathcal{N}(\\mu_{1},\\sigma^2_1), \\ \\dots, \\ \\mathcal{N}(\\mu_{N_{act}},\\sigma^2_{N_{act}}) \\bigg\\}$$\n",
    "It is the same as defining multivariate gaussian with set of parameters\n",
    "$$\\vec{\\mu} = \\{ \\mu_1, \\dots, \\mu_{N_{act}} \\}$$\n",
    "$$\\vec{\\sigma} = \\{\\sigma_1, \\dots, \\sigma_{N_{act}} \\}$$\n",
    "\n",
    "> If given a list of $N$ states, we stack vectors of parameters in a larger $N \\times N_{acts}$ matrix.\n",
    "\n",
    "__NOTE:__ It is not necessary to generate individual $\\sigma_i$. Each action can have universal parameter.\n",
    "\n",
    "_Reminder: Of course these parameters are implicitly dependent on weights $\\theta$ of NN $\\mu_i = \\mu_i(\\theta)$._\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling action values for $\\mathcal{N}_\\pi(\\mu,\\sigma^2)$ can be done via \"reparemetrization trick\", used in Variational Autoencoders (VAE) (its used there to enable backprop):\n",
    "$$\\vec{a} \\sim \\vec{\\mu}+ \\mathcal{N}_\\pi(\\vec{0},\\vec{1})\\cdot\\vec{\\sigma}$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood of trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 1D case likelihood that N i.i.d entries $x_i$ came from gaussian distribution, can be computed as follows ([Gaussian_Properties.ipynb](../../Statistics/Gaussian_Properties.ipynb))\n",
    "$$L(\\sigma^2;\\mu, \\vec{x}) = \\prod_{i=1}^N \\mathcal{N}_\\pi(x_i, \\sigma^2; \\mu)= \\left(\\frac{1}{\\sqrt{2\\sigma^2\\pi}}\\right)^N \\exp \\left(-\\frac{1}{2\\sigma^2}\\sum_{i = 1}^N (x_i-\\mu)^2\\right)$$\n",
    "For computation stability we can calcualte log-likelihood:\n",
    "$$l(\\sigma^2;\\mu, \\vec{x}) = \\log \\left(L(\\sigma^2;\\mu, \\vec{x})\\right)=-\\frac{1}{2}\\left( N\\log 2\\pi + 2 N \\log \\sigma + \\left(\\frac{\\sum_{i = 1}^N x_i-\\mu}{\\sigma}\\right)^2\\right) $$\n",
    "> Reminder: $\\mu$ ($\\sigma$ is unversal) parametrize state. Main distribution argument is action $x_i = a_i$.<br>\n",
    "> In practice we will compute log-likelihood for individual state-action ($N=1$)\n",
    "$$\\boxed{l(\\sigma_i;\\mu_i, a_i) = -\\frac{1}{2}\\left( \\log 2\\pi + 2 \\log \\sigma_i + \\left(\\frac{a_i-\\mu_i}{\\sigma_i}\\right)^2\\right)}$$\n",
    "Log form is needed for KL divergence. We undo log by exponentiation\n",
    "$$L(\\sigma_i;\\mu_i, a_i)= \\exp \\left(l(\\sigma_i;\\mu_i, a_i)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which allows us to compute importance-sampling ratio\n",
    "$$\\rho_t = \\sum_{a}\\exp \\frac{l(\\sigma^2;\\mu, a)}{l(\\sigma^2_{old};\\mu_{old}, a)}$$\n",
    "We have same considerations of $R(t)$ being time dependent, so we want to keep each each term $\\rho_t$ of $\\vec{\\rho}$ separately.\n",
    "\n",
    "$$U(\\theta) = \\mathbb{E}_{\\tau \\sim \\theta_{old}} \\bigg[ \\frac{P(\\tau|\\theta)}{P(\\tau|\\theta_{old})} R(\\tau,t)\\bigg] \\rightarrow\n",
    "\\mathbb{E}_{\\tau \\sim \\theta_{old}} \\bigg[ \\prod_{t=0}^{T-1} \\rho_t \\cdot R(t)\\bigg] = \\frac{1}{T_{all}} \\prod_{t=0}^{T_{all}} \\rho_t \\cdot R(t)$$\n",
    "> as in previous methods we batch many trajectories together. Have to concatenate rewards/advantages accordingly.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KL divergence (not complete /correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kl divergence can be calculated analytically for two normal multivariate diagonal distributions\n",
    "([Gaussian_Properties.ipynb](../../Statistics/Gaussian_Properties.ipynb))\n",
    "$$\\boxed{D_{KL}\\left(\\mathcal{N}_\\pi(\\sigma^2_1; \\mu_1)||\\mathcal{N}_\\pi(\\sigma^2_2; \\mu_2)\\right) = \\frac{1}{2}\\left(\\frac{(\\mu_1 -\\mu_2)^2 +\\sigma_1^2}{\\sigma_2^2} - 1\\right) + \\log \\sigma_2 - \\log \\sigma_1}$$\n",
    "We compute sum over actions\n",
    "$$D_{KL}(\\pi(\\cdot|s_i)||\\pi_{old}(\\cdot|s_i)) = \\sum_{i\\in \\vec{a}} \\frac{1}{2}\\left(\\frac{(\\vec{\\mu_1}_i-\\vec{\\mu_2}_i)^2 +\\vec{\\sigma_1}_i^2}{\\vec{\\sigma_2}_i^2} - 1\\right) + \\log \\vec{\\sigma_2 }_i- \\log \\vec{\\sigma_1}_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "img {\n",
    "  display: block;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<img  src=\"https://spinningup.openai.com/en/latest/_images/math/5808864ea60ebc3702704717d9f4c3773c90540d.svg\" \n",
    "      width=460 \n",
    "      style=\"background-color:white\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 8. Start unpacking from the end:\n",
    "1. KL divergence of sample\n",
    "\n",
    "conjugate gradient method explored in\n",
    "\n",
    "[Notes_Method_of_Conjugate_Gradient_Descent.ipynb](../../optimization/Notes_Method_of_Conjugate_Gradient_Descent.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "step 9. uses backtracking line search. notes in\n",
    "\n",
    "[Notes_Line_search.ipynb](../../optimization/Notes_Line_search.ipynb)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
