{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from misc_tools.print_latex import print_tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env=gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=True,render_mode=\"ansi\")#,render_mode=\"human\"\n",
    "env.reset()\n",
    "#env.close()\n",
    "def p_sp_sa(state, action):\n",
    "    1\n",
    "print(env.render())\n",
    "rewards = np.array(env.reward_range)# [0,1]\n",
    "num_states= env.observation_space.n\n",
    "num_actions = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(s', a) normalized: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.33333333, 0.33333333,\n",
       "       0.33333333])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "policy = np.ones((num_states, num_actions))/num_actions\n",
    "state_value = np.zeros((num_states,))\n",
    "p_spr_sa = np.zeros((num_states,  num_actions, num_states, rewards.size))\n",
    "for state, actions_d in env.P.items():\n",
    "    for action, transitions in actions_d.items():\n",
    "        for (p, state_tar, reward, is_terminal) in transitions:\n",
    "            p_spr_sa[state, action, state_tar, int(reward)] += p   \n",
    "            # apparently walls actions transition to same state = different actions to same state. so += p\n",
    "print(f'(s\\', a) normalized: {np.all(p_spr_sa.sum(axis=(-2,-1)) == 1)}')\n",
    "#norm = p_spr_sa.sum(axis=(-2,-1), keepdims = 1)\n",
    "#p_spr_sa = np.divide(p_spr_sa, norm, out=np.zeros_like(p_spr_sa), where= (norm != 0))\n",
    "p_spr_sa[14,1].sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from [RL_Reinforced_Learning/Meaning_of_Psasr_probability_matrix.ipynb](data_processing/neural_networks/RL_Reinforced_Learning/Meaning_of_Psasr_probability_matrix.ipynb)\n",
    "\n",
    "$p(s^\\prime, r|s,a)$ is $P[s,a]$, an entry of matrix $P$; $r_i$ is i-th entry $ \\vec{R}[i]$ of vector $ \\vec{R}$.\n",
    "$$\\sum_{r \\in R} r \\cdot p(s^\\prime, r|s,a)$$\n",
    "is a $[s,a]$ entry of matrix multiplication \n",
    "$$Q[s,a] = P[s,a]  \\vec{R}; \\ \\text{dim}(Q) = [s^\\prime,r]\\times[r] = [s^\\prime]$$\n",
    "$$\\boxed{r(s,a) =  \\sum_{r \\in R} \\sum_{s^\\prime \\in S} r \\cdot p(s^\\prime, r|s,a) = \\sum_{s^\\prime \\in S} (P \\vec{R})_{s,a,s^\\prime}}$$\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boxed{r(s,a,s^\\prime) = \\frac{\\sum_r  r\\cdot p(s^\\prime,r|s,a)}{p(s^\\prime|s,a)}}$$\n",
    "$$\\boxed{\n",
    "p(s^\\prime|s,a) = \\sum_r p(s^\\prime,r|s,a)}$$\n",
    "$$q_{\\pi}(s,a) = \\sum_{r \\in R} \\sum_{s^\\prime \\in S} p(s^\\prime, r|s,a) \\cdot \\biggl[ r + \\gamma \\cdot v_{\\pi}(s^\\prime)\\biggr]=$$\n",
    "$$ =  \\sum_{s^\\prime \\in S} \\left[\\sum_{r \\in R} p(s^\\prime, r|s,a) \\cdot r + \\gamma \\sum_{r \\in R} p(s^\\prime, r|s,a) \\cdot v_{\\pi}(s^\\prime)\\right] = $$\n",
    "$$ = \\sum_{s^\\prime \\in S} \\left[p(s^\\prime|s,a)\\cdot r(s,a,s^\\prime) + \\gamma \\cdot v_{\\pi}(s^\\prime) \\cdot \\sum_{r \\in R} p(s^\\prime, r|s,a) \\right] =$$ \n",
    "$$ = \\sum_{s^\\prime \\in S} \\biggl[  p(s^\\prime|s,a)\\cdot r(s,a,s^\\prime) + \\gamma \\cdot v_{\\pi}(s^\\prime) \\cdot p(s^\\prime|s,a) \\biggr] =$$ \n",
    "$$= \\sum_{s^\\prime \\in S} p(s^\\prime|s,a)\\cdot\\biggl[r(s,a,s^\\prime) + \\gamma \\cdot v_{\\pi}(s^\\prime)\\biggr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_sp_sa = p_spr_sa.sum(-1)\n",
    "r_sas_top = p_spr_sa @ rewards\n",
    "r_sas = np.divide(r_sas_top, p_sp_sa, out = np.zeros_like(r_sas_top), where= (p_sp_sa != 0))\n",
    "\n",
    "#a = r_sas + gamma*state_value\n",
    "#print_tex((p_sp_sa* a).sum((-1)))\n",
    "#r_sas[14,1,15], p_sp_sa[14,1,15]\n",
    "#r_sas_top.shape, p_sp_sa.shape, r_sas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "$$v_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s) \\sum_{r \\in R} \\sum_{s^\\prime \\in S} p(s^\\prime, r|s,a) [ r + \\gamma v_{\\pi}(s^\\prime)] = $$\n",
    "$$ = \\sum_{a \\in A} \\pi(a|s) \\cdot q_{\\pi}(s,a) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\begin{bmatrix} 0.068891 & 0.061415 & 0.074410 & 0.055807 \\\\ 0.091855 & 0 & 0.112208 & 0 \\\\ 0.145436 & 0.247497 & 0.299618 & 0 \\\\ 0 & 0.379936 & 0.639020 & 0 \\end{bmatrix}\\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 1 & 0 & 0 & 0 \\\\ 1/4 & 1/4 & 1/4 & 1/4 \\\\ 1/2 & 0 & 1/2 & 0 \\\\ 1/4 & 1/4 & 1/4 & 1/4 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ 1/4 & 1/4 & 1/4 & 1/4 \\\\ 1/4 & 1/4 & 1/4 & 1/4 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 1/4 & 1/4 & 1/4 & 1/4 \\end{bmatrix}\\begin{bmatrix} 0.068891 & 0.066648 & 0.066648 & 0.059759 \\\\ 0.039092 & 0.042990 & 0.040747 & 0.061415 \\\\ 0.074410 & 0.068829 & 0.072728 & 0.057489 \\\\ 0.039065 & 0.039065 & 0.033484 & 0.055807 \\\\ 0.091855 & 0.071187 & 0.064298 & 0.048224 \\\\ 0 & 0 & 0 & 0 \\\\ 0.112208 & 0.089885 & 0.112208 & 0.022323 \\\\ 0 & 0 & 0 & 0 \\\\ 0.071187 & 0.117880 & 0.101805 & 0.145436 \\\\ 0.157612 & 0.247497 & 0.203866 & 0.133516 \\\\ 0.299618 & 0.265955 & 0.225369 & 0.107912 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0.188230 & 0.305687 & 0.379936 & 0.265955 \\\\ 0.395572 & 0.639020 & 0.614925 & 0.537199 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#PR = (p_spr_sa @ rewards)#.sum(axis=(-1))\n",
    "\n",
    "def get_q_sa(state_value):\n",
    "   return (p_sp_sa * (r_sas + gamma*state_value)).sum(-1)\n",
    "\n",
    "def evaluate_policy(state_action_value, policy):\n",
    "   v_pi = (policy * state_action_value).sum(-1)\n",
    "   return v_pi\n",
    "\n",
    "def evaluate_policy_k(state_value, policy, k):\n",
    "   v_pi = state_value.copy()\n",
    "   for _ in range(k):\n",
    "      q_pi  = get_q_sa(v_pi)\n",
    "      v_pi  = evaluate_policy(q_pi, policy)\n",
    "   q_pi  = get_q_sa(v_pi)\n",
    "   return v_pi, q_pi\n",
    "\n",
    "def improve_policy(state_action_value):\n",
    "   a = state_action_value\n",
    "   max_values = np.max(a, axis=-1)\n",
    "   max_indices = [np.where(a[i] == max_values[i])[0] for i in range(a.shape[0])]\n",
    "   max_indices\n",
    "\n",
    "   one_hot_matrix = np.zeros_like(a)\n",
    "\n",
    "   for i, indices in enumerate(max_indices):\n",
    "      one_hot_matrix[i, indices] = 1/len(indices)\n",
    "   max_indices,one_hot_matrix\n",
    "\n",
    "   return one_hot_matrix\n",
    "\n",
    "v_pi  = state_value.copy()\n",
    "pi    = policy.copy()\n",
    "for _ in range(50):\n",
    "   v_pi, q_pi = evaluate_policy_k(v_pi, pi, 50)\n",
    "   pi = improve_policy(q_pi)\n",
    "   \n",
    "\n",
    "#print_tex(v_pi.reshape(4,4))\n",
    "print_tex(v_pi.reshape(4,4),pi,q_pi)\n",
    "\n",
    "#print_tex(sv2.reshape(4,4),valueFunctionVectorComputed.reshape(4,4))\n",
    "# print(env.render())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatePolicy(env,valueFunctionVector,policy,discountRate,maxNumberOfIterations,convergenceTolerance):\n",
    "    import numpy as np\n",
    "    convergenceTrack=[]\n",
    "    for iterations in range(maxNumberOfIterations):\n",
    "        convergenceTrack.append(np.linalg.norm(valueFunctionVector,2))\n",
    "        valueFunctionVectorNextIteration=np.zeros(env.observation_space.n)\n",
    "        for state in env.P:\n",
    "            outerSum=0\n",
    "            for action in env.P[state]:\n",
    "                innerSum=0\n",
    "                for probability, nextState, reward, isTerminalState in env.P[state][action]:\n",
    "                    #print(probability, nextState, reward, isTerminalState)\n",
    "                    innerSum=innerSum+ probability*(reward+discountRate*valueFunctionVector[nextState])\n",
    "                outerSum=outerSum+policy[state,action]*innerSum\n",
    "            valueFunctionVectorNextIteration[state]=outerSum\n",
    "        if(np.max(np.abs(valueFunctionVectorNextIteration-valueFunctionVector))<convergenceTolerance):\n",
    "            valueFunctionVector=valueFunctionVectorNextIteration\n",
    "            print('Iterative policy evaluation algorithm converged!')\n",
    "            break\n",
    "        valueFunctionVector=valueFunctionVectorNextIteration       \n",
    "    return valueFunctionVector\n",
    "\n",
    "def improvePolicy(env,valueFunctionVector,numberActions,numberStates,discountRate):\n",
    "    import numpy as np\n",
    "    # this matrix will store the q-values (action value functions) for every state\n",
    "    # this matrix is returned by the function \n",
    "    qvaluesMatrix=np.zeros((numberStates,numberActions))\n",
    "    # this is the improved policy\n",
    "    # this matrix is returned by the function\n",
    "    improvedPolicy=np.zeros((numberStates,numberActions))\n",
    "     \n",
    "    for stateIndex in range(numberStates):\n",
    "        # computes a row of the qvaluesMatrix[stateIndex,:] for fixed stateIndex, \n",
    "        # this loop iterates over the actions\n",
    "        for actionIndex in range(numberActions):\n",
    "            # computes the Bellman equation for the action value function\n",
    "            for probability, nextState, reward, isTerminalState in env.P[stateIndex][actionIndex]:\n",
    "                qvaluesMatrix[stateIndex,actionIndex]=qvaluesMatrix[stateIndex,actionIndex]+probability*(reward+discountRate*valueFunctionVector[nextState])\n",
    "             \n",
    "        # find the action indices that produce the highest values of action value functions\n",
    "        bestActionIndex=np.where(qvaluesMatrix[stateIndex,:]==np.max(qvaluesMatrix[stateIndex,:]))\n",
    " \n",
    "        # form the improved policy        \n",
    "        improvedPolicy[stateIndex,bestActionIndex]=1/np.size(bestActionIndex)\n",
    "    return improvedPolicy,qvaluesMatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterative policy evaluation algorithm converged!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.00447465, 0.00422061, 0.01006512, 0.00411696],\n",
       "       [0.00672042, 0.        , 0.02633306, 0.        ],\n",
       "       [0.01867524, 0.05760645, 0.10697145, 0.        ],\n",
       "       [0.        , 0.13038262, 0.39148975, 0.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discountRate=0.9\n",
    "stateNumber=16\n",
    "actionNumber=4\n",
    "\n",
    "maxNumberOfIterationsOfPolicyIteration=30\n",
    "initialPolicy=(1/actionNumber)*np.ones((stateNumber,actionNumber))\n",
    "valueFunctionVectorInitial=np.zeros(env.observation_space.n)\n",
    "# maximum number of iterations of the iterative policy evaluation algorithm\n",
    "maxNumberOfIterationsOfIterativePolicyEvaluation=1000\n",
    "# convergence tolerance \n",
    "convergenceToleranceIterativePolicyEvaluation=10**(-6)\n",
    "currentPolicy=initialPolicy\n",
    "valueFunctionVectorComputed =evaluatePolicy(env,valueFunctionVectorInitial,currentPolicy,discountRate,maxNumberOfIterationsOfIterativePolicyEvaluation,convergenceToleranceIterativePolicyEvaluation)\n",
    "improvedPolicy,qvaluesMatrix=improvePolicy(env,valueFunctionVectorComputed,actionNumber,stateNumber,discountRate)\n",
    "valueFunctionVectorComputed.reshape(4,4)\n",
    "#improvedPolicy\n",
    "# qvaluesMatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
