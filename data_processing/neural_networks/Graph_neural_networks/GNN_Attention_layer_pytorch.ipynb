{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input example : \n",
      ">>> arr_T = np.array([[r'\\vec{v}_1', r'\\vec{v}_2']]).T\n",
      ">>> print_tex(arr_T,'=', np.arange(1,5).reshape(2,-1)/4, r'; symbols: \\otimes, \\cdot,\\times')\n",
      "output: \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\begin{bmatrix}\\vec{v}_1 \\\\ \\vec{v}_2\\end{bmatrix}=\\begin{bmatrix} 1/4 & 1/2 \\\\ 3/4 & 1 \\end{bmatrix}; symbols: \\otimes, \\cdot,\\times$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch#, time\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cpu\")# torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from misc_tools.print_latex import print_tex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For derivation see GNN_Attention_notes.ipynb\n",
    "Im not sure whether implement unique $\\vec{a}$ for all heads. Original paper hints on that. Implementation i have found reuses it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle A = \\begin{bmatrix} 0 & 1 & 1 & 0 & 0 \\\\ 1 & 0 & 0 & 1 & 0 \\\\ 1 & 0 & 0 & 1 & 1 \\\\ 0 & 1 & 1 & 0 & 1 \\\\ 0 & 0 & 1 & 1 & 0 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "G_house = nx.house_graph()\n",
    "A = nx.adjacency_matrix(G_house).todense()\n",
    "print_tex('A = ', A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N_FEATURES, N_NODES = A.shape\n",
    "N_NODES = len(G_house.nodes())\n",
    "N_FEATURES = 2\n",
    "N_HIDDEN = 2\n",
    "N_HEADS = 2\n",
    "OUT_FEATURES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle H = \\begin{bmatrix} 0 & 1/2 \\\\ 1 & 3/2 \\\\ 2 & 5/2 \\\\ 3 & 7/2 \\\\ 4 & 9/2 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "H = 0.5*torch.arange(N_NODES*N_FEATURES, dtype = float).view(N_NODES, N_FEATURES)\n",
    "print_tex('H = ', H.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create matrix transformed embeddings (concatenated N_HEADS (K) blocks of shape (N_nodes,N_HIDDEN):\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle G_K = H W_{K=1}^T || H W_{K=2}^T = G_{K=1} || G_{K=2}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle G_K = H W_K^T = \\begin{bmatrix} 0 & 1/2 \\\\ 1 & 3/2 \\\\ 2 & 5/2 \\\\ 3 & 7/2 \\\\ 4 & 9/2 \\end{bmatrix}\\begin{bmatrix} 2 & 0 & 4 & 0 \\\\ 0 & 2 & 0 & 4 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 & 0 & 2 \\\\ 2 & 3 & 4 & 6 \\\\ 4 & 5 & 8 & 10 \\\\ 6 & 7 & 12 & 14 \\\\ 8 & 9 & 16 & 18 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshape G_K form (N_NODES, N_HEADS*N_FEATURES) to (N_NODES, N_HEADS, N_FEATURES)\n",
      "Prepare C_K: stacked N_HEADS attention matrices of shape (N_NODES, N_NODES, N_HEADS, 2*N_FEATURES)\n",
      "Each entry i,j of C_K holds N_HEADS of concatenated feature pair which via attention mechanism will determine weights if edge(i,j).\n",
      "Concatenation is not broadcasted. Create it from flattened features that have proper ordering. shape (N_NODES*N_NODES, N_HEADS, 2*N_FEATURES)\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Repeat_{flat} \\ (K=1): \\begin{bmatrix} 0 & 1 \\\\ 0 & 1 \\\\ 0 & 1 \\\\ 0 & 1 \\\\ 0 & 1 \\\\ 2 & 3 \\\\ 2 & 3 \\\\ 2 & 3 \\\\ 2 & 3 \\\\ 2 & 3 \\\\ 4 & 5 \\\\ 4 & 5 \\\\ 4 & 5 \\\\ 4 & 5 \\\\ 4 & 5 \\\\ 6 & 7 \\\\ 6 & 7 \\\\ 6 & 7 \\\\ 6 & 7 \\\\ 6 & 7 \\\\ 8 & 9 \\\\ 8 & 9 \\\\ 8 & 9 \\\\ 8 & 9 \\\\ 8 & 9 \\end{bmatrix}Interleave_{flat} \\ (K=1): \\begin{bmatrix} 0 & 1 \\\\ 2 & 3 \\\\ 4 & 5 \\\\ 6 & 7 \\\\ 8 & 9 \\\\ 0 & 1 \\\\ 2 & 3 \\\\ 4 & 5 \\\\ 6 & 7 \\\\ 8 & 9 \\\\ 0 & 1 \\\\ 2 & 3 \\\\ 4 & 5 \\\\ 6 & 7 \\\\ 8 & 9 \\\\ 0 & 1 \\\\ 2 & 3 \\\\ 4 & 5 \\\\ 6 & 7 \\\\ 8 & 9 \\\\ 0 & 1 \\\\ 2 & 3 \\\\ 4 & 5 \\\\ 6 & 7 \\\\ 8 & 9 \\end{bmatrix}C_{flat} \\ (K=1): \\begin{bmatrix} 0 & 1 & 0 & 1 \\\\ 0 & 1 & 2 & 3 \\\\ 0 & 1 & 4 & 5 \\\\ 0 & 1 & 6 & 7 \\\\ 0 & 1 & 8 & 9 \\\\ 2 & 3 & 0 & 1 \\\\ 2 & 3 & 2 & 3 \\\\ 2 & 3 & 4 & 5 \\\\ 2 & 3 & 6 & 7 \\\\ 2 & 3 & 8 & 9 \\\\ 4 & 5 & 0 & 1 \\\\ 4 & 5 & 2 & 3 \\\\ 4 & 5 & 4 & 5 \\\\ 4 & 5 & 6 & 7 \\\\ 4 & 5 & 8 & 9 \\\\ 6 & 7 & 0 & 1 \\\\ 6 & 7 & 2 & 3 \\\\ 6 & 7 & 4 & 5 \\\\ 6 & 7 & 6 & 7 \\\\ 6 & 7 & 8 & 9 \\\\ 8 & 9 & 0 & 1 \\\\ 8 & 9 & 2 & 3 \\\\ 8 & 9 & 4 & 5 \\\\ 8 & 9 & 6 & 7 \\\\ 8 & 9 & 8 & 9 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle E = \\sigma(\\vec{a}[C_K])$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To make attention vector interpretable for debug, fill it with ones. Dot = element sum.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Attention \\ vector = \\begin{bmatrix} 1 & 1 & 1 & 1 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply attention and activation to C_K \n",
      "(N_NODES, N_NODES, N_HEADS, 2*N_FEATURES) . (2*N_FEATURES, 1) -> (N_NODES, N_NODES, N_HEADS, 2*N_FEATURES, 1) -> squeeze last\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle E_{K=1} = \\begin{bmatrix} 2 & 6 & 10 & 14 & 18 \\\\ 6 & 10 & 14 & 18 & 22 \\\\ 10 & 14 & 18 & 22 & 26 \\\\ 14 & 18 & 22 & 26 & 30 \\\\ 18 & 22 & 26 & 30 & 34 \\end{bmatrix}\\rightarrow MASK \\rightarrow \\begin{bmatrix} 0 & 6 & 10 & 0 & 0 \\\\ 6 & 0 & 0 & 18 & 0 \\\\ 10 & 0 & 0 & 22 & 26 \\\\ 0 & 18 & 22 & 0 & 30 \\\\ 0 & 0 & 26 & 30 & 0 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle E_{K=2} = \\begin{bmatrix} 4 & 12 & 20 & 28 & 36 \\\\ 12 & 20 & 28 & 36 & 44 \\\\ 20 & 28 & 36 & 44 & 52 \\\\ 28 & 36 & 44 & 52 & 60 \\\\ 36 & 44 & 52 & 60 & 68 \\end{bmatrix}\\rightarrow MASK \\rightarrow \\begin{bmatrix} 0 & 12 & 20 & 0 & 0 \\\\ 12 & 0 & 0 & 36 & 0 \\\\ 20 & 0 & 0 & 44 & 52 \\\\ 0 & 36 & 44 & 0 & 60 \\\\ 0 & 0 & 52 & 60 & 0 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply row-wise softmax:\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\alpha_{K=1} = \\begin{bmatrix} 0 & 0.017986 & 0.982014 & 0 & 0 \\\\ 0.000006 & 0 & 0 & 0.999994 & 0 \\\\ 0.000000 & 0 & 0 & 0.017986 & 0.982014 \\\\ 0 & 0.000006 & 0.000335 & 0 & 0.999659 \\\\ 0 & 0 & 0.017986 & 0.982014 & 0 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\alpha_{K=2} = \\begin{bmatrix} 0 & 0.000335 & 0.999665 & 0 & 0 \\\\ 0.000000 & 0 & 0 & 1.000000 & 0 \\\\ 0 & 0 & 0 & 0.000335 & 0.999665 \\\\ 0 & 0.000000 & 0.000000 & 0 & 1.000000 \\\\ 0 & 0 & 0.000335 & 0.999665 & 0 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self, H0, A, test = False):\n",
    "        super(net, self).__init__()\n",
    "        self.H = H0\n",
    "        self.A = A\n",
    "        self.W_gh   = nn.Linear(in_features=N_FEATURES, out_features=N_HEADS*N_HIDDEN, bias=False, dtype=H0.dtype)\n",
    "        self.Gk     = torch.zeros(size=(N_NODES, N_HEADS*N_FEATURES), dtype=H0.dtype)\n",
    "        self.GkR    = self.Gk.view(N_NODES, N_HEADS, N_FEATURES)\n",
    "        self.Ck_l   = torch.zeros(size=(N_NODES*N_NODES, N_HEADS, N_FEATURES), dtype=H0.dtype) \n",
    "        self.Ck_r   = torch.zeros_like(self.Ck_l)\n",
    "        self.Ck_f   = torch.zeros(size=(N_NODES*N_NODES, N_HEADS, 2*N_FEATURES), dtype=H0.dtype) \n",
    "        self.Ck     = self.Ck_f.view(N_NODES, N_NODES, N_HEADS, 2*N_FEATURES)\n",
    "        self.attnt  = nn.Linear(2*N_FEATURES, 1, bias=False, dtype = H0.dtype)\n",
    "        self.activ  = nn.ReLU(0.2)\n",
    "        self.E      = torch.zeros(size=(N_NODES,N_NODES, N_HEADS), dtype=H0.dtype)\n",
    "        self.alpha  = torch.zeros_like(self.E)\n",
    "        self.softmax= nn.Softmax(dim = 1)\n",
    "         \n",
    "        if test:\n",
    "            self.debug()\n",
    "\n",
    "    def debug(self):\n",
    "        with torch.no_grad():\n",
    "            print('Create matrix transformed embeddings (concatenated N_HEADS (K) blocks of shape (N_nodes,N_HIDDEN):')\n",
    "            print_tex('G_K = H W_{K=1}^T || H W_{K=2}^T = G_{K=1} || G_{K=2}')\n",
    "            # set scaling transforms\n",
    "            for i, s in enumerate(range(2,2*N_HEADS + 2, 2)):\n",
    "                self.W_gh.weight[i*N_FEATURES:(i+1)*N_FEATURES] = s*torch.eye(N_FEATURES)\n",
    "            self.Gk += self.W_gh(self.H)        # cannot redefine, it will break a view\n",
    "            print_tex('G_K = H W_K^T = ', H.numpy() , self.W_gh.weight[:].T.numpy(),' = ', self.Gk.numpy())\n",
    "            print('Reshape G_K form (N_NODES, N_HEADS*N_FEATURES) to (N_NODES, N_HEADS, N_FEATURES)')\n",
    "            print(\"Prepare C_K: stacked N_HEADS attention matrices of shape (N_NODES, N_NODES, N_HEADS, 2*N_FEATURES)\")\n",
    "            print('Each entry i,j of C_K holds N_HEADS of concatenated feature pair which via attention mechanism will determine weights if edge(i,j).')\n",
    "            print('Concatenation is not broadcasted. Create it from flattened features that have proper ordering. shape (N_NODES*N_NODES, N_HEADS, 2*N_FEATURES)')\n",
    "            self.Ck_l += self.GkR.repeat_interleave(N_NODES, dim=0)\n",
    "            self.Ck_r += self.GkR.repeat(N_NODES, 1, 1)\n",
    "            self.Ck_f += torch.cat([self.Ck_l, self.Ck_r], dim=-1);\n",
    "            print_tex(r'Repeat_{flat} \\ (K=1): '    ,self.Ck_l[:,[0]].squeeze(1).numpy(), \n",
    "                      r'Interleave_{flat} \\ (K=1): ',self.Ck_r[:,[0]].squeeze(1).numpy(),\n",
    "                       r'C_{flat} \\ (K=1): '        ,self.Ck_f[:,[0]].squeeze(1).numpy())\n",
    "            \n",
    "            self.attnt.weight[:] = torch.ones_like(self.attnt.weight[:])\n",
    "            print_tex(r'E = \\sigma(\\vec{a}[C_K])')\n",
    "            print('To make attention vector interpretable for debug, fill it with ones. Dot = element sum.')\n",
    "            print_tex('Attention \\ vector = ', self.attnt.weight[:].numpy())\n",
    "            print('Apply attention and activation to C_K ')\n",
    "            print('(N_NODES, N_NODES, N_HEADS, 2*N_FEATURES) . (2*N_FEATURES, 1) -> (N_NODES, N_NODES, N_HEADS, 2*N_FEATURES, 1) -> squeeze last')\n",
    "            self.E +=  self.activ(self.attnt(self.Ck)).squeeze(-1)#\n",
    "            self.alpha += self.E.masked_fill(self.A.view(N_NODES,N_NODES,1) == 0, float('-inf'))\n",
    "            for i in range(N_HEADS):\n",
    "                print_tex('E_{K='+str(i + 1)+'} = ',self.E.numpy()[:,:,i], r'\\rightarrow MASK \\rightarrow ',(self.E[:,:,i]*self.A).numpy() )\n",
    "\n",
    "\n",
    "            print('Apply row-wise softmax:')\n",
    "            self.alpha = self.softmax(self.alpha)\n",
    "            \n",
    "            for i in range(N_HEADS):\n",
    "                print_tex(r'\\alpha_{K='+str(i + 1)+'} = ', self.alpha.numpy()[:,:,i])\n",
    "\n",
    "model = net(H,torch.tensor(A),True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some things ive learned\n",
    "* you can define an array, and its reshaped representation. if you dont redefine array, you can change it, and reshaped representation will also change (duh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3, 4])\n",
      "tensor([[2],\n",
      "        [3],\n",
      "        [4]])\n",
      "tensor([5, 7, 9])\n",
      "tensor([[5],\n",
      "        [7],\n",
      "        [9]])\n"
     ]
    }
   ],
   "source": [
    "asd = torch.arange(2,5,1);print(asd)\n",
    "asd2 = asd.reshape(-1,1);print(asd2)\n",
    "asd += torch.arange(3,6,1);print(asd)\n",
    "print(asd2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* masking does not create a view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3, 4])\n",
      "tensor([ True, False, False])\n",
      "tensor([0, 3, 4])\n",
      "tensor([2, 3, 8])\n",
      "tensor([0, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "asd = torch.arange(2,5,1);print(asd)\n",
    "mask = torch.tensor([1,0,0], dtype=bool);print(mask)\n",
    "asd2 = asd.masked_fill(mask=mask,value= 0);print(asd2)\n",
    "asd[2] = 8; print(asd)\n",
    "print(asd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) torch.Size([3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  5],\n",
       "        [ 9, 11]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd = torch.tensor([[1,2,3],[4,5,6]])\n",
    "asd2 = torch.stack([torch.tensor([[1,2,3],[4,5,6]]),\n",
    "                    torch.tensor([[7,8,9],[10,11,12]])])\n",
    "\n",
    "a = torch.tensor([[0,0,1],[0,1,0]]).T\n",
    "print(asd2.shape, a.shape)\n",
    "torch.diagonal(asd2 @ a, dim1=-2, dim2=-1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
