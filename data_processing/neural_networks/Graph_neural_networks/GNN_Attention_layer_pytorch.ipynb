{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch#, time\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cpu\")# torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from misc_tools.print_latex import print_tex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For derivation see GNN_Attention_notes.ipynb\n",
    "Im not sure whether implement unique $\\vec{a}$ for all heads. Original paper hints on that. Implementation i have found reuses it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle A = \\begin{bmatrix} 0 & 1 & 1 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 0 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "#G = nx.house_graph()\n",
    "N = 3\n",
    "G = nx.gnm_random_graph(N, 2*N)\n",
    "\n",
    "A = nx.adjacency_matrix(G).todense()\n",
    "print_tex('A = ', A)\n",
    "A = torch.tensor(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N_FEATURES, N_NODES = A.shape\n",
    "N_NODES = len(G.nodes())\n",
    "N_FEATURES = 2\n",
    "N_HIDDEN = 2\n",
    "N_HEADS = 2\n",
    "OUT_FEATURES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle H = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "H = torch.arange(N_NODES*N_FEATURES, dtype = float).view(N_NODES, N_FEATURES) + 1\n",
    "print_tex('H = ', H.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hot Mexican\\AppData\\Local\\Temp\\ipykernel_13704\\3991785311.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  model = debug_net(H,torch.tensor(A),True)\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle N_{nodes} = 3; \\ N_{heads} = 2; \\ N_{features} = 2; \\ N_{hidden} = 2$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle G_{K} \\text{ is a matrix of concatenated embeddings } \\vec{g}_i^{k} , \\ shape : [N_{nodes} \\times N_{heads}*N_{hidden}]$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle G_K = H W_K^T = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix}\\begin{bmatrix} 1 & 0 & 4 & 0 \\\\ 0 & 1 & 0 & 4 \\end{bmatrix} = \\begin{bmatrix} 1 & 2 & 4 & 8 \\\\ 3 & 4 & 12 & 16 \\\\ 5 & 6 & 20 & 24 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{Reshape } G_{K} \\ to \\ [N_{nodes} \\times N_{heads} \\times N_{hidden}] \\text{ to isolate each head`s data to its own dimension}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{Goal: a matrix } C_K \\text{ that holds concatenated node feature pairs. Shape: }[N_{nodes} \\times N_{nodes}\\times N_{heads} \\times 2 N_{hidden}]$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its only (?) possible with flattening, concatenating and unflattening. See notes.\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle C_{flat} \\ (K=1) = Repeat_{flat} \\ ||  \\ Interleave_{flat} = \\begin{bmatrix} 1 & 2 \\\\ 1 & 2 \\\\ 1 & 2 \\\\ 3 & 4 \\\\ 3 & 4 \\\\ 3 & 4 \\\\ 5 & 6 \\\\ 5 & 6 \\\\ 5 & 6 \\end{bmatrix} \\ \\bigg|\\bigg| \\ \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\\\ 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\\\ 1 & 2 \\\\ 3 & 4 \\\\ 5 & 6 \\end{bmatrix} = \\begin{bmatrix} 1 & 2 & 1 & 2 \\\\ 1 & 2 & 3 & 4 \\\\ 1 & 2 & 5 & 6 \\\\ 3 & 4 & 1 & 2 \\\\ 3 & 4 & 3 & 4 \\\\ 3 & 4 & 5 & 6 \\\\ 5 & 6 & 1 & 2 \\\\ 5 & 6 & 3 & 4 \\\\ 5 & 6 & 5 & 6 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{Features C}_{0,0} = \\begin{bmatrix} 1 & 2 & 1 & 2 \\\\ 4 & 8 & 4 & 8 \\end{bmatrix}; \\ shape: \\ [N_{heads} \\times 2 N_{hidden}]$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{Goal: a matrix E that holds edge weights. Shape: }[N_{nodes} \\times N_{nodes} \\times N_{heads}]$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle E = \\sigma(\\vec{a}[C_K])$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test attention vectors:\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\vec{a}_0 = \\begin{bmatrix} 1 & 1 & 1 & 1 \\end{bmatrix}^T ; \\ \\vec{a}_1 = \\begin{bmatrix} 1/2 & 1/2 & 1/2 & 1/2 \\end{bmatrix}^T ; \\ $"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>See how to apply multiple attention vectors to data in notes<<<\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{Features E}_{0,0} = \\begin{bmatrix} 6 & 12 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle E_{K=1} = \\begin{bmatrix} 6 & 10 & 14 \\\\ 10 & 14 & 18 \\\\ 14 & 18 & 22 \\end{bmatrix}\\rightarrow MASK \\rightarrow \\begin{bmatrix} 0 & 10 & 14 \\\\ 10 & 0 & 18 \\\\ 14 & 18 & 0 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle E_{K=2} = \\begin{bmatrix} 12 & 20 & 28 \\\\ 20 & 28 & 36 \\\\ 28 & 36 & 44 \\end{bmatrix}\\rightarrow MASK \\rightarrow \\begin{bmatrix} 0 & 20 & 28 \\\\ 20 & 0 & 36 \\\\ 28 & 36 & 0 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{Goal: a matrix } \\Alpha \\ or \\ \\alpha \\text{ with row-wise softmax normalized weights. Shape: }[N_{nodes} \\times N_{nodes} \\times N_{heads}]$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\alpha_{K=1} = \\begin{bmatrix} 0 & 0.017986 & 0.982014 \\\\ 0.000335 & 0 & 0.999665 \\\\ 0.017986 & 0.982014 & 0 \\end{bmatrix}{\\rightarrow set \\ to \\ A \\ for \\ example \\rightarrow }:\\begin{bmatrix} 0 & 1 & 1 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 0 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\alpha_{K=2} = \\begin{bmatrix} 0 & 0.000335 & 0.999665 \\\\ 0.000000 & 0 & 1.000000 \\\\ 0.000335 & 0.999665 & 0 \\end{bmatrix}{\\rightarrow set \\ to \\ A \\ for \\ example \\rightarrow }:\\begin{bmatrix} 0 & 1 & 1 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 0 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{Goal: updated features } G_k^\\prime \\text{ based on aggregation of features } \\vec{g}_i^k \\text{ with weights } \\Alpha \\text{ . Shape: }[N_{nodes} \\times N_{heads} \\times N_{hidden}]$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>See how to aggregate multi head case in notes<<<\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle G_1 = \\begin{bmatrix} 3 & 4 \\\\ 12 & 16 \\end{bmatrix}; \\ G_2 = \\begin{bmatrix} 5 & 6 \\\\ 20 & 24 \\end{bmatrix}; \\ \\Alpha|_{row,1}= \\begin{bmatrix} 0 & 0 \\\\ 1 & 1 \\\\ 1 & 1 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle G_0^\\prime = \\vec{A}_0 \\otimes G = \\begin{bmatrix} 8 & 10 \\\\ 32 & 40 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New embeddings can be either concatenated across different variants of k or averaged\"\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle G_0^{concat} = \\begin{bmatrix} 8 & 10 & 32 & 40 \\end{bmatrix}; \\ G_0^{Avg} = \\begin{bmatrix} 20 & 25 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class debug_net(nn.Module):\n",
    "    def __init__(self, H0, A, test = False):\n",
    "        super(debug_net, self).__init__()\n",
    "        self.H = H0\n",
    "        self.A = A\n",
    "        self.W_gh   = nn.Linear(in_features=N_FEATURES, out_features=N_HEADS*N_HIDDEN, bias=False, dtype=H0.dtype)\n",
    "        self.Gk     = torch.zeros(size=(N_NODES, N_HEADS*N_HIDDEN), dtype=H0.dtype)\n",
    "        self.GkR    = self.Gk.view(N_NODES, N_HEADS, N_FEATURES)\n",
    "        self.Ck_l   = torch.zeros(size=(N_NODES*N_NODES, N_HEADS, N_HIDDEN), dtype=H0.dtype) \n",
    "        self.Ck_r   = torch.zeros_like(self.Ck_l)\n",
    "        self.Ck_f   = torch.zeros(size=(N_NODES*N_NODES, N_HEADS, 2*N_HIDDEN), dtype=H0.dtype) \n",
    "        self.Ck     = self.Ck_f.view(N_NODES, N_NODES, N_HEADS, 2*N_HIDDEN)\n",
    "        self.attnt  = nn.Parameter(torch.zeros(size=(2*N_HIDDEN, N_HEADS), dtype=H0.dtype))\n",
    "        self.activ  = nn.LeakyReLU(0.2)\n",
    "        self.E      = torch.zeros(size=(N_NODES,N_NODES, N_HEADS), dtype=H0.dtype)\n",
    "        self.alpha  = torch.zeros_like(self.E)\n",
    "        self.softmax= nn.Softmax(dim = 1)\n",
    "        self.GkPrime= torch.zeros_like(self.GkR)\n",
    "         \n",
    "        if test:\n",
    "            self.debug()\n",
    "\n",
    "    def debug(self):\n",
    "        with torch.no_grad():\n",
    "            print_tex(r'N_{nodes} = '+ str(N_NODES) + r'; \\ N_{heads} = '+ str(N_HEADS) + r'; \\ N_{features} = '+ str(N_FEATURES)+ r'; \\ N_{hidden} = '+ str(N_HIDDEN))\n",
    "            print_tex(r'G_{K} \\text{ is a matrix of concatenated embeddings } \\vec{g}_i^{k} , \\ shape : [N_{nodes} \\times N_{heads}*N_{hidden}]')\n",
    "\n",
    "            # set scaling transforms\n",
    "            for i in range(N_HEADS):\n",
    "                s = 1 if i == 0 else 4*i\n",
    "\n",
    "                self.W_gh.weight[i*N_FEATURES:(i+1)*N_FEATURES] = s*torch.eye(N_FEATURES)\n",
    "\n",
    "            self.Gk += self.W_gh(self.H)        # cannot redefine, it will break a view\n",
    "\n",
    "            print_tex('G_K = H W_K^T = ', H.numpy() , self.W_gh.weight[:].T.numpy(),' = ', self.Gk.numpy())\n",
    "            print_tex(r'\\text{Reshape } G_{K} \\ to \\ [N_{nodes} \\times N_{heads} \\times N_{hidden}] \\text{ to isolate each head`s data to its own dimension}')\n",
    "\n",
    "            print_tex(r\"\\text{Goal: a matrix } C_K \\text{ that holds concatenated node feature pairs. Shape: }[N_{nodes} \\times N_{nodes}\\times N_{heads} \\times 2 N_{hidden}]\")\n",
    "            print(\"Its only (?) possible with flattening, concatenating and unflattening. See notes.\")\n",
    "\n",
    "            self.Ck_l += self.GkR.repeat_interleave(N_NODES, dim=0)\n",
    "            self.Ck_r += self.GkR.repeat(N_NODES, 1, 1)\n",
    "            self.Ck_f += torch.cat([self.Ck_l, self.Ck_r], dim=-1);\n",
    "            \n",
    "            print_tex(r'C_{flat} \\ (K=1) = Repeat_{flat} \\ ||  \\ Interleave_{flat} = '\n",
    "                      ,self.Ck_l[:,[0]].squeeze(1).numpy(),r' \\ \\bigg|\\bigg| \\ ', self.Ck_r[:,[0]].squeeze(1).numpy(), ' = ',\n",
    "                      self.Ck_f[:,[0]].squeeze(1).numpy())\n",
    "            \n",
    "            print_tex(r'\\text{Features C}_{0,0} = ', self.Ck[0,0].numpy(), r'; \\ shape: \\ [N_{heads} \\times 2 N_{hidden}]')\n",
    "            prnt_vec = [r'\\vec{a}_'+str(i)+ ' = ' for i in range(N_HEADS)]\n",
    "            prnt_vec2 = [r'^T ; \\ ' for i in range(N_HEADS)]\n",
    "\n",
    "            self.attnt += np.repeat(1/(torch.arange(N_HEADS, dtype=self.H.dtype).unsqueeze(0)+1), repeats=2*N_HIDDEN, axis = 0)\n",
    "            print_tex(r\"\\text{Goal: a matrix E that holds edge weights. Shape: }[N_{nodes} \\times N_{nodes} \\times N_{heads}]\")\n",
    "            print_tex(r'E = \\sigma(\\vec{a}[C_K])')\n",
    "            print('Test attention vectors:')\n",
    "            print_tex(*[l for lists in zip(prnt_vec,self.attnt.T.numpy(),prnt_vec2) for l in lists])\n",
    "            print('>>>See how to apply multiple attention vectors to data in notes<<<')\n",
    "            \n",
    "            self.E += self.activ(torch.einsum('ijkf,fk -> ijk', self.Ck, self.attnt)).squeeze(-1)#\n",
    "\n",
    "            print_tex(r'\\text{Features E}_{0,0} = ', self.E[0,0].numpy())\n",
    "            self.alpha += self.E.masked_fill(self.A.view(N_NODES,N_NODES,1) == 0, float('-inf'))\n",
    "            for i in range(N_HEADS):\n",
    "                print_tex('E_{K='+str(i + 1)+'} = ',self.E.numpy()[:,:,i], r'\\rightarrow MASK \\rightarrow ',(self.E[:,:,i]*self.A).numpy() )\n",
    "            print_tex(r\"\\text{Goal: a matrix } \\Alpha \\ or \\ \\alpha \\text{ with row-wise softmax normalized weights. Shape: }[N_{nodes} \\times N_{nodes} \\times N_{heads}]\")\n",
    "            self.alpha = self.softmax(self.alpha)\n",
    "            \n",
    "            for i in range(N_HEADS):\n",
    "                print_tex(r'\\alpha_{K='+str(i + 1)+'} = ', self.alpha.numpy()[:,:,i], r'{\\rightarrow set \\ to \\ A \\ for \\ example \\rightarrow }:', self.A.numpy())\n",
    "                self.alpha.numpy()[:,:,i] = self.A\n",
    "            self.GkPrime += torch.einsum('ijk,jkf->ikf', self.alpha , self.GkR)\n",
    "\n",
    "            print_tex(r\"\\text{Goal: updated features } G_k^\\prime \\text{ based on aggregation of features } \\vec{g}_i^k \\text{ with weights } \\Alpha \\text{ . Shape: }[N_{nodes} \\times N_{heads} \\times N_{hidden}]\")\n",
    "            print('>>>See how to aggregate multi head case in notes<<<')\n",
    "            n1_neighbors_id = torch.argwhere(self.A[0] == 1).flatten().numpy()\n",
    "            \n",
    "            a = [r'G_'+str(i)+ ' = ' for i in n1_neighbors_id]\n",
    "            b = [self.GkR[i].numpy() for i in n1_neighbors_id]\n",
    "            c = [r'; \\ ' for i in n1_neighbors_id]\n",
    "            print_tex(*[l for lists in zip(a,b,c) for l in lists], r'\\Alpha|_{row,1}= ', self.alpha[0].numpy() )\n",
    "            print_tex(r'G_0^\\prime = \\vec{A}_0 \\otimes G = ', self.GkPrime[0].numpy())\n",
    "            print('New embeddings can be either concatenated across different variants of k or averaged\"')\n",
    "            GkP_concat = self.GkPrime.reshape(N_NODES, N_HEADS * N_HIDDEN)\n",
    "            GkP_avg  = self.GkPrime.mean(dim=1)\n",
    "            print_tex(r'G_0^{concat} = ', GkP_concat[0].numpy(), r'; \\ G_0^{Avg} = ',GkP_avg[0].numpy())\n",
    "\n",
    "\n",
    "model = debug_net(H,torch.tensor(A),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ck.shape = torch.Size([3, 3, 2, 4])\n",
      "alpha.shape = torch.Size([3, 3, 2])\n",
      "Gk_new.shape = torch.Size([3, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\begin{bmatrix} 0.906279 & 0.093721 & 1/2 & 1/2 \\\\ 1/2 & 1/2 & 1/2 & 1/2 \\\\ 0.890478 & 0.109522 & 0.031919 & 0.968081 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, A, N_FEATURES, N_HEADS, N_HIDDEN, non_lin, concat: bool = False, l_ReLU_slope: float = 0.2, dropout: float = 0.6, dtype = torch.float):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        # 1) globals\n",
    "        self.adj = A\n",
    "        self.N_FEATURES, self.N_HEADS,  self.N_HIDDEN  = N_FEATURES, N_HEADS, N_HIDDEN\n",
    "        self.concat = concat\n",
    "        self.dtype = dtype\n",
    "        # 2) weights \n",
    "        # 2.1) project H0 to G      [N_nodes x N_features] -> [N_nodes x N_hidden]\n",
    "        self.W_gh   = nn.Linear(in_features=N_FEATURES, out_features=N_HEADS*N_HIDDEN, bias=False, dtype=dtype)\n",
    "        # 2.2) project Ck to alpha  [N_nodes x N_nodes x N_heads x N_features] -> [N_nodes x N_hidden x N_heads]\n",
    "        self.attention  = nn.Parameter(torch.empty(size=(2*N_HIDDEN, N_HEADS), dtype=dtype))\n",
    "        nn.init.xavier_uniform_(self.attention) \n",
    "\n",
    "        # misc processing\n",
    "        self.activ_alpha    = nn.LeakyReLU(l_ReLU_slope)\n",
    "        self.softmax        = nn.Softmax(dim = 1)\n",
    "        self.activ_Gn_new   = non_lin\n",
    "        self.dropout        = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, H):\n",
    "        H = H.to(self.dtype)\n",
    "        N_NODES = H.size(0)\n",
    "        N_FEATURES, N_HEADS, N_HIDDEN = self.N_FEATURES, self.N_HEADS,  self.N_HIDDEN\n",
    "        # 1) Get features\n",
    "        Gk_reshaped = self.W_gh(H).view(N_NODES, N_HEADS, N_HIDDEN)\n",
    "        # 2) Prepare attention/adjacency matrix\n",
    "        # 2.1) Concatenated pairwise features for each head\n",
    "        C_left  = Gk_reshaped.repeat_interleave(N_NODES, dim=0)\n",
    "        C_right = Gk_reshaped.repeat(N_NODES, 1, 1)\n",
    "        Ck      = torch.cat([C_left, C_right], dim=-1).view(N_NODES, N_NODES, N_HEADS, 2*N_HIDDEN)\n",
    "        print(f'{Ck.shape = }')\n",
    "        # 2.2) Project concat features to a scalar\n",
    "        alpha   = self.activ_alpha(torch.einsum('ijkf,fk -> ijk', Ck, self.attention)).view(N_NODES, N_NODES, N_HEADS)\n",
    "        print(f'{alpha.shape = }')\n",
    "        # 2.3) Apply adjacency matrix mask, softmax and dropout\n",
    "        alpha.masked_fill_(self.adj.reshape(N_NODES,N_NODES,1) == 0, float('-inf'))\n",
    "        alpha = self.softmax(alpha)\n",
    "        alpha = self.dropout(alpha)\n",
    "        # 3) Calculate new feature vectors by aggregation, using attention matrix.\n",
    "        Gk_new = torch.einsum('ijk,jkf->ikf', alpha , Gk_reshaped )\n",
    "        print(f'{Gk_new.shape = }')\n",
    "        # 4) Additional processing. Like in paper, its either mean or concat, with activation.\n",
    "        if self.concat:\n",
    "            # activation before concat\n",
    "            Gk_new = self.activ_Gn_new(Gk_new)\n",
    "            return Gk_new.reshape(N_NODES, N_HEADS * N_HIDDEN)\n",
    "        else:\n",
    "            # activation after\n",
    "            Gk_new = Gk_new.mean(dim=1).view(N_NODES, 1, N_HIDDEN)\n",
    "            print(f'mean {Gk_new.shape = }')\n",
    "            return self.activ_Gn_new(Gk_new)\n",
    "        \n",
    "model = GraphAttentionLayer(A, N_FEATURES, N_HEADS, N_HIDDEN, nn.Softmax(dim = 2), concat=True)\n",
    "print_tex(model(H).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recreate case from a paper:\n",
    "\n",
    "Transductive learning.\n",
    "\n",
    "1. GAT layers = 2<br>\n",
    "    1. Layer 1<br>\n",
    "            1. N_HEADS = 8 <br>\n",
    "            2. N_HIDDEN = 8 <br>\n",
    "            3. activation = ELU <br>\n",
    "            4. concatenation = True <br>\n",
    "            5. N_FEATURES = original number\n",
    "    2. Layer 2<br>\n",
    "        1. N_HEADS = 1 <br>\n",
    "        2. N_HIDDEN = number of classes <br>\n",
    "        3. activation = SoftMax <br>\n",
    "        4. concatenation = false <br>\n",
    "        5. N_FEATURES = Layer 1 N_HEADS * layer 1 N_HIDDEN\n",
    "2. regularization = $L_2$, scale = 0.0005\n",
    "3. dropout, p = 0.6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ck.shape = torch.Size([3, 3, 8, 12])\n",
      "alpha.shape = torch.Size([3, 3, 8])\n",
      "Gk_new.shape = torch.Size([3, 8, 6])\n",
      "Ck.shape = torch.Size([3, 3, 1, 4])\n",
      "alpha.shape = torch.Size([3, 3, 1])\n",
      "Gk_new.shape = torch.Size([3, 1, 2])\n",
      "mean Gk_new.shape = torch.Size([3, 1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8188, 0.1812]],\n",
       "\n",
       "        [[0.5000, 0.5000]],\n",
       "\n",
       "        [[0.8963, 0.1037]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class net(nn.Module):\n",
    "    def __init__(self, H0, A, num_classes):\n",
    "        super(net, self).__init__()\n",
    "        N_NODES     , N_FEATURES_1  = H0.shape\n",
    "        N_HEADS_1   , N_HEADS_2     = 8, 1\n",
    "        N_HIDDEN_1  , N_HIDDEN_2    = 6, num_classes\n",
    "        N_FEATURES_2                = N_HEADS_1 * N_HIDDEN_1\n",
    "        self.GAT1 = GraphAttentionLayer(A, N_FEATURES_1, N_HEADS_1, N_HIDDEN_1, nn.ELU(), concat = True)\n",
    "        self.GAT2 = GraphAttentionLayer(A, N_FEATURES_2, N_HEADS_2, N_HIDDEN_2, nn.Softmax(dim = 2))\n",
    "\n",
    "    def forward(self, H):\n",
    "        x = self.GAT1(H)\n",
    "        y = self.GAT2(x)\n",
    "        return y\n",
    "    \n",
    "model2 = net(H, A, 2)\n",
    "model2(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some things ive learned\n",
    "* you can define an array, and its reshaped representation. If you dont redefine array, you can change it, and reshaped representation will also change (duh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3, 4])\n",
      "tensor([[2],\n",
      "        [3],\n",
      "        [4]])\n",
      "tensor([5, 7, 9])\n",
      "tensor([[5],\n",
      "        [7],\n",
      "        [9]])\n"
     ]
    }
   ],
   "source": [
    "asd = torch.arange(2,5,1);print(asd)\n",
    "asd2 = asd.reshape(-1,1);print(asd2)\n",
    "asd += torch.arange(3,6,1);print(asd)\n",
    "print(asd2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
