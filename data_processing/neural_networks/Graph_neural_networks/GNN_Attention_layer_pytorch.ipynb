{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch#, time\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cpu\")# torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from misc_tools.print_latex import print_tex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For derivation see GNN_Attention_notes.ipynb\n",
    "Im not sure whether implement unique $\\vec{a}$ for all heads. Original paper hints on that. Implementation i have found reuses it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle A = \\begin{bmatrix} 0 & 1 & 1 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 0 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "#G = nx.house_graph()\n",
    "N = 3\n",
    "G = nx.gnm_random_graph(N, 2*N)\n",
    "\n",
    "A = nx.adjacency_matrix(G).todense()\n",
    "print_tex('A = ', A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N_FEATURES, N_NODES = A.shape\n",
    "N_NODES = len(G.nodes())\n",
    "N_FEATURES = 2\n",
    "N_HIDDEN = 2\n",
    "N_HEADS = 2\n",
    "OUT_FEATURES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle H = \\begin{bmatrix} 0 & 1/2 \\\\ 1 & 3/2 \\\\ 2 & 5/2 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "H = 0.5*torch.arange(N_NODES*N_FEATURES, dtype = float).view(N_NODES, N_FEATURES)\n",
    "print_tex('H = ', H.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create matrix transformed embeddings (concatenated N_HEADS (K) blocks of shape (N_nodes,N_HIDDEN):\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle G_K = H W_{K=1}^T || H W_{K=2}^T = G_{K=1} || G_{K=2}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle G_K = H W_K^T = \\begin{bmatrix} 0 & 1/2 \\\\ 1 & 3/2 \\\\ 2 & 5/2 \\end{bmatrix}\\begin{bmatrix} 2 & 0 & 4 & 0 \\\\ 0 & 2 & 0 & 4 \\end{bmatrix} = \\begin{bmatrix} 0 & 1 & 0 & 2 \\\\ 2 & 3 & 4 & 6 \\\\ 4 & 5 & 8 & 10 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshape G_K form (N_NODES, N_HEADS*N_HIDDEN) to (N_NODES, N_HEADS, N_HIDDEN)\n",
      "Prepare C_K: stacked N_HEADS attention matrices of shape (N_NODES, N_NODES, N_HEADS, 2*N_HIDDEN)\n",
      "Each entry i,j of C_K holds N_HEADS of concatenated feature pair which via attention mechanism will determine weights if edge(i,j).\n",
      "Concatenation is not broadcasted. Create it from flattened features that have proper ordering. shape (N_NODES*N_NODES, N_HEADS, 2*N_HIDDEN)\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle Repeat_{flat} \\ (K=1): \\begin{bmatrix} 0 & 1 \\\\ 0 & 1 \\\\ 0 & 1 \\\\ 2 & 3 \\\\ 2 & 3 \\\\ 2 & 3 \\\\ 4 & 5 \\\\ 4 & 5 \\\\ 4 & 5 \\end{bmatrix}Interleave_{flat} \\ (K=1): \\begin{bmatrix} 0 & 1 \\\\ 2 & 3 \\\\ 4 & 5 \\\\ 0 & 1 \\\\ 2 & 3 \\\\ 4 & 5 \\\\ 0 & 1 \\\\ 2 & 3 \\\\ 4 & 5 \\end{bmatrix}C_{flat} \\ (K=1): \\begin{bmatrix} 0 & 1 & 0 & 1 \\\\ 0 & 1 & 2 & 3 \\\\ 0 & 1 & 4 & 5 \\\\ 2 & 3 & 0 & 1 \\\\ 2 & 3 & 2 & 3 \\\\ 2 & 3 & 4 & 5 \\\\ 4 & 5 & 0 & 1 \\\\ 4 & 5 & 2 & 3 \\\\ 4 & 5 & 4 & 5 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{Features C}_{0,0} = \\begin{bmatrix} 0 & 1 & 0 & 1 \\\\ 0 & 2 & 0 & 2 \\end{bmatrix}; \\ shape: \\ [N_{heads} \\times 2 N_{hidden}]$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle E = \\sigma(\\vec{a}[C_K])$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test attention vectors:\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\vec{a}_0 = \\begin{bmatrix} 1 & 1 & 1 & 1 \\end{bmatrix}^T ; \\ \\vec{a}_1 = \\begin{bmatrix} 2 & 2 & 2 & 2 \\end{bmatrix}^T ; \\ $"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>See how to apply multiple attention vectors to data in notes<<<\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{Features E}_{0,0} = \\begin{bmatrix} 2 & 8 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle E_{K=1} = \\begin{bmatrix} 2 & 6 & 10 \\\\ 6 & 10 & 14 \\\\ 10 & 14 & 18 \\end{bmatrix}\\rightarrow MASK \\rightarrow \\begin{bmatrix} 0 & 6 & 10 \\\\ 6 & 0 & 14 \\\\ 10 & 14 & 0 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle E_{K=2} = \\begin{bmatrix} 8 & 24 & 40 \\\\ 24 & 40 & 56 \\\\ 40 & 56 & 72 \\end{bmatrix}\\rightarrow MASK \\rightarrow \\begin{bmatrix} 0 & 24 & 40 \\\\ 24 & 0 & 56 \\\\ 40 & 56 & 0 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apply row-wise softmax:\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\alpha_{K=1} = \\begin{bmatrix} 0 & 0.017986 & 0.982014 \\\\ 0.000335 & 0 & 0.999665 \\\\ 0.017986 & 0.982014 & 0 \\end{bmatrix}{\\rightarrow set \\ to \\ A \\ for \\ example \\rightarrow }:\\begin{bmatrix} 0 & 1 & 1 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 0 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\alpha_{K=2} = \\begin{bmatrix} 0 & 0.000000 & 1.000000 \\\\ 0 & 0 & 1 \\\\ 0.000000 & 1.000000 & 0 \\end{bmatrix}{\\rightarrow set \\ to \\ A \\ for \\ example \\rightarrow }:\\begin{bmatrix} 0 & 1 & 1 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 0 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>See how to aggregate multi head case in notes<<<\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\vec{g}_1^T = \\begin{bmatrix} 2 & 3 \\\\ 4 & 6 \\end{bmatrix}; \\ \\vec{g}_2^T = \\begin{bmatrix} 4 & 5 \\\\ 8 & 10 \\end{bmatrix}; \\ \\vec{A}_0 = \\begin{bmatrix} 0 & 1 & 1 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\vec{g}_0^\\prime = \\vec{A}_0 \\otimes G = \\begin{bmatrix} 6 & 8 \\\\ 12 & 16 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\begin{bmatrix} 6 & 8 & 12 & 16 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self, H0, A, test = False):\n",
    "        super(net, self).__init__()\n",
    "        self.H = H0\n",
    "        self.A = A\n",
    "        self.W_gh   = nn.Linear(in_features=N_FEATURES, out_features=N_HEADS*N_HIDDEN, bias=False, dtype=H0.dtype)\n",
    "        self.Gk     = torch.zeros(size=(N_NODES, N_HEADS*N_HIDDEN), dtype=H0.dtype)\n",
    "        self.GkR    = self.Gk.view(N_NODES, N_HEADS, N_FEATURES)\n",
    "        self.Ck_l   = torch.zeros(size=(N_NODES*N_NODES, N_HEADS, N_HIDDEN), dtype=H0.dtype) \n",
    "        self.Ck_r   = torch.zeros_like(self.Ck_l)\n",
    "        self.Ck_f   = torch.zeros(size=(N_NODES*N_NODES, N_HEADS, 2*N_HIDDEN), dtype=H0.dtype) \n",
    "        self.Ck     = self.Ck_f.view(N_NODES, N_NODES, N_HEADS, 2*N_HIDDEN)\n",
    "        #self.attnt  = nn.Linear(2*N_FEATURES, 1, bias=False, dtype = H0.dtype)\n",
    "        self.attnt  = nn.Parameter(torch.zeros(size=(2*N_HIDDEN, N_HEADS), dtype=H0.dtype))\n",
    "        self.activ  = nn.ReLU(0.2)\n",
    "        self.E      = torch.zeros(size=(N_NODES,N_NODES, N_HEADS), dtype=H0.dtype)\n",
    "        self.alpha  = torch.zeros_like(self.E)\n",
    "        self.softmax= nn.Softmax(dim = 1)\n",
    "        self.GkPrime= torch.zeros_like(self.GkR)\n",
    "         \n",
    "        if test:\n",
    "            self.debug()\n",
    "\n",
    "    def debug(self):\n",
    "        with torch.no_grad():\n",
    "            print('Create matrix transformed embeddings (concatenated N_HEADS (K) blocks of shape (N_nodes,N_HIDDEN):')\n",
    "            print_tex('G_K = H W_{K=1}^T || H W_{K=2}^T = G_{K=1} || G_{K=2}')\n",
    "\n",
    "            # set scaling transforms\n",
    "            for i, s in enumerate(range(2,2*N_HEADS + 2, 2)):\n",
    "                self.W_gh.weight[i*N_FEATURES:(i+1)*N_FEATURES] = s*torch.eye(N_FEATURES)\n",
    "\n",
    "            self.Gk += self.W_gh(self.H)        # cannot redefine, it will break a view\n",
    "\n",
    "            print_tex('G_K = H W_K^T = ', H.numpy() , self.W_gh.weight[:].T.numpy(),' = ', self.Gk.numpy())\n",
    "            print('Reshape G_K form (N_NODES, N_HEADS*N_HIDDEN) to (N_NODES, N_HEADS, N_HIDDEN)')\n",
    "            print(\"Prepare C_K: stacked N_HEADS attention matrices of shape (N_NODES, N_NODES, N_HEADS, 2*N_HIDDEN)\")\n",
    "            print('Each entry i,j of C_K holds N_HEADS of concatenated feature pair which via attention mechanism will determine weights if edge(i,j).')\n",
    "            print('Concatenation is not broadcasted. Create it from flattened features that have proper ordering. shape (N_NODES*N_NODES, N_HEADS, 2*N_HIDDEN)')\n",
    "\n",
    "            self.Ck_l += self.GkR.repeat_interleave(N_NODES, dim=0)\n",
    "            self.Ck_r += self.GkR.repeat(N_NODES, 1, 1)\n",
    "            self.Ck_f += torch.cat([self.Ck_l, self.Ck_r], dim=-1);\n",
    "            \n",
    "            print_tex(r'Repeat_{flat} \\ (K=1): '    ,self.Ck_l[:,[0]].squeeze(1).numpy(), \n",
    "                      r'Interleave_{flat} \\ (K=1): ',self.Ck_r[:,[0]].squeeze(1).numpy(),\n",
    "                       r'C_{flat} \\ (K=1): '        ,self.Ck_f[:,[0]].squeeze(1).numpy())\n",
    "            \n",
    "            print_tex(r'\\text{Features C}_{0,0} = ', self.Ck[0,0].numpy(), r'; \\ shape: \\ [N_{heads} \\times 2 N_{hidden}]')\n",
    "            prnt_vec = [r'\\vec{a}_'+str(i)+ ' = ' for i in range(N_HEADS)]\n",
    "            prnt_vec2 = [r'^T ; \\ ' for i in range(N_HEADS)]\n",
    "\n",
    "            self.attnt += np.repeat(torch.arange(N_HEADS, dtype=self.H.dtype).unsqueeze(0)+1, repeats=2*N_HIDDEN, axis = 0)\n",
    "\n",
    "            print_tex(r'E = \\sigma(\\vec{a}[C_K])')\n",
    "            print('Test attention vectors:')\n",
    "            print_tex(*[l for lists in zip(prnt_vec,self.attnt.T.numpy(),prnt_vec2) for l in lists])\n",
    "            print('>>>See how to apply multiple attention vectors to data in notes<<<')\n",
    "            \n",
    "            self.E +=  self.activ(torch.einsum('ijkf,fk -> ijk', self.Ck, self.attnt)).squeeze(-1)#\n",
    "\n",
    "            print_tex(r'\\text{Features E}_{0,0} = ', self.E[0,0].numpy())\n",
    "            self.alpha += self.E.masked_fill(self.A.view(N_NODES,N_NODES,1) == 0, float('-inf'))\n",
    "            for i in range(N_HEADS):\n",
    "                print_tex('E_{K='+str(i + 1)+'} = ',self.E.numpy()[:,:,i], r'\\rightarrow MASK \\rightarrow ',(self.E[:,:,i]*self.A).numpy() )\n",
    "\n",
    "            print('Apply row-wise softmax:')\n",
    "            self.alpha = self.softmax(self.alpha)\n",
    "            \n",
    "            for i in range(N_HEADS):\n",
    "                print_tex(r'\\alpha_{K='+str(i + 1)+'} = ', self.alpha.numpy()[:,:,i], r'{\\rightarrow set \\ to \\ A \\ for \\ example \\rightarrow }:', self.A.numpy())\n",
    "                self.alpha.numpy()[:,:,i] = self.A\n",
    "            self.GkPrime += torch.einsum('ijk,jkf->ikf', self.alpha , self.GkR)\n",
    "\n",
    "            print('>>>See how to aggregate multi head case in notes<<<')\n",
    "            n1_neighbors_id = torch.argwhere(self.A[0] == 1).flatten().numpy()\n",
    "            \n",
    "            a = [r'\\vec{g}_'+str(i)+ '^T = ' for i in n1_neighbors_id]\n",
    "            b = [self.GkR[i].numpy() for i in n1_neighbors_id]\n",
    "            c = [r'; \\ ' for i in n1_neighbors_id]\n",
    "            print_tex(*[l for lists in zip(a,b,c) for l in lists], r'\\vec{A}_0 = ', self.A[0].numpy() )\n",
    "            print_tex(r'\\vec{g}_0^\\prime = \\vec{A}_0 \\otimes G = ', self.GkPrime[0].numpy())\n",
    "            concat = self.GkPrime.reshape(N_NODES, N_HEADS * N_HIDDEN)\n",
    "            print_tex(concat[0].numpy())\n",
    "\n",
    "model = net(H,torch.tensor(A),True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some things ive learned\n",
    "* you can define an array, and its reshaped representation. If you dont redefine array, you can change it, and reshaped representation will also change (duh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3, 4])\n",
      "tensor([[2],\n",
      "        [3],\n",
      "        [4]])\n",
      "tensor([5, 7, 9])\n",
      "tensor([[5],\n",
      "        [7],\n",
      "        [9]])\n"
     ]
    }
   ],
   "source": [
    "asd = torch.arange(2,5,1);print(asd)\n",
    "asd2 = asd.reshape(-1,1);print(asd2)\n",
    "asd += torch.arange(3,6,1);print(asd)\n",
    "print(asd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* masking does not create a view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3, 4])\n",
      "tensor([ True, False, False])\n",
      "tensor([0, 3, 4])\n",
      "tensor([2, 3, 8])\n",
      "tensor([0, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "asd = torch.arange(2,5,1);print(asd)\n",
    "mask = torch.tensor([1,0,0], dtype=bool);print(mask)\n",
    "asd2 = asd.masked_fill(mask=mask,value= 0);print(asd2)\n",
    "asd[2] = 8; print(asd)\n",
    "print(asd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) torch.Size([3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  5],\n",
       "        [ 9, 11]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd = torch.tensor([[1,2,3],[4,5,6]])\n",
    "asd2 = torch.stack([torch.tensor([[1,2,3],[4,5,6]]),\n",
    "                    torch.tensor([[7,8,9],[10,11,12]])])\n",
    "\n",
    "a = torch.tensor([[0,0,1],[0,1,0]]).T\n",
    "print(asd2.shape, a.shape)\n",
    "torch.diagonal(asd2 @ a, dim1=-2, dim2=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "iterable unpacking cannot be used in comprehension (3707840137.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[93], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    print([*[i, k] for i,k in zip([1,2],[10,20])])\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m iterable unpacking cannot be used in comprehension\n"
     ]
    }
   ],
   "source": [
    "print([*[i, k] for i,k in zip([1,2],[10,20])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 10, 2, 20]\n"
     ]
    }
   ],
   "source": [
    "list1 = [1, 2]\n",
    "list2 = [10, 20]\n",
    "\n",
    "interleaved_list = [item for sublist in zip(list1, list2) for item in sublist]\n",
    "\n",
    "print(interleaved_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
