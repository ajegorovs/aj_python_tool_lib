{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAT (Graph Attention Network) dummy flowchart notebook\n",
    "This is a small companion notebook to GNN_Attention_notes.ipynb, which presents a more compact version of 'pseudo solved' case.\n",
    "\n",
    "Full implementation does many operations in-place and adds features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input example : \n",
      ">>> arr_T = np.array([[r'\\vec{v}_1', r'\\vec{v}_2']]).T\n",
      ">>> print_tex(arr_T,'=', np.arange(1,5).reshape(2,-1)/4, r'; symbols: \\otimes, \\cdot,\\times')\n",
      "output: \n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\begin{bmatrix}\\vec{v}_1 \\\\ \\vec{v}_2\\end{bmatrix}=\\begin{bmatrix} 1/4 & 1/2 \\\\ 3/4 & 1 \\end{bmatrix}; symbols: \\otimes, \\cdot,\\times$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cpu\")# torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from misc_tools.print_latex import print_tex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle A = \\begin{bmatrix} 0 & 1 & 1 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 0 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1337)\n",
    "N = 3\n",
    "G = nx.gnm_random_graph(N, 2*N)\n",
    "A = nx.adjacency_matrix(G).todense()\n",
    "print_tex('A = ', A)\n",
    "A = torch.tensor(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N_FEATURES, N_NODES = A.shape\n",
    "N_NODES = len(G.nodes())\n",
    "N_FEATURES = 3\n",
    "N_HIDDEN = 2\n",
    "N_HEADS = 2\n",
    "OUT_FEATURES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle H = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\\\ 7 & 8 & 9 \\end{bmatrix}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "H = torch.arange(N_NODES*N_FEATURES, dtype = float).view(N_NODES, N_FEATURES) + 1\n",
    "print_tex('H = ', H.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[3, 2, 3]' is invalid for input of size 12",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 90\u001b[0m\n\u001b[0;32m     86\u001b[0m             GkP_avg  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGkPrime\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     87\u001b[0m             print_tex(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mG_0^\u001b[39m\u001b[38;5;132;01m{concat}\u001b[39;00m\u001b[38;5;124m = \u001b[39m\u001b[38;5;124m'\u001b[39m, GkP_concat[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m; \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m G_0^\u001b[39m\u001b[38;5;132;01m{Avg}\u001b[39;00m\u001b[38;5;124m = \u001b[39m\u001b[38;5;124m'\u001b[39m,GkP_avg[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m---> 90\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mdebug_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m, in \u001b[0;36mdebug_net.__init__\u001b[1;34m(self, H0, A, test)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_gh   \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(in_features\u001b[38;5;241m=\u001b[39mN_FEATURES, out_features\u001b[38;5;241m=\u001b[39mN_HEADS\u001b[38;5;241m*\u001b[39mN_HIDDEN, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mH0\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGk     \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(size\u001b[38;5;241m=\u001b[39m(N_NODES, N_HEADS\u001b[38;5;241m*\u001b[39mN_HIDDEN), dtype\u001b[38;5;241m=\u001b[39mH0\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGkR    \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN_NODES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_HEADS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_FEATURES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCk_l   \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(size\u001b[38;5;241m=\u001b[39m(N_NODES\u001b[38;5;241m*\u001b[39mN_NODES, N_HEADS, N_HIDDEN), dtype\u001b[38;5;241m=\u001b[39mH0\u001b[38;5;241m.\u001b[39mdtype) \n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCk_r   \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCk_l)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[3, 2, 3]' is invalid for input of size 12"
     ]
    }
   ],
   "source": [
    "class debug_net(nn.Module):\n",
    "    def __init__(self, H0, A, test = False):\n",
    "        super(debug_net, self).__init__()\n",
    "        self.H = H0\n",
    "        self.A = A\n",
    "        self.W_gh   = nn.Linear(in_features=N_FEATURES, out_features=N_HEADS*N_HIDDEN, bias=False, dtype=H0.dtype)\n",
    "        self.Gk     = torch.zeros(size=(N_NODES, N_HEADS*N_HIDDEN), dtype=H0.dtype)\n",
    "        self.GkR    = self.Gk.view(N_NODES, N_HEADS, N_FEATURES)\n",
    "        self.Ck_l   = torch.zeros(size=(N_NODES*N_NODES, N_HEADS, N_HIDDEN), dtype=H0.dtype) \n",
    "        self.Ck_r   = torch.zeros_like(self.Ck_l)\n",
    "        self.Ck_f   = torch.zeros(size=(N_NODES*N_NODES, N_HEADS, 2*N_HIDDEN), dtype=H0.dtype) \n",
    "        self.Ck     = self.Ck_f.view(N_NODES, N_NODES, N_HEADS, 2*N_HIDDEN)\n",
    "        self.attnt  = nn.Parameter(torch.zeros(size=(2*N_HIDDEN, N_HEADS), dtype=H0.dtype))\n",
    "        self.activ  = nn.LeakyReLU(0.2)\n",
    "        self.E      = torch.zeros(size=(N_NODES,N_NODES, N_HEADS), dtype=H0.dtype)\n",
    "        self.alpha  = torch.zeros_like(self.E)\n",
    "        self.softmax= nn.Softmax(dim = 1)\n",
    "        self.GkPrime= torch.zeros_like(self.GkR)\n",
    "         \n",
    "        if test:\n",
    "            self.debug()\n",
    "\n",
    "    def debug(self):\n",
    "        with torch.no_grad():\n",
    "            print_tex(r'N_{nodes} = '+ str(N_NODES) + r'; \\ N_{heads} = '+ str(N_HEADS) + r'; \\ N_{features} = '+ str(N_FEATURES)+ r'; \\ N_{hidden} = '+ str(N_HIDDEN))\n",
    "            print_tex(r'G_{K} \\text{ is a matrix of concatenated embeddings } \\vec{g}_i^{k} , \\ shape : [N_{nodes} \\times N_{heads}*N_{hidden}]')\n",
    "\n",
    "            # set scaling transforms\n",
    "            for i in range(N_HEADS):\n",
    "                s = 1 if i == 0 else 4*i\n",
    "\n",
    "                self.W_gh.weight[i*N_FEATURES:(i+1)*N_FEATURES] = s*torch.eye(N_FEATURES)\n",
    "\n",
    "            self.Gk += self.W_gh(self.H)        # cannot redefine, it will break a view\n",
    "\n",
    "            print_tex('G_K = H W_K^T = ', H.numpy() , self.W_gh.weight[:].T.numpy(),' = ', self.Gk.numpy())\n",
    "            print_tex(r'\\text{Reshape } G_{K} \\ to \\ [N_{nodes} \\times N_{heads} \\times N_{hidden}] \\text{ to isolate each head`s data to its own dimension}')\n",
    "\n",
    "            print_tex(r\"\\text{Goal: a matrix } C_K \\text{ that holds concatenated node feature pairs. Shape: }[N_{nodes} \\times N_{nodes}\\times N_{heads} \\times 2 N_{hidden}]\")\n",
    "            print(\"Its only (?) possible with flattening, concatenating and unflattening. See notes.\")\n",
    "\n",
    "            self.Ck_l += self.GkR.repeat_interleave(N_NODES, dim=0)\n",
    "            self.Ck_r += self.GkR.repeat(N_NODES, 1, 1)\n",
    "            self.Ck_f += torch.cat([self.Ck_l, self.Ck_r], dim=-1);\n",
    "            \n",
    "            print_tex(r'C_{flat} \\ (K=1) = Repeat_{flat} \\ ||  \\ Interleave_{flat} = '\n",
    "                      ,self.Ck_l[:,[0]].squeeze(1).numpy(),r' \\ \\bigg|\\bigg| \\ ', self.Ck_r[:,[0]].squeeze(1).numpy(), ' = ',\n",
    "                      self.Ck_f[:,[0]].squeeze(1).numpy())\n",
    "            \n",
    "            print_tex(r'\\text{Features C}_{0,0} = ', self.Ck[0,0].numpy(), r'; \\ shape: \\ [N_{heads} \\times 2 N_{hidden}]')\n",
    "            prnt_vec = [r'\\vec{a}_'+str(i)+ ' = ' for i in range(N_HEADS)]\n",
    "            prnt_vec2 = [r'^T ; \\ ' for i in range(N_HEADS)]\n",
    "\n",
    "            self.attnt += np.repeat(1/(torch.arange(N_HEADS, dtype=self.H.dtype).unsqueeze(0)+1), repeats=2*N_HIDDEN, axis = 0)\n",
    "            print_tex(r\"\\text{Goal: a matrix E that holds edge weights. Shape: }[N_{nodes} \\times N_{nodes} \\times N_{heads}]\")\n",
    "            print_tex(r'E = \\sigma(\\vec{a}[C_K])')\n",
    "            print('Test attention vectors:')\n",
    "            print_tex(*[l for lists in zip(prnt_vec,self.attnt.T.numpy(),prnt_vec2) for l in lists])\n",
    "            print('>>>See how to apply multiple attention vectors to data in notes<<<')\n",
    "            \n",
    "            self.E += self.activ(torch.einsum('ijkf,fk -> ijk', self.Ck, self.attnt)).squeeze(-1)#\n",
    "\n",
    "            print_tex(r'\\text{Features E}_{0,0} = ', self.E[0,0].numpy())\n",
    "            self.alpha += self.E.masked_fill(self.A.view(N_NODES,N_NODES,1) == 0, float('-inf'))\n",
    "            for i in range(N_HEADS):\n",
    "                print_tex('E_{K='+str(i + 1)+'} = ',self.E.numpy()[:,:,i], r'\\rightarrow MASK \\rightarrow ',(self.E[:,:,i]*self.A).numpy() )\n",
    "            print_tex(r\"\\text{Goal: a matrix } \\Alpha \\ or \\ \\alpha \\text{ with row-wise softmax normalized weights. Shape: }[N_{nodes} \\times N_{nodes} \\times N_{heads}]\")\n",
    "            self.alpha = self.softmax(self.alpha)\n",
    "            \n",
    "            for i in range(N_HEADS):\n",
    "                print_tex(r'\\alpha_{K='+str(i + 1)+'} = ', self.alpha.numpy()[:,:,i], r'{\\rightarrow set \\ to \\ A \\ for \\ example \\rightarrow }:', self.A.numpy())\n",
    "                self.alpha.numpy()[:,:,i] = self.A\n",
    "            self.GkPrime += torch.einsum('ijk,jkf->ikf', self.alpha , self.GkR)\n",
    "\n",
    "            print_tex(r\"\\text{Goal: updated features } G_k^\\prime \\text{ based on aggregation of features } \\vec{g}_i^k \\text{ with weights } \\Alpha \\text{ . Shape: }[N_{nodes} \\times N_{heads} \\times N_{hidden}]\")\n",
    "            print('>>>See how to aggregate multi head case in notes<<<')\n",
    "            n1_neighbors_id = torch.argwhere(self.A[0] == 1).flatten().numpy()\n",
    "            \n",
    "            a = [r'G_'+str(i)+ ' = ' for i in n1_neighbors_id]\n",
    "            b = [self.GkR[i].numpy() for i in n1_neighbors_id]\n",
    "            c = [r'; \\ ' for i in n1_neighbors_id]\n",
    "            print_tex(*[l for lists in zip(a,b,c) for l in lists], r'\\Alpha|_{row,1}= ', self.alpha[0].numpy() )\n",
    "            print_tex(r'G_0^\\prime = \\vec{A}_0 \\otimes G = ', self.GkPrime[0].numpy())\n",
    "            print('New embeddings can be either concatenated across different variants of k or averaged\"')\n",
    "            GkP_concat = self.GkPrime.reshape(N_NODES, N_HEADS * N_HIDDEN)\n",
    "            GkP_avg  = self.GkPrime.mean(dim=1)\n",
    "            print_tex(r'G_0^{concat} = ', GkP_concat[0].numpy(), r'; \\ G_0^{Avg} = ',GkP_avg[0].numpy())\n",
    "\n",
    "\n",
    "model = debug_net(H,A,True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
